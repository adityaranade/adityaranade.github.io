[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Following are the courses I have taught. All the courses are taught at Iowa State University unless otherwise mentioned.\n\nSpring 2025\n\n(Course Instructor) Introduction to Business Statistics I\n\n\n\n\nFall 2024\n\n(Course Instructor) Introduction to Business Statistics I\n\nSpring 2024\n\n(Course Instructor) Introduction to Business Statistics I\n\nFall 2023\n\n(Course Instructor) Introduction to Business Statistics I\n\nSpring 2023\n\n(Course Instructor) Introduction to Business Statistics I\n\nFall 2022\n\n(Course Instructor) Introduction to Business Statistics I\n\nSummer 2022\n\n(Course Instructor) Introduction to Business Statistics II\n\nSpring 2022 (Teaching Excellence Award)\n\n(Course Instructor) Introduction to Business Statistics I\n\nFall 2021\n\n(Course Instructor) Introduction to Business Statistics I\n\nSpring 2021\n\n(Teaching Assistant) STAT 474/574: Introduction to Bayesian Data Analysis\n(Grading Assistant) STAT 104: Introduction to Statistics\n\nFall 2020\n\n(Teaching Assistant) STAT 587: Statistical Methods for Research Workers\n(Lab Instructor) STAT 326: Introduction to Business Statistics II\n\nSpring 2020\n\n(Teaching Assistant) STAT 404: Regression for Social and Behavioral Research\n(Lab Instructor) STAT 326: Introduction to Business Statistics II\n\nFall 2019\n\n(Teaching Assistant) STAT 404: Regression for Social and Behavioral Research\n(Grading Assistant) STAT 226: Introduction to Business Statistics I\n\nSpring 2019 (San Diego State University)\n\n(Lab Instructor) STAT 250: Statistical Principles and Practices\n\nFall 2018 (San Diego State University)\n\n(Lab Instructor) STAT 250: Statistical Principles and Practices\n\nSpring 2018 (San Diego State University)\n\n(Grading Assistant) STAT 596: Advanced Topics in Statistics"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Starbucks beverages nutritional information\n\n\nWe all like starbucks beverages, but what about the nutritional value of them?\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nAditya Ranade\n\n\n\n\n\n\n\n\n\n\n\n\nCherry Blossom prediction using Time Series\n\n\nA time series analysis to forecast 2024 cherry blossom bloom date\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nAditya Ranade\n\n\n\n\n\n\n\n\n\n\n\n\nDoes the best team win the Indian Premier League ?\n\n\nA simple Bayesian analysis to check if the best team wins the league reveals a rather surprising result.\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nAditya Ranade\n\n\n\n\n\n\n\n\n\n\n\n\nComparing batting averages of the fabulous four players of Indian test cricket by opposition\n\n\nA Bayesian hierarchical model to compare the batting averages of the Indian test cricket team’s fabulous four players from late 1990’s and early 2000’s.\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\nAditya Ranade\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fab4/index.html",
    "href": "posts/fab4/index.html",
    "title": "Comparing batting averages of the fabulous four players of Indian test cricket by opposition",
    "section": "",
    "text": "Sachin Tendulkar, Rahul Dravid, Sourav Ganguly and VVS Laxman are usually dubbed the faboulous four of Indian cricket team. In the late 1990’s and early 2000’s, these players were at their peak and were the backbone of Indian Test Cricket team from the batting perspective.\nWe will have a look at the career batting average of each of the player against the nine test playing nations and try to understand which of the player has been a consistent performer across teams. Sachin Tendulkar made his test cricket debut in 1989 whereas Rahul Dravid, Sourav Ganguly and VVS Laxman made their debuts in the year 1996. Sourav Ganguly was the first to retire in 2008 followed by Rahul Dravid, VVS Laxman both of whom retired in the year 2012 and lastly Sachin Tendulkar retired in the year 2013.\nThe career summary of the four players under consideration can be seen in table below\n\n\n\nCareer details for players\n\n\nSince all the players have played more that 100 matches (and more that 150 innings), we can compare the batting averages for them against all the nine test playing nations. However, we will get a little adventurous and use a Bayesian approach to calculate the credible intervals for the batting average for each of the player.\nIn case you are wondering what a Bayesian method means, it is simply reconciling our belief (prior) with the data (likelihood) to obtain updated belief (posterior). And the credible interval gives us the plausible values for the batting average based on the data we have. Along with the average against each opponent, we are incorporating the variation in the scoring as well which is nothing but considering how consistently or inconsistently the runs were scored.\nWe will incorporate another layer of uncertainty in the sense that we let the parameter at the first level have their own uncertainty and assign a distribution of their own. This is done through a Bayesian hierarchical model\nThe Bayesian Hierarchical model we will use for each player is as follows\n\\[Y_{i} \\overset{\\mathrm{i.i.d}}\\sim N(\\mu_{opposition[i]}, \\sigma_{opposition[i]}^2)\\]\n\\[\\mu_{opposition} \\overset{ind}{\\sim} N(\\theta,\\tau^2)\\]\n\\[\\theta \\sim N(\\theta_{0}, \\tau_{0}^2)\\]\n\\[\\tau \\sim Ca^{+}(0,1)\\]\n\\[\\sigma_{opposition} \\sim Uniform(10,200)\\]\nwhere \\(\\theta_{0} = 50\\) and \\(\\tau_{0} = 10\\) has been used which is consistent with each of the player’s career average.\nThe model was run using a JAGS (Just Another Gibbs Sampler) which samples from the posterior distribution of the parameters. We will not go into much details about the individual parameters but use the parameters to come up with the credible intervals for the average runs scored per innings for all the players against each of the test playing nation. We than combine the credible intervals for all the players into a single plot which can be found below.\n\n\n\n\nCredible intervals for the average runs scored per inning against each of the opposition\n\n\nWe have the credible intervals for each of the four players against the nine test playing nations. Opposition is indicated on the x axis and the horizontal lines corresponding to any of the opposition is the credible interval for average against that particular opposition. We use different colors to indicate which player the interval corresponds to. Purple colour corresponds to intervals for Sachin Tendulkar, red colour corresponds to intervals for Rahul Dravid, green colour corresponds to intervals for Sourav Ganguly and blue colour corresponds to intervals for VVS Laxman.\nSince all the players have played against each of the opposition, we have intervals for all the players against all the opposition. However, all the players have batted less than 10 innings against Bangladesh and Zimbabwe and hence the intervals are not much meaningful.\nThere are two characteristics which can be understood looking at any of the interval. The first is the width of interval which indicates how consistent the player was against that particular opposition. If the width of the interval is big, that indicates the player scored runs inconsistently against the particular opposition. So we ideally want the interval to be as narrow as possible.\nThe second characteristic is the location of the interval which indicates the average of the player againt a particular opposition. If the average is higher, the location of the inteval will be towards the right and if the average is on the lower side the interval will lie towards the left side. Ideally we want a interval to be towards the right side.\nNow that we have understood how to read the intervals, let us start comparing the intervals for players against each of the opposition. In professional sports, every player seems to have a ‘favored’ opposition against whom the player plays extraordinarily well which will be indicated in the plot by a narrow interval towards the right side. Similarly every player has an opposition who they do not enjoy playing which will be indicated in the plot by a wide interval and/or interval towards the left side.\nAgainst SENA countries\nFor teams in the Indian subcontinent, test cricket against SENA countries (South Africa, England, New Zealand and Australia) are considered an good indicator of how good a player is.\nAustralia was the most dominant opposition during the playing career of the four players. Australia tour of India in 2001 was a legendary series which was the turning point of Indian test cricket. The second test match in the series at Kolkata which India won while following on (an overwhelmingly difficult task) was nothing short of a legendary match. It is well know that Australia is VVS Laxman’s favoured opposition. However, if we look at the interval’s corresponding to Australia, we can see that Sachin Tendulkar has a better average than the other three players against Australia followed by VVS Laxman, Rahul Dravid and lastly Sourav Ganguly.\nAgainst New Zealand, Rahul Dravid seems to have the higher average followed by VVS Laxman, Sachin Tendulkar and lastly Sourav Ganguly.\nAgainst England, Rahul Dravid seems to have the higher average followed by Sourav Ganguly, Sachin Tendulkar and lastly VVS Laxman. VVS Laxman seems to have unusually low interval for average against England.\nAll the players seem to have performed below par against South Africa. Among the four players, Sachin Tendulkar seems to have performed relatively better than then the other three players who seems to have a similar record against South Africa.\nAgainst other countries\nAgainst Pakistan, Rahul Dravid has the highest average followed by Sourav Ganguly, Sachin Tendulkar and lastly VVS Laxman.\nAgainst Sri Lanka, Sachin Tendulkar has the highest average followed by Rahul Dravid, Sourav Ganguly and lastly VVS Laxman.\nAgainst West Indies, Rahul Dravid has the highest average followed by Sachin Tendulkar, VVS Laxman and lastly Sourav Ganguly. Sourav Ganguly has an unusually low interval for average against West Indies.\nAll the players have played less than 10 innings against Zimbabwe and Bangladesh and hence the scores have more variation in them which can be seen by the wide intervals. So comparing the intervals for the players against these two teams is not meaningful."
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "I like to read (mostly fiction). Some of the books I have enjoyed (not in a particular order) are as follows.\n\n\n\n\n\n\n\nBook\nAuthor\n\n\n\n\nNo Motive (short story)\nDaphne du Maurier\n\n\nAll She was Worth\nMiyuki Miyabe\n\n\nThe Devotion of Suspect X\nKeigo Higashino\n\n\nSalvation of a Saint\nKeigo Higashino\n\n\nA Midsummer’s Equation\nKeigo Higashino\n\n\nSilent Parade\nKeigo Higashino\n\n\nMalice\nKeigo Higashino\n\n\nNewcomer\nKeigo Higashino\n\n\nA Death in Tokyo\nKeigo Higashino\n\n\nThe Final Curtain\nKeigo Higashino\n\n\nInvisible Helix\nKeigo Higashino\n\n\nThe Name of the Game is a Kidnapping\nKeigo Higashino\n\n\nJourney Under the Midnight Sun\nKeigo Higashino\n\n\nNaoko\nKeigo Higashino\n\n\nSix-Four\nHideo Yokoyama\n\n\nAnimal Farm\nGeorge Orwell\n\n\nThe Good Earth\nPearl S. Buck\n\n\nThe Long Walk: The True Story Of A Trek To Freedom\nSlawomir Rawicz\n\n\nThe Seventh Secret\nIrving Wallace\n\n\nAs Far as My Feet Will Carry Me\nJosef Martin Bauer\n\n\nThe Harry Potter series\nJ. K. Rowling"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aditya Ranade",
    "section": "",
    "text": "Hello! My name is Aditya Ranade. I am a PhD student in the Department of Statistics, Iowa State University. My advisor is Dr. Jarad Niemi. My research interest is application of Bayesian methods and currently work on developing machine learning techniques for functional data. I also try to write about sports analytics during my free time."
  },
  {
    "objectID": "posts/cherry_blossom_prediction/index.html",
    "href": "posts/cherry_blossom_prediction/index.html",
    "title": "Cherry Blossom prediction using Time Series",
    "section": "",
    "text": "Cherry Blossom is one of the most scenic visuals one can experience. Cherry blossom season marks the arrival of spring season and can be considered as transition from winter to summer. People try to make plans travel to enjoy this phenomenon. So how about using some simple statistical techniques to try and forecast / predict the peak cherry blossom time ?\n\n\nAlong with some of my fellow PhD classmates, I participated in the International Cherry Blossom Prediction Competition hosted by George Mason university. We explored a lot of models and I am going to show a very basic model which I tried during the early stages. The model is the Autogegressive (AR) model. The notation of this model is AR(1) model is as follows\n\n\\[\nY_{t} = \\beta_{1} Y_{t-1} + \\epsilon_{t}\n\\] where $Y_{t}$ is the bloom day for year $t$, \\(\\beta_{i}\\) is the model parameter and \\(\\epsilon_{t}\\) is the white noise\n\nThis simply means the present value of the response variable $Y$ (in our case the bloom day for this year) is influenced by previous value of the response variable (in our case the bloom day of the previous year). If you are aware of the simple linear regression, think of this as the explanatory variable being the same as the predictor variable in rough sense. In the competition, we tried to predict the bloom date for multiple location across the world based on the data available provide by the university. However, for the purpose of this post, I will show the analysis only for one location, Kyoto in Japan.\n\n\nLet us start with first reading in the dataset and loading the R packages required for the analysis\n\n\n# Load the packages\nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(fpp2)\nlibrary(dplyr)\nlibrary(vars)\n\n# Load the dataset\nkyoto &lt;- read.csv(\"https://raw.githubusercontent.com/GMU-CherryBlossomCompetition/peak-bloom-prediction/main/data/kyoto.csv\",header=T)\n\n# Plot of the bloom date over the years\nggplot(kyoto,aes(x=year,y=bloom_doy))+\n  geom_point()+\n  labs(x=\"Year\",y=\"Bloom Day\")+\n  ggtitle(\"Bloom Day by Year\")\n\n\n\n\n\n\n\n\n\nAs we can see from the plot, towards the later end (in the recent past), the bloom day has started to go down. This means in the recent past, the bloom day is happening earlier than before. Let us look at the plot only from the year 1950.\n\n\n# Filter data only for year since 1951\nkyoto_new &lt;- kyoto %&gt;% filter(year&gt;1950)\n\n# Plot of the bloom date over the years\nggplot(kyoto_new,aes(x=year,y=bloom_doy))+\n  geom_point()+\n  labs(x=\"Year\",y=\"Bloom Day\")+\n  ggtitle(\"Bloom Day by Year\")\n\n\n\n\n\n\n\n\n\nAs we can see from the plot for year 1951 onward, there seems to be a downward trend which indicates the bloom date is in general arriving earlier.\n\n\nWe will use the data from 1951 to 2022 to predict the bloom date for year 2023 and compare that to actual bloom date. For this, we will use the bloom day as response and the year as the predictor\n\n\n# Prepare the data for ARIMA(1,0,0) model\ny_kyoto &lt;- kyoto_new$bloom_doy # bloom day as the response\n\nExclude the year 2023 from response and explanatory variable to test the model on year 2023\n\n# First test on 2023 model\nytest &lt;- y_kyoto[-length(y_kyoto)] # exclude the bloom day for 2023 year\n\n# Model based on year 1951 to 2022\nfit_kyoto_test &lt;- Arima(ytest, order=c(1,0,0)) # order=c(1,0,0) indicates AR(1) model\nfit_kyoto_test\n\nSeries: ytest \nARIMA(1,0,0) with non-zero mean \n\nCoefficients:\n         ar1     mean\n      0.2821  97.2885\ns.e.  0.1182   0.7483\n\nsigma^2 = 21.84:  log likelihood = -215.16\nAIC=436.32   AICc=436.67   BIC=443.19\n\n#Forecast\nfcast_kyoto_test &lt;- forecast(fit_kyoto_test)\nfcast_kyoto_test\n\n   Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n74       93.53975 87.55120  99.5283 84.38105 102.6984\n75       96.23094 90.00866 102.4532 86.71479 105.7471\n76       96.99013 90.74963 103.2306 87.44611 106.5342\n77       97.20430 90.96235 103.4463 87.65806 106.7505\n78       97.26472 91.02265 103.5068 87.71830 106.8111\n79       97.28176 91.03969 103.5238 87.73533 106.8282\n80       97.28657 91.04450 103.5286 87.74014 106.8330\n81       97.28793 91.04585 103.5300 87.74150 106.8344\n82       97.28831 91.04624 103.5304 87.74188 106.8347\n83       97.28842 91.04634 103.5305 87.74199 106.8349\n\n# Check actual bloom date for 2023\ny_kyoto[length(y_kyoto)] \n\n[1] 95\n\n\n\nThe AR(1) model predicts 96 as the bloom day for year 2023 whereas the actual bloom day was 84 for year 2023 which is a difference of 12 days. Considering its a basic model, this does not seem to be too bad.\n\n\nNow we check the performance of the model using some charts where we first check the prediction plot, then the Regression and model errors.\n\n\n# Plot the prediction\nautoplot(fcast_kyoto_test) + xlab(\"Year\") +\n  ylab(\"Percentage change\")\n\n\n\n\n\n\n\n# recover estimates of nu(t) and epsilon(t) \ncbind(\"Regression Errors\" = residuals(fit_kyoto_test, type=\"regression\"),\n      \"ARIMA errors\" = residuals(fit_kyoto_test, type=\"innovation\")) %&gt;%\n  autoplot(facets=TRUE)\n\n\n\n\n\n\n\n\n\nThere does not seem to be any issues with either of the plots. Now we check the residuals to see if they are normally distributed.\n\n\n# Check the residuals\ncheckresiduals(fit_kyoto_test)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,0) with non-zero mean\nQ* = 13.92, df = 9, p-value = 0.1252\n\nModel df: 1.   Total lags used: 10\n\n\n\nThe residuals seem to be normally distributed and the Ljung-Box test indicates we have little evidence against the null hypothesis of independently distributed errors.\n\n\nNow we will use the data from 1951 upto 2023 to predict the bloom date for the year 2024. Basically its the same model with one extra data point available\n\n\n# Now use data upto 2023 to predict 2024\nfit_kyoto &lt;- Arima(y_kyoto, order=c(1,0,0)) # order=c(1,0,0) indicates AR(1) model\nfit_kyoto\n\nSeries: y_kyoto \nARIMA(1,0,0) with non-zero mean \n\nCoefficients:\n         ar1     mean\n      0.2707  97.3162\ns.e.  0.1111   0.7264\n\nsigma^2 = 21.56:  log likelihood = -217.65\nAIC=441.3   AICc=441.64   BIC=448.21\n\n#Forecast\nfcast_kyoto &lt;- forecast(fit_kyoto)\nfcast_kyoto\n\n   Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n75       96.68923 90.73853 102.6399 87.58842 105.7900\n76       97.14647 90.98163 103.3113 87.71816 106.5748\n77       97.27023 91.08999 103.4505 87.81837 106.7221\n78       97.30373 91.12237 103.4851 87.85015 106.7573\n79       97.31280 91.13135 103.4943 87.85909 106.7665\n80       97.31526 91.13380 103.4967 87.86154 106.7690\n81       97.31592 91.13447 103.4974 87.86220 106.7696\n82       97.31610 91.13465 103.4976 87.86238 106.7698\n83       97.31615 91.13469 103.4976 87.86243 106.7699\n84       97.31616 91.13471 103.4976 87.86244 106.7699\n\n\n\nThe model predicts the cherry blossom to bloom on day 93 which is 2nd April 2024 (due to 2024 being a leap year) for Kyoto, Tokyo. Now lets look at the diagnostics of the model to see if the model is reasonable.\n\n\n# Plot the forecast\nautoplot(fcast_kyoto) + xlab(\"Year\") +\n  ylab(\"Percentage change\")\n\n\n\n\n\n\n\n# recover estimates of nu(t) and epsilon(t) \ncbind(\"Regression Errors\" = residuals(fit_kyoto, type=\"regression\"),\n      \"ARIMA errors\" = residuals(fit_kyoto, type=\"innovation\")) %&gt;%\n  autoplot(facets=TRUE)\n\n\n\n\n\n\n\n# Check the residuals\ncheckresiduals(fit_kyoto)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,0) with non-zero mean\nQ* = 13.9, df = 9, p-value = 0.1259\n\nModel df: 1.   Total lags used: 10\n\n\n\nAgain, the residuals seem to be normally distributed and the Ljung-Box test indicates we have little evidence against the null hypothesis of independently distributed errors."
  },
  {
    "objectID": "posts/ipl_best_team/index.html",
    "href": "posts/ipl_best_team/index.html",
    "title": "Does the best team win the Indian Premier League ?",
    "section": "",
    "text": "Indian Premier League (IPL) is an annual cricket league which is played under the T20 format. It was started in the year 2008 and has quickly gained popularity among cricket playing nations and has probably become one of the biggest sporting event in the Indian sports calendar.\nThe tournament is played between various teams based out of different cities in India. IPL is currently in its 14th season where the tournament has been played with 8 teams from 2008 - 2010 and 2014-2021. It was played with 10 teams in 2011 and 9 teams in 2012 and 2013. From 2022, it has been expanded again to 10 teams.\nThe tournament followed a double round-robin format from 2008 - 2021 (except for the year 2011) where each team plays the other team twice (once at each team’s ‘home’ ground). From the year 2022, the teams are divided into two groups and every team plays 14 games (twice against the teams in the same group, twice against one team from other group and once against the other teams from the other group). At the conclusion of the league stage, the top four teams qualify for the playoffs. For the first 3 years (2008-2010), the traditional semifinals and final approach was considered. From 2011, a playoff structure was introduced to award the top two teams with an additional chance to reach the finals. This ensured the competition stays relevant till the last game of the league stage as teams vie for top 2 position. The figure below explains the playoff structure.\n\n\n\nIPL Playoff Structure\n\n\nThe top two teams of the league stage have two chances to reach the final. The thought behind this playoff structure was that the top two teams should not be out of the tournament due to one bad day in the semi-finals.\nA question that pops up in the mind is “Does the best team win the tournament”. In other words, does the team which finishes the league stage at the top of the table have the best chance to win the league. The team which finishes the league stage position 1 has won the most number of points (and in case of tied points, has a superior net run rate) and hence is considered the “best” team in the league. We would be tempted to say yes. But let’s find out what the data says.\nWe have the data available for 11 years (2011 - 2021) for the league stage standings and the playoff results for the team topping the league stage from IPL official website which can be seen below\n\n\n\nTeam finishing first during league stage\n\n\nWe can observe, the team which tops the league stage has won the tournament only three 3 of 11 times which corresponds to approximately 27.27%. However, this is just the observation from 11 years so cannot be considered as the best indicator of probability. We will consider a Bayesian statistical model to calculate the probability of the team finishing the tournament as winner, runners up and third place.\nIn simple terms, a Bayesian model is reconciliation of our belief ( ‘prior’ distribution) with the observed data (likelihood) to give the updated belief (‘posterior’ distribution). Here the information (prior, likelihood and posterior) is in the form of mathematical distribution functions.\nIf we consider the random variable ‘result’ which indicates the position of the team at the end of the tournament. The team which finishes the league stage at the top of the table can finish at any position from 1 to 3 at the end of the tournament where 1 stands for winner, 2 stands for runner’s up and 3 stands for 3rd place. Since the variable can take only 1 out of the 3 values, it follows a categorical distribution which is denoted as result ~ categorical(\\(p_{1}, p_{2}, p_{3}\\)) where \\(p_{1}\\), \\(p_{2}\\) and \\(p_{3}\\) indicates the probability of finishing at each of the position at the end of the tournament.\nWe will consider the probabilities as a random variable as well. We have to assign a distribution to the variable in such a way that our beliefs are reflected in the distribution. We will consider the team has an equal chance at finishing at each of the positions. This is ensured by assigning a Dirichlet(1,1,1) distribution to (\\(p_{1}\\), \\(p_{2}\\), \\(p_{3}\\))\nSo the model is as follows\n\\[result \\overset{\\mathrm{i.i.d.}}{\\sim} Categorical(p_{1}, p_{2}, p_{3}) \\]\n\\[(p_{1}, p_{2}, p_{3}) \\sim Dirichlet(\\alpha_{1}, \\alpha_{2}, \\alpha_{3}) \\]\nWe run this model on 11 years of the data available (2011 - 2021) and we get the credible interval for the posterior probabilities of finishing at each of the position\n\n\n\nCredible intervals for probabilities for team finishing first during league stage\n\n\nThe horizontal line indicates the plausible values for the probability of team finishing in a particular position and black point indicates the estimated value of the probability from the available data and model used.\nWe can see the probability of being runner up is higher than the probability of finishing as winner or 3rd place which seem to have almost the same probability. This means the team finishing at the top of the table at the end of the league will more often go on to lose in the playoffs (finish as runner up or \\(3^{rd}\\) place) at the end of the tournament.\nIsn’t it surprising? It surely is. Let us look at some of the reasons this might be the case. More often than not, the team which finishes at the top of the table during the league stage accumulates most of the points during the early stages and tends to lose steam towards the business end of the tournament. It might be a case of peaking too early. There might be some other psychological impact that might be worthwhile to study.\nSo probably the best team in the league (team finishing first during league stage) does not win the tournament.!\nLet us perform the same analysis for the team which finishes at second position in the league stage. We will use the same model used for the team finishing first at the league stage. Using data from 11 years, we get the following plot\n\n\n\nCredible intervals for probabilities for team finishing second during league stage\n\n\nWe can see the probability of being winner is higher than the probability of finishing as runner’s up or 3rd place. This means the team finishing second in the league stage will more often go on to win the tournament. Surely a surprising and fascinating result!"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Good things take time! Something exciting is coming up soon! Keep calm and work hard!🙂"
  },
  {
    "objectID": "posts/starbucks/index.html",
    "href": "posts/starbucks/index.html",
    "title": "Starbucks beverages nutritional information",
    "section": "",
    "text": "Starbucks is one of the most valued coffee chain in the world. A lot of people like to consume the beverages available at starbucks. But how good are they in terms of the nutritional value?\n\n\nI found this dataset on Kaggle which gives the nutritional information about their beverages. We will look at the exploratory data analysis first and later try some simple models. First let us access and process the data through R.\n\n\n# Load the packages\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(GGally) # for pairs plot using ggplot framework\n\n\n# Get starbucks data from github repo\npath &lt;- \"https://raw.githubusercontent.com/adityaranade/starbucks/refs/heads/main/data/starbucks-menu-nutrition-drinks.csv\"\ndata0 &lt;- read.csv(path, header = TRUE)\n\n# Data processing\ncolnames(data0) &lt;- c(\"name\", \"calories\", \"fat\", \n                     \"carbs\", \"fiber\",\"protein\", \n                     \"sodium\")\n\n# Check the first 6 rows of the dataset\ndata0 |&gt; head()\n\n                                                name calories fat carbs fiber\n1           Cool Lime Starbucks Refreshers™ Beverage       45   0    11     0\n2                                   Ombré Pink Drink        -   -     -     -\n3                                         Pink Drink        -   -     -     -\n4     Strawberry Acai Starbucks Refreshers™ Beverage       80   0    18     1\n5 Very Berry Hibiscus Starbucks Refreshers™ Beverage       60   0    14     1\n6                                       Violet Drink        -   -     -     -\n  protein sodium\n1       0     10\n2       -      -\n3       -      -\n4       0     10\n5       0     10\n6       -      -\n\n# Check the type of data\ndata0 |&gt; str()\n\n'data.frame':   177 obs. of  7 variables:\n $ name    : chr  \"Cool Lime Starbucks Refreshers™ Beverage\" \"Ombré Pink Drink\" \"Pink Drink\" \"Strawberry Acai Starbucks Refreshers™ Beverage\" ...\n $ calories: chr  \"45\" \"-\" \"-\" \"80\" ...\n $ fat     : chr  \"0\" \"-\" \"-\" \"0\" ...\n $ carbs   : chr  \"11\" \"-\" \"-\" \"18\" ...\n $ fiber   : chr  \"0\" \"-\" \"-\" \"1\" ...\n $ protein : chr  \"0\" \"-\" \"-\" \"0\" ...\n $ sodium  : chr  \"10\" \"-\" \"-\" \"10\" ...\n\n\n\nThe data from second column should be numeric but shows as character. So we first convert it into numeric form and also exclude the rows with missing information\n\n\n# convert the data to numberic second row onwards\ndata0$calories &lt;- as.numeric(data0$calories)\ndata0$fat &lt;- as.numeric(data0$fat)\ndata0$carbs &lt;- as.numeric(data0$carbs)\ndata0$fiber &lt;- as.numeric(data0$fiber)\ndata0$protein &lt;- as.numeric(data0$protein)\ndata0$sodium &lt;- as.numeric(data0$sodium)\n\n# Check the type of data again\ndata0 |&gt; str()\n\n'data.frame':   177 obs. of  7 variables:\n $ name    : chr  \"Cool Lime Starbucks Refreshers™ Beverage\" \"Ombré Pink Drink\" \"Pink Drink\" \"Strawberry Acai Starbucks Refreshers™ Beverage\" ...\n $ calories: num  45 NA NA 80 60 NA NA NA 110 0 ...\n $ fat     : num  0 NA NA 0 0 NA NA NA 0 0 ...\n $ carbs   : num  11 NA NA 18 14 NA NA NA 28 0 ...\n $ fiber   : num  0 NA NA 1 1 NA NA NA 0 0 ...\n $ protein : num  0 NA NA 0 0 NA NA NA 0 0 ...\n $ sodium  : num  10 NA NA 10 10 NA NA NA 5 0 ...\n\n# Check the rows which do not have any entries\nind.na &lt;- which(is.na(data0[,2]))\nlength(ind.na) # 85 NA values\n\n[1] 85\n\n# exclude the rows which has NA values \ndata &lt;- data0[-ind.na,]\n\n\nNow that we have the data ready, let us look at the histogram each of the variables namely calories, fat, carbs, fiber, protein and sodium\n\n\n# Data for histogram\nmelted_data &lt;- melt(data, id.vars=\"name\")\n\n# Plot the histogram of all the variables\nggplot(melted_data,aes(value))+\n  # geom_histogram(aes(y = after_stat(density)),bins = 20)+\n  geom_histogram(bins = 20)+\n  facet_grid2(~variable, scales=\"free\")+theme_bw()\n\n\n\n\n\n\n\n\n\nHistogram does not give much information. Let us look at the correlation plot to get an idea of how the variables are correlated with each other.\n\n\n# correlation plot of all the variables\ncorr &lt;- round(cor(data[,-1]), 1)\np.mat &lt;- cor_pmat(mtcars) # correlation p-value\n# Barring the no significant coefficient\nggcorrplot(corr, hc.order = TRUE,\n           type = \"lower\", p.mat = p.mat)\n\n\n\n\n\n\n\n# All positive correlation\n\n\nAll the variables are positively correlated (which indicates when one variable increases, the other variable will increase as well. ) which is not a surprising. Most important part is the correlation of calories with all the other variables are considerably high. Next we look at the pairs plot which will show the bivariate scatter plots as well as the correlation between each variables.\n\n\nlibrary(GGally)\nggpairs(data,columns = 2:ncol(data),\n        lower = list(continuous = \"smooth\"))\n\n\n\n\n\n\n\n\n\nMost of the bivariate scatter plots indicate a linear relationship between the variables. The most important result according to us is the relationship between calories with all the other variables. We can now use the dataset for predictions where we try to predict the calories based o the fat, carb, fiber, protein and sodium content using multiple linear regression.\n\n\n# split the data into training and testing data\nseed &lt;- 23\nset.seed(seed)\n\nind &lt;- sample(floor(0.8*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train &lt;- data[ind,-1]\n# Testing dataset\ndata_test &lt;- data[-ind,-1]\n\n# Multiple linear regression using raw data\nmodel &lt;- lm(calories ~ fat + carbs + fiber + protein + sodium, data = data_train)\nsummary(model)\n\n\nCall:\nlm(formula = calories ~ fat + carbs + fiber + protein + sodium, \n    data = data_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.2823  -1.7523   0.0195   2.1092   8.1259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.75158    0.94777   1.848   0.0690 .  \nfat          7.65711    0.25133  30.466  &lt; 2e-16 ***\ncarbs        3.79744    0.04057  93.599  &lt; 2e-16 ***\nfiber        2.51563    0.96443   2.608   0.0112 *  \nprotein      0.75585    0.31682   2.386   0.0199 *  \nsodium       0.32290    0.03265   9.889 1.01e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.131 on 67 degrees of freedom\nMultiple R-squared:  0.9976,    Adjusted R-squared:  0.9974 \nF-statistic:  5493 on 5 and 67 DF,  p-value: &lt; 2.2e-16\n\n# Prediction on the testing dataset\ny_pred &lt;- predict(model, data_test)\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred,data_test$calories))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+theme_minimal()+geom_abline()\n\n\n\n\n\n\n\n# Calculate RMSE\nrmse &lt;- (y_pred-data_test$calories)^2 |&gt; sum() |&gt; sqrt()\nrmse\n\n[1] 102.3342\n\n\nNow we will run the model using the principal components. We use the first four principal components for the model since the first four principal components explain around 96.72 % variation in the data.\n\npc &lt;- prcomp(data[,-(1:2)],\n             center = TRUE,\n             scale. = TRUE)\nattributes(pc)\n\n$names\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n$class\n[1] \"prcomp\"\n\n# Check the factor loading of the principal components\nprint(pc)\n\nStandard deviations (1, .., p=5):\n[1] 1.7690827 0.8908878 0.7580688 0.5812294 0.4051785\n\nRotation (n x k) = (5 x 5):\n              PC1        PC2        PC3        PC4         PC5\nfat     0.4610104  0.3810085  0.1092277  0.7933177  0.03190861\ncarbs   0.3784887 -0.5448572 -0.7329252  0.1443775 -0.04304364\nfiber   0.3758452 -0.6263993  0.6433047 -0.0153384  0.22866603\nprotein 0.5161622  0.1647649  0.1161063 -0.3649765 -0.74815813\nsodium  0.4863462  0.3720747 -0.1535201 -0.4651440  0.62056454\n\n# Check the summary of the principal components\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5\nStandard deviation     1.7691 0.8909 0.7581 0.58123 0.40518\nProportion of Variance 0.6259 0.1587 0.1149 0.06757 0.03283\nCumulative Proportion  0.6259 0.7847 0.8996 0.96717 1.00000\n\ndata_pc &lt;- cbind(data[,1:2],pc$x)\n# training data\ndata_pc_train &lt;- data_pc[ind,-1]\n# testing data\ndata_pc_test &lt;- data_pc[-ind,-1]\n\n# Multiple linear regression using PC\nmodel_pc &lt;- lm(calories ~ PC1 + PC2 + PC3 + PC4, data = data_pc_train)\nsummary(model_pc)\n\n\nCall:\nlm(formula = calories ~ PC1 + PC2 + PC3 + PC4, data = data_pc_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3780  -1.5028   0.7182   2.2345   7.4206 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 135.7766     0.5642  240.67   &lt;2e-16 ***\nPC1          49.2547     0.3746  131.47   &lt;2e-16 ***\nPC2         -12.8205     0.9443  -13.58   &lt;2e-16 ***\nPC3         -40.1362     0.9209  -43.59   &lt;2e-16 ***\nPC4          22.9143     0.9862   23.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.442 on 68 degrees of freedom\nMultiple R-squared:  0.9971,    Adjusted R-squared:  0.997 \nF-statistic:  5935 on 4 and 68 DF,  p-value: &lt; 2.2e-16\n\n# Prediction on the testing dataset\ny_pred_pc &lt;- predict(model_pc, data_pc_test)\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred_pc,data_test$calories))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+theme_minimal()+geom_abline()\n\n\n\n\n\n\n\n# Calculate RMSE\nrmse &lt;- (y_pred_pc-data_pc_test$calories)^2 |&gt; sum() |&gt; sqrt()\nrmse\n\n[1] 63.72554\n\n\nRMSE for the regression model using the raw data is 102.33 and for the model using the first four principal components is 63.73. So using the principal components is beneficial for predictions."
  }
]