[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Following are the courses I have taught. All the courses are taught at Iowa State University unless otherwise mentioned.\n\nSpring 2025\n\n(Course Instructor) Introduction to Business Statistics I\n\n\n\n\nFall 2024\n\n(Course Instructor) Introduction to Business Statistics I\n\nSpring 2024\n\n(Course Instructor) Introduction to Business Statistics I\n\nFall 2023\n\n(Course Instructor) Introduction to Business Statistics I\n\nSpring 2023\n\n(Course Instructor) Introduction to Business Statistics I\n\nFall 2022\n\n(Course Instructor) Introduction to Business Statistics I\n\nSummer 2022\n\n(Course Instructor) Introduction to Business Statistics II\n\nSpring 2022 (Teaching Excellence Award)\n\n(Course Instructor) Introduction to Business Statistics I\n\nFall 2021\n\n(Course Instructor) Introduction to Business Statistics I\n\nSpring 2021\n\n(Teaching Assistant) STAT 474/574: Introduction to Bayesian Data Analysis\n(Grading Assistant) STAT 104: Introduction to Statistics\n\nFall 2020\n\n(Teaching Assistant) STAT 587: Statistical Methods for Research Workers\n(Lab Instructor) STAT 326: Introduction to Business Statistics II\n\nSpring 2020\n\n(Teaching Assistant) STAT 404: Regression for Social and Behavioral Research\n(Lab Instructor) STAT 326: Introduction to Business Statistics II\n\nFall 2019\n\n(Teaching Assistant) STAT 404: Regression for Social and Behavioral Research\n(Grading Assistant) STAT 226: Introduction to Business Statistics I\n\nSpring 2019 (San Diego State University)\n\n(Lab Instructor) STAT 250: Statistical Principles and Practices\n\nFall 2018 (San Diego State University)\n\n(Lab Instructor) STAT 250: Statistical Principles and Practices\n\nSpring 2018 (San Diego State University)\n\n(Grading Assistant) STAT 596: Advanced Topics in Statistics"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Portfolio",
    "section": "",
    "text": "Starbucks food nutritional information\n\n\nWe all like starbucks food, but what about the nutritional value of them?\n\n\n\nanalysis\n\n\npython\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nAditya Ranade\n\n\n\n\n\n\n\n\n\n\n\n\nStarbucks beverages nutritional information\n\n\nWe all like starbucks beverages, but what about the nutritional value of them?\n\n\n\nanalysis\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nAditya Ranade\n\n\n\n\n\n\n\n\n\n\n\n\nCherry Blossom prediction using Time Series\n\n\nA time series analysis to forecast 2024 cherry blossom bloom date\n\n\n\nanalysis\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nAditya Ranade\n\n\n\n\n\n\n\n\n\n\n\n\nDoes the best team win the Indian Premier League ?\n\n\nA simple Bayesian analysis to check if the best team wins the league reveals a rather surprising result.\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nAditya Ranade\n\n\n\n\n\n\n\n\n\n\n\n\nComparing batting averages of the fabulous four players of Indian test cricket by opposition\n\n\nA Bayesian hierarchical model to compare the batting averages of the Indian test cricket team‚Äôs fabulous four players from late 1990‚Äôs and early 2000‚Äôs.\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\nAditya Ranade\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fab4/index.html",
    "href": "posts/fab4/index.html",
    "title": "Comparing batting averages of the fabulous four players of Indian test cricket by opposition",
    "section": "",
    "text": "Sachin Tendulkar, Rahul Dravid, Sourav Ganguly and VVS Laxman are usually dubbed the faboulous four of Indian cricket team. In the late 1990‚Äôs and early 2000‚Äôs, these players were at their peak and were the backbone of Indian Test Cricket team from the batting perspective.\nWe will have a look at the career batting average of each of the player against the nine test playing nations and try to understand which of the player has been a consistent performer across teams. Sachin Tendulkar made his test cricket debut in 1989 whereas Rahul Dravid, Sourav Ganguly and VVS Laxman made their debuts in the year 1996. Sourav Ganguly was the first to retire in 2008 followed by Rahul Dravid, VVS Laxman both of whom retired in the year 2012 and lastly Sachin Tendulkar retired in the year 2013.\nThe career summary of the four players under consideration can be seen in table below\n\n\n\nCareer details for players\n\n\nSince all the players have played more that 100 matches (and more that 150 innings), we can compare the batting averages for them against all the nine test playing nations. However, we will get a little adventurous and use a Bayesian approach to calculate the credible intervals for the batting average for each of the player.\nIn case you are wondering what a Bayesian method means, it is simply reconciling our belief (prior) with the data (likelihood) to obtain updated belief (posterior). And the credible interval gives us the plausible values for the batting average based on the data we have. Along with the average against each opponent, we are incorporating the variation in the scoring as well which is nothing but considering how consistently or inconsistently the runs were scored.\nWe will incorporate another layer of uncertainty in the sense that we let the parameter at the first level have their own uncertainty and assign a distribution of their own. This is done through a Bayesian hierarchical model\nThe Bayesian Hierarchical model we will use for each player is as follows\n\\[Y_{i} \\overset{\\mathrm{i.i.d}}\\sim N(\\mu_{opposition[i]}, \\sigma_{opposition[i]}^2)\\]\n\\[\\mu_{opposition} \\overset{ind}{\\sim} N(\\theta,\\tau^2)\\]\n\\[\\theta \\sim N(\\theta_{0}, \\tau_{0}^2)\\]\n\\[\\tau \\sim Ca^{+}(0,1)\\]\n\\[\\sigma_{opposition} \\sim Uniform(10,200)\\]\nwhere \\(\\theta_{0} = 50\\) and \\(\\tau_{0} = 10\\) has been used which is consistent with each of the player‚Äôs career average.\nThe model was run using a JAGS (Just Another Gibbs Sampler) which samples from the posterior distribution of the parameters. We will not go into much details about the individual parameters but use the parameters to come up with the credible intervals for the average runs scored per innings for all the players against each of the test playing nation. We than combine the credible intervals for all the players into a single plot which can be found below.\n\n\n\n\nCredible intervals for the average runs scored per inning against each of the opposition\n\n\nWe have the credible intervals for each of the four players against the nine test playing nations. Opposition is indicated on the x axis and the horizontal lines corresponding to any of the opposition is the credible interval for average against that particular opposition. We use different colors to indicate which player the interval corresponds to. Purple colour corresponds to intervals for Sachin Tendulkar, red colour corresponds to intervals for Rahul Dravid, green colour corresponds to intervals for Sourav Ganguly and blue colour corresponds to intervals for VVS Laxman.\nSince all the players have played against each of the opposition, we have intervals for all the players against all the opposition. However, all the players have batted less than 10 innings against Bangladesh and Zimbabwe and hence the intervals are not much meaningful.\nThere are two characteristics which can be understood looking at any of the interval. The first is the width of interval which indicates how consistent the player was against that particular opposition. If the width of the interval is big, that indicates the player scored runs inconsistently against the particular opposition. So we ideally want the interval to be as narrow as possible.\nThe second characteristic is the location of the interval which indicates the average of the player againt a particular opposition. If the average is higher, the location of the inteval will be towards the right and if the average is on the lower side the interval will lie towards the left side. Ideally we want a interval to be towards the right side.\nNow that we have understood how to read the intervals, let us start comparing the intervals for players against each of the opposition. In professional sports, every player seems to have a ‚Äòfavored‚Äô opposition against whom the player plays extraordinarily well which will be indicated in the plot by a narrow interval towards the right side. Similarly every player has an opposition who they do not enjoy playing which will be indicated in the plot by a wide interval and/or interval towards the left side.\nAgainst SENA countries\nFor teams in the Indian subcontinent, test cricket against SENA countries (South Africa, England, New Zealand and Australia) are considered an good indicator of how good a player is.\nAustralia was the most dominant opposition during the playing career of the four players. Australia tour of India in 2001 was a legendary series which was the turning point of Indian test cricket. The second test match in the series at Kolkata which India won while following on (an overwhelmingly difficult task) was nothing short of a legendary match. It is well know that Australia is VVS Laxman‚Äôs favoured opposition. However, if we look at the interval‚Äôs corresponding to Australia, we can see that Sachin Tendulkar has a better average than the other three players against Australia followed by VVS Laxman, Rahul Dravid and lastly Sourav Ganguly.\nAgainst New Zealand, Rahul Dravid seems to have the higher average followed by VVS Laxman, Sachin Tendulkar and lastly Sourav Ganguly.\nAgainst England, Rahul Dravid seems to have the higher average followed by Sourav Ganguly, Sachin Tendulkar and lastly VVS Laxman. VVS Laxman seems to have unusually low interval for average against England.\nAll the players seem to have performed below par against South Africa. Among the four players, Sachin Tendulkar seems to have performed relatively better than then the other three players who seems to have a similar record against South Africa.\nAgainst other countries\nAgainst Pakistan, Rahul Dravid has the highest average followed by Sourav Ganguly, Sachin Tendulkar and lastly VVS Laxman.\nAgainst Sri Lanka, Sachin Tendulkar has the highest average followed by Rahul Dravid, Sourav Ganguly and lastly VVS Laxman.\nAgainst West Indies, Rahul Dravid has the highest average followed by Sachin Tendulkar, VVS Laxman and lastly Sourav Ganguly. Sourav Ganguly has an unusually low interval for average against West Indies.\nAll the players have played less than 10 innings against Zimbabwe and Bangladesh and hence the scores have more variation in them which can be seen by the wide intervals. So comparing the intervals for the players against these two teams is not meaningful."
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "I like to read (mostly fiction). Some of the books I have enjoyed (not in a particular order) are as follows.\n\n\n\n\n\n\n\nBook\nAuthor\n\n\n\n\nNo Motive (short story)\nDaphne du Maurier\n\n\nAll She was Worth\nMiyuki Miyabe\n\n\nThe Devotion of Suspect X\nKeigo Higashino\n\n\nSalvation of a Saint\nKeigo Higashino\n\n\nA Midsummer‚Äôs Equation\nKeigo Higashino\n\n\nSilent Parade\nKeigo Higashino\n\n\nMalice\nKeigo Higashino\n\n\nNewcomer\nKeigo Higashino\n\n\nA Death in Tokyo\nKeigo Higashino\n\n\nThe Final Curtain\nKeigo Higashino\n\n\nInvisible Helix\nKeigo Higashino\n\n\nThe Name of the Game is a Kidnapping\nKeigo Higashino\n\n\nJourney Under the Midnight Sun\nKeigo Higashino\n\n\nNaoko\nKeigo Higashino\n\n\nSix-Four\nHideo Yokoyama\n\n\nAnimal Farm\nGeorge Orwell\n\n\nThe Good Earth\nPearl S. Buck\n\n\nThe Long Walk: The True Story Of A Trek To Freedom\nSlawomir Rawicz\n\n\nThe Seventh Secret\nIrving Wallace\n\n\nAs Far as My Feet Will Carry Me\nJosef Martin Bauer\n\n\nThe Harry Potter series\nJ. K. Rowling"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aditya Ranade",
    "section": "",
    "text": "Hello! My name is Aditya Ranade. I am a PhD candidate in the Department of Statistics, Iowa State University. My advisor is Dr.¬†Jarad Niemi. My research interest is application of Bayesian methods and my current work focuses on developing machine learning techniques for functional data. I also try to write about sports analytics during my free time."
  },
  {
    "objectID": "posts/cherry_blossom_prediction/index.html",
    "href": "posts/cherry_blossom_prediction/index.html",
    "title": "Cherry Blossom prediction using Time Series",
    "section": "",
    "text": "Cherry Blossom is one of the most scenic visuals one can experience. Cherry blossom season marks the arrival of spring season and can be considered as transition from winter to summer. People try to make plans travel to enjoy this phenomenon. So how about using some simple statistical techniques to try and forecast / predict the peak cherry blossom time ?\n\n\nAlong with some of my fellow PhD classmates, I participated in the International Cherry Blossom Prediction Competition hosted by George Mason university. We explored a lot of models and I am going to show a very basic model which I tried during the early stages. The model is the Autogegressive (AR) model. The notation of this model is AR(1) model is as follows\n\n\\[\nY_{t} = \\beta_{1} Y_{t-1} + \\epsilon_{t}\n\\] where $Y_{t}$ is the bloom day for year $t$, \\(\\beta_{i}\\) is the model parameter and \\(\\epsilon_{t}\\) is the white noise\n\nThis simply means the present value of the response variable $Y$ (in our case the bloom day for this year) is influenced by previous value of the response variable (in our case the bloom day of the previous year). If you are aware of the simple linear regression, think of this as the explanatory variable being the same as the predictor variable in rough sense. In the competition, we tried to predict the bloom date for multiple location across the world based on the data available provide by the university. However, for the purpose of this post, I will show the analysis only for one location, Kyoto in Japan.\n\n\nLet us start with first reading in the dataset and loading the R packages required for the analysis\n\n\n# Load the packages\nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(fpp2)\nlibrary(dplyr)\nlibrary(vars)\n\n# Load the dataset\nkyoto &lt;- read.csv(\"https://raw.githubusercontent.com/GMU-CherryBlossomCompetition/peak-bloom-prediction/main/data/kyoto.csv\",header=T)\n\n# Plot of the bloom date over the years\nggplot(kyoto,aes(x=year,y=bloom_doy))+\n  geom_point()+\n  labs(x=\"Year\",y=\"Bloom Day\")+\n  ggtitle(\"Bloom Day by Year\")\n\n\n\n\n\n\n\n\n\nAs we can see from the plot, towards the later end (in the recent past), the bloom day has started to go down. This means in the recent past, the bloom day is happening earlier than before. Let us look at the plot only from the year 1950.\n\n\n# Filter data only for year since 1951\nkyoto_new &lt;- kyoto %&gt;% filter(year&gt;1950)\n\n# Plot of the bloom date over the years\nggplot(kyoto_new,aes(x=year,y=bloom_doy))+\n  geom_point()+\n  labs(x=\"Year\",y=\"Bloom Day\")+\n  ggtitle(\"Bloom Day by Year\")\n\n\n\n\n\n\n\n\n\nAs we can see from the plot for year 1951 onward, there seems to be a downward trend which indicates the bloom date is in general arriving earlier.\n\n\nWe will use the data from 1951 to 2022 to predict the bloom date for year 2023 and compare that to actual bloom date. For this, we will use the bloom day as response and the year as the predictor\n\n\n# Prepare the data for ARIMA(1,0,0) model\ny_kyoto &lt;- kyoto_new$bloom_doy # bloom day as the response\n\nExclude the year 2023 from response and explanatory variable to test the model on year 2023\n\n# First test on 2023 model\nytest &lt;- y_kyoto[-length(y_kyoto)] # exclude the bloom day for 2023 year\n\n# Model based on year 1951 to 2022\nfit_kyoto_test &lt;- Arima(ytest, order=c(1,0,0)) # order=c(1,0,0) indicates AR(1) model\nfit_kyoto_test\n\nSeries: ytest \nARIMA(1,0,0) with non-zero mean \n\nCoefficients:\n         ar1     mean\n      0.2821  97.2885\ns.e.  0.1182   0.7483\n\nsigma^2 = 21.84:  log likelihood = -215.16\nAIC=436.32   AICc=436.67   BIC=443.19\n\n#Forecast\nfcast_kyoto_test &lt;- forecast(fit_kyoto_test)\nfcast_kyoto_test\n\n   Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n74       93.53975 87.55120  99.5283 84.38105 102.6984\n75       96.23094 90.00866 102.4532 86.71479 105.7471\n76       96.99013 90.74963 103.2306 87.44611 106.5342\n77       97.20430 90.96235 103.4463 87.65806 106.7505\n78       97.26472 91.02265 103.5068 87.71830 106.8111\n79       97.28176 91.03969 103.5238 87.73533 106.8282\n80       97.28657 91.04450 103.5286 87.74014 106.8330\n81       97.28793 91.04585 103.5300 87.74150 106.8344\n82       97.28831 91.04624 103.5304 87.74188 106.8347\n83       97.28842 91.04634 103.5305 87.74199 106.8349\n\n# Check actual bloom date for 2023\ny_kyoto[length(y_kyoto)] \n\n[1] 95\n\n\n\nThe AR(1) model predicts 96 as the bloom day for year 2023 whereas the actual bloom day was 84 for year 2023 which is a difference of 12 days. Considering its a basic model, this does not seem to be too bad.\n\n\nNow we check the performance of the model using some charts where we first check the prediction plot, then the Regression and model errors.\n\n\n# Plot the prediction\nautoplot(fcast_kyoto_test) + xlab(\"Year\") +\n  ylab(\"Percentage change\")\n\n\n\n\n\n\n\n# recover estimates of nu(t) and epsilon(t) \ncbind(\"Regression Errors\" = residuals(fit_kyoto_test, type=\"regression\"),\n      \"ARIMA errors\" = residuals(fit_kyoto_test, type=\"innovation\")) %&gt;%\n  autoplot(facets=TRUE)\n\n\n\n\n\n\n\n\n\nThere does not seem to be any issues with either of the plots. Now we check the residuals to see if they are normally distributed.\n\n\n# Check the residuals\ncheckresiduals(fit_kyoto_test)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,0) with non-zero mean\nQ* = 13.92, df = 9, p-value = 0.1252\n\nModel df: 1.   Total lags used: 10\n\n\n\nThe residuals seem to be normally distributed and the Ljung-Box test indicates we have little evidence against the null hypothesis of independently distributed errors.\n\n\nNow we will use the data from 1951 upto 2023 to predict the bloom date for the year 2024. Basically its the same model with one extra data point available\n\n\n# Now use data upto 2023 to predict 2024\nfit_kyoto &lt;- Arima(y_kyoto, order=c(1,0,0)) # order=c(1,0,0) indicates AR(1) model\nfit_kyoto\n\nSeries: y_kyoto \nARIMA(1,0,0) with non-zero mean \n\nCoefficients:\n         ar1     mean\n      0.2707  97.3162\ns.e.  0.1111   0.7264\n\nsigma^2 = 21.56:  log likelihood = -217.65\nAIC=441.3   AICc=441.64   BIC=448.21\n\n#Forecast\nfcast_kyoto &lt;- forecast(fit_kyoto)\nfcast_kyoto\n\n   Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n75       96.68923 90.73853 102.6399 87.58842 105.7900\n76       97.14647 90.98163 103.3113 87.71816 106.5748\n77       97.27023 91.08999 103.4505 87.81837 106.7221\n78       97.30373 91.12237 103.4851 87.85015 106.7573\n79       97.31280 91.13135 103.4943 87.85909 106.7665\n80       97.31526 91.13380 103.4967 87.86154 106.7690\n81       97.31592 91.13447 103.4974 87.86220 106.7696\n82       97.31610 91.13465 103.4976 87.86238 106.7698\n83       97.31615 91.13469 103.4976 87.86243 106.7699\n84       97.31616 91.13471 103.4976 87.86244 106.7699\n\n\n\nThe model predicts the cherry blossom to bloom on day 93 which is 2nd April 2024 (due to 2024 being a leap year) for Kyoto, Tokyo. Now lets look at the diagnostics of the model to see if the model is reasonable.\n\n\n# Plot the forecast\nautoplot(fcast_kyoto) + xlab(\"Year\") +\n  ylab(\"Percentage change\")\n\n\n\n\n\n\n\n# recover estimates of nu(t) and epsilon(t) \ncbind(\"Regression Errors\" = residuals(fit_kyoto, type=\"regression\"),\n      \"ARIMA errors\" = residuals(fit_kyoto, type=\"innovation\")) %&gt;%\n  autoplot(facets=TRUE)\n\n\n\n\n\n\n\n# Check the residuals\ncheckresiduals(fit_kyoto)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,0) with non-zero mean\nQ* = 13.9, df = 9, p-value = 0.1259\n\nModel df: 1.   Total lags used: 10\n\n\n\nAgain, the residuals seem to be normally distributed and the Ljung-Box test indicates we have little evidence against the null hypothesis of independently distributed errors."
  },
  {
    "objectID": "posts/ipl_best_team/index.html",
    "href": "posts/ipl_best_team/index.html",
    "title": "Does the best team win the Indian Premier League ?",
    "section": "",
    "text": "Indian Premier League (IPL) is an annual cricket league which is played under the T20 format. It was started in the year 2008 and has quickly gained popularity among cricket playing nations and has probably become one of the biggest sporting event in the Indian sports calendar.\nThe tournament is played between various teams based out of different cities in India. IPL is currently in its 14th season where the tournament has been played with 8 teams from 2008 - 2010 and 2014-2021. It was played with 10 teams in 2011 and 9 teams in 2012 and 2013. From 2022, it has been expanded again to 10 teams.\nThe tournament followed a double round-robin format from 2008 - 2021 (except for the year 2011) where each team plays the other team twice (once at each team‚Äôs ‚Äòhome‚Äô ground). From the year 2022, the teams are divided into two groups and every team plays 14 games (twice against the teams in the same group, twice against one team from other group and once against the other teams from the other group). At the conclusion of the league stage, the top four teams qualify for the playoffs. For the first 3 years (2008-2010), the traditional semifinals and final approach was considered. From 2011, a playoff structure was introduced to award the top two teams with an additional chance to reach the finals. This ensured the competition stays relevant till the last game of the league stage as teams vie for top 2 position. The figure below explains the playoff structure.\n\n\n\nIPL Playoff Structure\n\n\nThe top two teams of the league stage have two chances to reach the final. The thought behind this playoff structure was that the top two teams should not be out of the tournament due to one bad day in the semi-finals.\nA question that pops up in the mind is ‚ÄúDoes the best team win the tournament‚Äù. In other words, does the team which finishes the league stage at the top of the table have the best chance to win the league. The team which finishes the league stage position 1 has won the most number of points (and in case of tied points, has a superior net run rate) and hence is considered the ‚Äúbest‚Äù team in the league. We would be tempted to say yes. But let‚Äôs find out what the data says.\nWe have the data available for 11 years (2011 - 2021) for the league stage standings and the playoff results for the team topping the league stage from IPL official website which can be seen below\n\n\n\nTeam finishing first during league stage\n\n\nWe can observe, the team which tops the league stage has won the tournament only three 3 of 11 times which corresponds to approximately 27.27%. However, this is just the observation from 11 years so cannot be considered as the best indicator of probability. We will consider a Bayesian statistical model to calculate the probability of the team finishing the tournament as winner, runners up and third place.\nIn simple terms, a Bayesian model is reconciliation of our belief ( ‚Äòprior‚Äô distribution) with the observed data (likelihood) to give the updated belief (‚Äòposterior‚Äô distribution). Here the information (prior, likelihood and posterior) is in the form of mathematical distribution functions.\nIf we consider the random variable ‚Äòresult‚Äô which indicates the position of the team at the end of the tournament. The team which finishes the league stage at the top of the table can finish at any position from 1 to 3 at the end of the tournament where 1 stands for winner, 2 stands for runner‚Äôs up and 3 stands for 3rd place. Since the variable can take only 1 out of the 3 values, it follows a categorical distribution which is denoted as result ~ categorical(\\(p_{1}, p_{2}, p_{3}\\)) where \\(p_{1}\\), \\(p_{2}\\) and \\(p_{3}\\) indicates the probability of finishing at each of the position at the end of the tournament.\nWe will consider the probabilities as a random variable as well. We have to assign a distribution to the variable in such a way that our beliefs are reflected in the distribution. We will consider the team has an equal chance at finishing at each of the positions. This is ensured by assigning a Dirichlet(1,1,1) distribution to (\\(p_{1}\\), \\(p_{2}\\), \\(p_{3}\\))\nSo the model is as follows\n\\[result \\overset{\\mathrm{i.i.d.}}{\\sim} Categorical(p_{1}, p_{2}, p_{3}) \\]\n\\[(p_{1}, p_{2}, p_{3}) \\sim Dirichlet(\\alpha_{1}, \\alpha_{2}, \\alpha_{3}) \\]\nWe run this model on 11 years of the data available (2011 - 2021) and we get the credible interval for the posterior probabilities of finishing at each of the position\n\n\n\nCredible intervals for probabilities for team finishing first during league stage\n\n\nThe horizontal line indicates the plausible values for the probability of team finishing in a particular position and black point indicates the estimated value of the probability from the available data and model used.\nWe can see the probability of being runner up is higher than the probability of finishing as winner or 3rd place which seem to have almost the same probability. This means the team finishing at the top of the table at the end of the league will more often go on to lose in the playoffs (finish as runner up or \\(3^{rd}\\) place) at the end of the tournament.\nIsn‚Äôt it surprising? It surely is. Let us look at some of the reasons this might be the case. More often than not, the team which finishes at the top of the table during the league stage accumulates most of the points during the early stages and tends to lose steam towards the business end of the tournament. It might be a case of peaking too early. There might be some other psychological impact that might be worthwhile to study.\nSo probably the best team in the league (team finishing first during league stage) does not win the tournament.!\nLet us perform the same analysis for the team which finishes at second position in the league stage. We will use the same model used for the team finishing first at the league stage. Using data from 11 years, we get the following plot\n\n\n\nCredible intervals for probabilities for team finishing second during league stage\n\n\nWe can see the probability of being winner is higher than the probability of finishing as runner‚Äôs up or 3rd place. This means the team finishing second in the league stage will more often go on to win the tournament. Surely a surprising and fascinating result!"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Good things take time! Something exciting is coming up soon! Keep calm and work hard!üôÇ"
  },
  {
    "objectID": "posts/starbucks/index.html",
    "href": "posts/starbucks/index.html",
    "title": "Starbucks beverages nutritional information",
    "section": "",
    "text": "Starbucks is one of the most valued coffee chain in the world. A lot of people like to consume the beverages available at starbucks. But how good are they in terms of the nutritional value?\n\n\nI found this dataset on Kaggle which gives the nutritional information about their beverages. We will look at the exploratory data analysis first and later try some simple prediction models. First let us access and process the data through R.\n\n\n# Load the packages\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(car) # to calculate the VIF values\nlibrary(GGally) # for pairs plot using ggplot framework\n\n\n# Get starbucks data from github repo\npath &lt;- \"https://raw.githubusercontent.com/adityaranade/starbucks/refs/heads/main/data/starbucks-menu-nutrition-drinks.csv\"\ndata0 &lt;- read.csv(path, header = TRUE)\n\n# Data processing\n# change the column names\ncolnames(data0) &lt;- c(\"name\", \"calories\", \"fat\", \n                     \"carbs\", \"fiber\",\"protein\", \n                     \"sodium\")\n\n# Check the first 6 rows of the dataset\ndata0 |&gt; head()\n\n                                                name calories fat carbs fiber\n1           Cool Lime Starbucks Refreshers‚Ñ¢ Beverage       45   0    11     0\n2                                   Ombr√© Pink Drink        -   -     -     -\n3                                         Pink Drink        -   -     -     -\n4     Strawberry Acai Starbucks Refreshers‚Ñ¢ Beverage       80   0    18     1\n5 Very Berry Hibiscus Starbucks Refreshers‚Ñ¢ Beverage       60   0    14     1\n6                                       Violet Drink        -   -     -     -\n  protein sodium\n1       0     10\n2       -      -\n3       -      -\n4       0     10\n5       0     10\n6       -      -\n\n# Check the type of data\ndata0 |&gt; str()\n\n'data.frame':   177 obs. of  7 variables:\n $ name    : chr  \"Cool Lime Starbucks Refreshers‚Ñ¢ Beverage\" \"Ombr√© Pink Drink\" \"Pink Drink\" \"Strawberry Acai Starbucks Refreshers‚Ñ¢ Beverage\" ...\n $ calories: chr  \"45\" \"-\" \"-\" \"80\" ...\n $ fat     : chr  \"0\" \"-\" \"-\" \"0\" ...\n $ carbs   : chr  \"11\" \"-\" \"-\" \"18\" ...\n $ fiber   : chr  \"0\" \"-\" \"-\" \"1\" ...\n $ protein : chr  \"0\" \"-\" \"-\" \"0\" ...\n $ sodium  : chr  \"10\" \"-\" \"-\" \"10\" ...\n\n\n\nThe data from second column should be numeric but shows as character. So we first convert it into numeric form and also exclude the rows with missing information\n\n\n# convert the data to numeric second row onwards\ndata0$calories &lt;- as.numeric(data0$calories)\ndata0$fat &lt;- as.numeric(data0$fat)\ndata0$carbs &lt;- as.numeric(data0$carbs)\ndata0$fiber &lt;- as.numeric(data0$fiber)\ndata0$protein &lt;- as.numeric(data0$protein)\ndata0$sodium &lt;- as.numeric(data0$sodium)\n\n# Check the type of data again\ndata0 |&gt; str()\n\n'data.frame':   177 obs. of  7 variables:\n $ name    : chr  \"Cool Lime Starbucks Refreshers‚Ñ¢ Beverage\" \"Ombr√© Pink Drink\" \"Pink Drink\" \"Strawberry Acai Starbucks Refreshers‚Ñ¢ Beverage\" ...\n $ calories: num  45 NA NA 80 60 NA NA NA 110 0 ...\n $ fat     : num  0 NA NA 0 0 NA NA NA 0 0 ...\n $ carbs   : num  11 NA NA 18 14 NA NA NA 28 0 ...\n $ fiber   : num  0 NA NA 1 1 NA NA NA 0 0 ...\n $ protein : num  0 NA NA 0 0 NA NA NA 0 0 ...\n $ sodium  : num  10 NA NA 10 10 NA NA NA 5 0 ...\n\n# Check the rows which do not have any entries\nind.na &lt;- which(is.na(data0[,2]))\nlength(ind.na) # 85 NA values\n\n[1] 85\n\n# exclude the rows which has NA values \ndata &lt;- data0[-ind.na,]\n\n\nNow that we have the data ready, let us look at the histogram each of the variables namely calories, fat, carbs, fiber, protein and sodium\n\n\n# Data for histogram\nmelted_data &lt;- melt(data, id.vars=\"name\")\n\n# Plot the histogram of all the variables\nggplot(melted_data,aes(value))+\n  # geom_histogram(aes(y = after_stat(density)),bins = 20)+\n  geom_histogram(bins = 20)+\n  facet_grid2(~variable, scales=\"free\")\n\n\n\n\n\n\n\n\n\nHistogram does not give much information. Let us look at the correlation plot to get an idea of how the variables are correlated with each other.\n\n\n# correlation plot of all the variables\ncorr &lt;- round(cor(data[,-1]), 1)\np.mat &lt;- cor_pmat(mtcars) # correlation p-value\n# Barring the no significant coefficient\nggcorrplot(corr, hc.order = TRUE,\n           type = \"lower\", p.mat = p.mat)\n\n\n\n\n\n\n\n# All positive correlation\n\n\nAll the variables are positively correlated (which indicates when one variable increases, the other variable will increase as well. ) which is not a surprising. Most important part is the correlation of calories with all the other variables are considerably high. Next we look at the pairs plot which will show the bivariate scatter plots as well as the correlation between each variables.\n\n\nggpairs(data,columns = 2:ncol(data),\n        lower = list(continuous = \"smooth\"))\n\n\n\n\n\n\n\n\n\nMost of the bivariate scatter plots indicate a linear relationship between the variables. The most important result according to us is the relationship between calories with all the other variables. We can now use the dataset for predictions where we try to predict the calories based o the fat, carb, fiber, protein and sodium content using multiple linear regression.\n\n\n# split the data into training and testing data\nseed &lt;- 23\nset.seed(seed)\n\nind &lt;- sample(floor(0.8*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train &lt;- data[ind,-1]\n# Testing dataset\ndata_test &lt;- data[-ind,-1]\n\n# Multiple linear regression using raw data\nmodel &lt;- lm(calories ~ fat + carbs + fiber + protein + sodium, data = data_train)\nsummary(model)\n\n\nCall:\nlm(formula = calories ~ fat + carbs + fiber + protein + sodium, \n    data = data_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.2823  -1.7523   0.0195   2.1092   8.1259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.75158    0.94777   1.848   0.0690 .  \nfat          7.65711    0.25133  30.466  &lt; 2e-16 ***\ncarbs        3.79744    0.04057  93.599  &lt; 2e-16 ***\nfiber        2.51563    0.96443   2.608   0.0112 *  \nprotein      0.75585    0.31682   2.386   0.0199 *  \nsodium       0.32290    0.03265   9.889 1.01e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.131 on 67 degrees of freedom\nMultiple R-squared:  0.9976,    Adjusted R-squared:  0.9974 \nF-statistic:  5493 on 5 and 67 DF,  p-value: &lt; 2.2e-16\n\n# Prediction on the testing dataset\ny_pred &lt;- predict(model, data_test)\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred,data_test$calories))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+geom_abline()\n\n\n\n\n\n\n\n# Calculate RMSE\nrmse &lt;- (y_pred-data_test$calories)^2 |&gt; sum() |&gt; sqrt()\nrmse\n\n[1] 102.3342\n\n# Check the variance inflation factor\nvif_values &lt;- vif(model)\nvif_values\n\n      fat     carbs     fiber   protein    sodium \n 3.849325  1.266592  3.382455 10.380709 11.704360 \n\n# Check the assumptions of the regression model\n# par(mfrow = c(2, 2))\n# plot(model)\n\n\nThe model is decent with RMSE 102.33 and the observed vs.¬†predicted plot also looks decent. However the variation inflation factor (VIF) value for protein and sodium is higher than 10 which indicates that these two variables are highly correlated with at least one other input variable and hence the variation of these variables is inflated. This might lead to unreliable models. One way to mitigate the multicollinearity problem is to use principal components in place of the correlated variables.\n\n\nWe will create principal components and look how much variation is explained by each of the principal components.\n\n\npc &lt;- prcomp(data[,-(1:2)],\n             center = TRUE,\n             scale. = TRUE)\nattributes(pc)\n\n$names\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n$class\n[1] \"prcomp\"\n\n# Check the factor loading of the principal components\nprint(pc)\n\nStandard deviations (1, .., p=5):\n[1] 1.7690827 0.8908878 0.7580688 0.5812294 0.4051785\n\nRotation (n x k) = (5 x 5):\n              PC1        PC2        PC3        PC4         PC5\nfat     0.4610104  0.3810085  0.1092277  0.7933177  0.03190861\ncarbs   0.3784887 -0.5448572 -0.7329252  0.1443775 -0.04304364\nfiber   0.3758452 -0.6263993  0.6433047 -0.0153384  0.22866603\nprotein 0.5161622  0.1647649  0.1161063 -0.3649765 -0.74815813\nsodium  0.4863462  0.3720747 -0.1535201 -0.4651440  0.62056454\n\n# Check the summary of the principal components\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5\nStandard deviation     1.7691 0.8909 0.7581 0.58123 0.40518\nProportion of Variance 0.6259 0.1587 0.1149 0.06757 0.03283\nCumulative Proportion  0.6259 0.7847 0.8996 0.96717 1.00000\n\n\n\nThe first four principal components explain around 96.72 % of the variation in the data. We will use the first four principal components for the regression model.\n\n\ndata_pc &lt;- cbind(data[,1:2],pc$x)\n# training data\ndata_pc_train &lt;- data_pc[ind,-1]\n# testing data\ndata_pc_test &lt;- data_pc[-ind,-1]\n\n# Multiple linear regression using PC\nmodel_pc &lt;- lm(calories ~ PC1 + PC2 + PC3 + PC4, data = data_pc_train)\nsummary(model_pc)\n\n\nCall:\nlm(formula = calories ~ PC1 + PC2 + PC3 + PC4, data = data_pc_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3780  -1.5028   0.7182   2.2345   7.4206 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 135.7766     0.5642  240.67   &lt;2e-16 ***\nPC1          49.2547     0.3746  131.47   &lt;2e-16 ***\nPC2         -12.8205     0.9443  -13.58   &lt;2e-16 ***\nPC3         -40.1362     0.9209  -43.59   &lt;2e-16 ***\nPC4          22.9143     0.9862   23.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.442 on 68 degrees of freedom\nMultiple R-squared:  0.9971,    Adjusted R-squared:  0.997 \nF-statistic:  5935 on 4 and 68 DF,  p-value: &lt; 2.2e-16\n\n# Prediction on the testing dataset\ny_pred_pc &lt;- predict(model_pc, data_pc_test)\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred_pc,data_test$calories))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+geom_abline()\n\n\n\n\n\n\n\n# Calculate RMSE\nrmse &lt;- (y_pred_pc-data_pc_test$calories)^2 |&gt; sum() |&gt; sqrt()\nrmse\n\n[1] 63.72554\n\n# Check the variance inflation factor\nvif_values_pc &lt;- vif(model_pc)\nvif_values_pc\n\n     PC1      PC2      PC3      PC4 \n1.129946 1.471886 1.373721 1.160650 \n\n# Check the assumptions of the regression model\n# par(mfrow = c(2, 2))\n# plot(model_pc)\n\n\nRMSE for the regression model using the first four principal components is 63.73. The variation inflation factor is less than 1.5 for all the principal components. So using less variables with principal components gives much better predictions."
  },
  {
    "objectID": "posts/starbucks_food/index.html",
    "href": "posts/starbucks_food/index.html",
    "title": "Starbucks food nutritional information",
    "section": "",
    "text": "Starbucks is one of the most valued coffee chain in the world. A lot of people like to consume the food available at starbucks. But how good are they in terms of the nutritional value?\n\n\nI found this dataset on Kaggle which gives the nutritional information about their food products. In my precious post, I built a multiple linear regression model to predict the calories in beverage based on the nutritional contents of the beverage. Now we will try to do the same for the food products.\nFirst, we look at the exploratory data analysis and later try some simple regression models. First let us access and process the data through R.\n\n\n# Load the packages\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(car) # to calculate the VIF values\nlibrary(GGally) # for pairs plot using ggplot framework\n\n\n# Get starbucks data from github repo\npath &lt;- \"https://raw.githubusercontent.com/adityaranade/starbucks/refs/heads/main/data/starbucks-menu-nutrition-food.csv\"\ndata0 &lt;- read.csv(path, header = TRUE)\n\n# Data processing\n# change the column names\ncolnames(data0) &lt;- c(\"name\", \"calories\", \"fat\", \n                     \"carbs\", \"fiber\",\"protein\")\n\n# Check the first 6 rows of the dataset\ndata0 |&gt; head()\n\n                                    name calories fat carbs fiber protein\n1                           Chonga Bagel      300   5    50     3      12\n2                           8-Grain Roll      380   6    70     7      10\n3                       Almond Croissant      410  22    45     3      10\n4                          Apple Fritter      460  23    56     2       7\n5                       Banana Nut Bread      420  22    52     2       6\n6 Blueberry Muffin with Yogurt and Honey      380  16    53     1       6\n\n# Check the type of data\ndata0 |&gt; str()\n\n'data.frame':   113 obs. of  6 variables:\n $ name    : chr  \"Chonga Bagel\" \"8-Grain Roll\" \"Almond Croissant\" \"Apple Fritter\" ...\n $ calories: int  300 380 410 460 420 380 420 240 350 320 ...\n $ fat     : num  5 6 22 23 22 16 17 12 22 16 ...\n $ carbs   : int  50 70 45 56 52 53 61 28 38 36 ...\n $ fiber   : int  3 7 3 2 2 1 2 1 0 1 ...\n $ protein : int  12 10 10 7 6 6 5 5 2 8 ...\n\n\n\nThe data from second column should be numeric but shows as character. So we first convert it into numeric form and also exclude the rows with missing information\n\n\n# convert the data to numeric second row onwards\ndata0$calories &lt;- as.numeric(data0$calories)\ndata0$fat &lt;- as.numeric(data0$fat)\ndata0$carbs &lt;- as.numeric(data0$carbs)\ndata0$fiber &lt;- as.numeric(data0$fiber)\ndata0$protein &lt;- as.numeric(data0$protein)\n\n# Check the type of data again\ndata0 |&gt; str()\n\n'data.frame':   113 obs. of  6 variables:\n $ name    : chr  \"Chonga Bagel\" \"8-Grain Roll\" \"Almond Croissant\" \"Apple Fritter\" ...\n $ calories: num  300 380 410 460 420 380 420 240 350 320 ...\n $ fat     : num  5 6 22 23 22 16 17 12 22 16 ...\n $ carbs   : num  50 70 45 56 52 53 61 28 38 36 ...\n $ fiber   : num  3 7 3 2 2 1 2 1 0 1 ...\n $ protein : num  12 10 10 7 6 6 5 5 2 8 ...\n\n# Check the rows which do not have any entries\nind.na &lt;- which(is.na(data0[,2]))\nlength(ind.na) # 0 NA values\n\n[1] 0\n\ndata &lt;- data0\n\n\nNow that we have the data ready, let us look at the histogram each of the variables namely calories, fat, carbs, fiber, protein and sodium\n\n\n# Data for histogram\nmelted_data &lt;- melt(data, id.vars=\"name\")\n\n# Plot the histogram of all the variables\nggplot(melted_data,aes(value))+\n  geom_histogram(bins = 20)+\n  facet_grid2(~variable, scales=\"free\")\n\n\n\n\n\n\n\n\n\nHistogram does not give much information. Let us look at the correlation plot to get an idea of how the variables are correlated with each other.\n\n\n# correlation plot of all the variables\ncorr &lt;- round(cor(data[,-1]), 1)\np.mat &lt;- cor_pmat(mtcars) # correlation p-value\n# Barring the no significant coefficient\nggcorrplot(corr, hc.order = TRUE,\n           type = \"lower\", p.mat = p.mat)\n\n\n\n\n\n\n\n# All positive correlation\n\n\nAll the variables are positively correlated (which indicates when one variable increases, the other variable will increase as well. ) which is not a surprising. Most important part is the correlation of calories with all the other variables are considerably high. Next we look at the pairs plot which will show the bivariate scatter plots as well as the correlation between each variables.\n\n\nggpairs(data,columns = 2:ncol(data),\n        lower = list(continuous = \"smooth\"))\n\n\n\n\n\n\n\n\n\nMost of the bivariate scatter plots indicate a linear relationship between the variables. The most important result according to us is the relationship between calories with all the other variables. We can now use the dataset for predictions where we try to predict the calories based on the fat, carb, fiber and protein content using multiple linear regression.\n\n\n# split the data into training and testing data\nseed &lt;- 23\nset.seed(seed)\n\nind &lt;- sample(floor(0.8*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train &lt;- data[ind,-1]\n# Testing dataset\ndata_test &lt;- data[-ind,-1]\n\n# Multiple linear regression using raw data\nmodel &lt;- lm(calories ~ fat + carbs + fiber + protein , data = data_train)\nsummary(model)\n\n\nCall:\nlm(formula = calories ~ fat + carbs + fiber + protein, data = data_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.675  -4.029  -0.109   3.102  27.055 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.14909    2.73597   0.054   0.9567    \nfat          8.80325    0.10889  80.842   &lt;2e-16 ***\ncarbs        4.01725    0.06185  64.955   &lt;2e-16 ***\nfiber       -0.90654    0.46687  -1.942   0.0555 .  \nprotein      4.16692    0.14242  29.258   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.6 on 85 degrees of freedom\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.996 \nF-statistic:  5508 on 4 and 85 DF,  p-value: &lt; 2.2e-16\n\n# Prediction on the testing dataset\ny_pred &lt;- predict(model, data_test)\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred,data_test$calories))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+geom_abline()\n\n\n\n\n\n\n\n# Calculate RMSE\nrmse &lt;- (y_pred-data_test$calories)^2 |&gt; sum() |&gt; sqrt()\nrmse\n\n[1] 70.16698\n\n# Check the variance inflation factor\nvif_values &lt;- vif(model)\nvif_values\n\n     fat    carbs    fiber  protein \n1.237161 1.298276 1.880520 1.762035 \n\n\n\nThe model is decent with RMSE 70.17 and the observed vs.¬†predicted plot also looks decent with all the points just around the line. The variation inflation factor (VIF) is also below 2 for all the variables. We will look at the residual plots to check if all the assumptions of multiple linear regression are satisfied.\n\n\n# Check the assumptions of the regression model\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\nNothing in the residual plots indicate a cause of concern regarding the model."
  },
  {
    "objectID": "posts/starbucks_food_python/index.html",
    "href": "posts/starbucks_food_python/index.html",
    "title": "Starbucks food nutritional information",
    "section": "",
    "text": "Starbucks is one of the most valued coffee chain in the world. A lot of people like to consume the food available at starbucks. But how good are they in terms of the nutritional value?\n\n\nI found this dataset on Kaggle which gives the nutritional information about their food products. In my precious post, I built a multiple linear regression model to predict the calories in beverage based on the nutritional contents of the beverage. Now we will try to do the same for the food products.\nFirst, we look at the exploratory data analysis and later try some simple regression models. First let us access and process the data through python\n\n\n# Load Libraries\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom plotnine import * # for plots\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nimport random\nfrom scipy.stats import pearsonr\n\n# Get starbucks data from github repo\ndf0=pd.read_csv(\"https://raw.githubusercontent.com//adityaranade//starbucks//refs//heads//main//data//starbucks-menu-nutrition-food.csv\", encoding='unicode_escape')\ndf0.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nCalories\nFat (g)\nCarb. (g)\nFiber (g)\nProtein (g)\n\n\n\n\n0\nChonga Bagel\n300\n5.0\n50\n3\n12\n\n\n1\n8-Grain Roll\n380\n6.0\n70\n7\n10\n\n\n2\nAlmond Croissant\n410\n22.0\n45\n3\n10\n\n\n3\nApple Fritter\n460\n23.0\n56\n2\n7\n\n\n4\nBanana Nut Bread\n420\n22.0\n52\n2\n6\n\n\n\n\n\n\n\n\n#modify the column names\ndf0.columns = ['name', 'calories','fat','carbs','fiber','protein']\ndf0.head()\n\n#convert data type to float for all the columns except name\nfor i in df0.columns[1:]:\n    df0[i]=df0[i].astype(\"float\")\n# df0.info()\n\n\ndf = df0\n# Use melt function for the histograms\ndf2 = pd.melt(df, id_vars=['name'])\n# df2.head()\n\n\nNow that we have the data ready, let us look at the histogram of each variables namely calories, fat, carbs, fiber, protein and sodium\n\n\np = (\n    ggplot(df2, aes(\"value\"))\n    + geom_histogram(bins=15)\n    + facet_grid(\". ~ variable\", scales='free_x')\n    )\n\np.show()\n\n\n\n\n\n\n\n\nThe histogram of each of the variables do not show any problems as all the plots look decent. We will look at the correlation plot.\n\n# Check the correlation between the variables\n# plt.figure(figsize=(20,7))\nsns.heatmap(df.iloc[:,1:].corr(),annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nCorrelation plot indicates positive association between all the variables which is desired. Now we will look at the pairs plot which will show the pairwise histogram.\n\n\n# Pairs plot\ng = sns.PairGrid(df.iloc[:,1:])\ng.map_diag(sns.histplot)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.scatterplot)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe scatterplots of each variable with calories which can be seen in the upper triangular plots in the very first row. It seems there is a linear association between calories and fat, carbs and protein. However, it does not seem to have a linear association with fiber.\n\n\n# Split data into train and test set\nindices = range(len(df)) # Create a list of indices\n\n# Get 75% random indices for training data\nrandom.seed(23) # for repreducible example\nrandom_indices = random.sample(indices, round(0.75*len(df)))\n\n# Training dataset\ndata_train = df.iloc[random_indices,]\n\n# Testing dataset\ndata_test = df.iloc[df.index.difference(random_indices),]\n\n\n# Build a multiple linear regression model to predict calories using other variables using training data\nresult = smf.ols(\"calories ~ fat + carbs + fiber + protein\", data = data_train).fit()\n# check the summary\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ncalories\nR-squared:\n0.995\n\n\nModel:\nOLS\nAdj. R-squared:\n0.995\n\n\nMethod:\nLeast Squares\nF-statistic:\n4351.\n\n\nDate:\nSun, 11 May 2025\nProb (F-statistic):\n1.06e-92\n\n\nTime:\n08:26:44\nLog-Likelihood:\n-304.54\n\n\nNo. Observations:\n85\nAIC:\n619.1\n\n\nDf Residuals:\n80\nBIC:\n631.3\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-2.8394\n3.006\n-0.945\n0.348\n-8.821\n3.142\n\n\nfat\n8.8760\n0.129\n68.933\n0.000\n8.620\n9.132\n\n\ncarbs\n4.0291\n0.066\n61.478\n0.000\n3.899\n4.160\n\n\nfiber\n-1.1151\n0.402\n-2.772\n0.007\n-1.916\n-0.315\n\n\nprotein\n4.3474\n0.156\n27.915\n0.000\n4.037\n4.657\n\n\n\n\n\n\n\n\nOmnibus:\n8.990\nDurbin-Watson:\n2.042\n\n\nProb(Omnibus):\n0.011\nJarque-Bera (JB):\n12.887\n\n\nSkew:\n0.426\nProb(JB):\n0.00159\n\n\nKurtosis:\n4.707\nCond. No.\n151.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nNow let us make prediction on the testing data and plot the observed vs.¬†predicted plot\n\n\n# Make predictions using testing data\npredictions = result.predict(data_test)\n\n# Observed vs. Predicted plot\nplt.figure(figsize=(20,7))\nsns.regplot(y = data_test[\"calories\"],x = predictions,line_kws={\"color\":\"red\"})\nplt.ylabel(\"Observed calories\")\nplt.xlabel(\"Predicted calories\")\nplt.show()\n# decent plot\n\n\n\n\n\n\n\n\n\nThe observed vs.¬†predicted looks good. However there is low number of data points and hence we should take this with a grain of salt. Let us check some evaluation metrics like the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nprint(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"calories\"],predictions),2))\nprint(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"calories\"],predictions))** 0.5,2))\n\nMean Absolute Error: 7.54\nRoot Mean Squared Error: 10.7\n\n\n\nRoot Mean Squared Error (RMSE) of 7.54 and Mean Absolute Error (MAE) of 10.7 is decent and indicates model is performing fairly well."
  },
  {
    "objectID": "posts/test/test.html",
    "href": "posts/test/test.html",
    "title": "Reproducible Quarto Document",
    "section": "",
    "text": "This is a reproducible Quarto document using format: html. It is written in Markdown and contains embedded R code. When you run the code, it will produce a plot.\n\nplot(cars)\n\n\n\n\n\n\n\n\nquarto check"
  },
  {
    "objectID": "posts/test/index.html",
    "href": "posts/test/index.html",
    "title": "check env",
    "section": "",
    "text": "QUARTO_PYTHON is set to ?env:QUARTO_PYTHON"
  },
  {
    "objectID": "posts/stpy/index.html",
    "href": "posts/stpy/index.html",
    "title": "matplotlib demo",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure¬†1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: A line plot on a polar axis"
  },
  {
    "objectID": "archives/starbucks_food/index0.html",
    "href": "archives/starbucks_food/index0.html",
    "title": "Starbucks food nutritional information",
    "section": "",
    "text": "Starbucks is one of the most valued coffee chain in the world. A lot of people like to consume the food available at starbucks. But how good are they in terms of the nutritional value?\n\n\nI found this dataset on Kaggle which gives the nutritional information about their food products. In my precious post, I built a multiple linear regression model to predict the calories in beverage based on the nutritional contents of the beverage. Now we will try to do the same for the food products.\nFirst, we look at the exploratory data analysis and later try some simple regression models. First let us access and process the data through R.\n\n\n# Load the packages\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(car) # to calculate the VIF values\nlibrary(GGally) # for pairs plot using ggplot framework\n\n\n# Get starbucks data from github repo\npath &lt;- \"https://raw.githubusercontent.com/adityaranade/starbucks/refs/heads/main/data/starbucks-menu-nutrition-food.csv\"\ndata0 &lt;- read.csv(path, header = TRUE)\n\n# Data processing\n# change the column names\ncolnames(data0) &lt;- c(\"name\", \"calories\", \"fat\", \n                     \"carbs\", \"fiber\",\"protein\")\n\n# Check the first 6 rows of the dataset\ndata0 |&gt; head()\n\n                                    name calories fat carbs fiber protein\n1                           Chonga Bagel      300   5    50     3      12\n2                           8-Grain Roll      380   6    70     7      10\n3                       Almond Croissant      410  22    45     3      10\n4                          Apple Fritter      460  23    56     2       7\n5                       Banana Nut Bread      420  22    52     2       6\n6 Blueberry Muffin with Yogurt and Honey      380  16    53     1       6\n\n# Check the type of data\ndata0 |&gt; str()\n\n'data.frame':   113 obs. of  6 variables:\n $ name    : chr  \"Chonga Bagel\" \"8-Grain Roll\" \"Almond Croissant\" \"Apple Fritter\" ...\n $ calories: int  300 380 410 460 420 380 420 240 350 320 ...\n $ fat     : num  5 6 22 23 22 16 17 12 22 16 ...\n $ carbs   : int  50 70 45 56 52 53 61 28 38 36 ...\n $ fiber   : int  3 7 3 2 2 1 2 1 0 1 ...\n $ protein : int  12 10 10 7 6 6 5 5 2 8 ...\n\n\n\nThe data from second column should be numeric but shows as character. So we first convert it into numeric form and also exclude the rows with missing information\n\n\n# convert the data to numeric second row onwards\ndata0$calories &lt;- as.numeric(data0$calories)\ndata0$fat &lt;- as.numeric(data0$fat)\ndata0$carbs &lt;- as.numeric(data0$carbs)\ndata0$fiber &lt;- as.numeric(data0$fiber)\ndata0$protein &lt;- as.numeric(data0$protein)\n\n# Check the type of data again\ndata0 |&gt; str()\n\n'data.frame':   113 obs. of  6 variables:\n $ name    : chr  \"Chonga Bagel\" \"8-Grain Roll\" \"Almond Croissant\" \"Apple Fritter\" ...\n $ calories: num  300 380 410 460 420 380 420 240 350 320 ...\n $ fat     : num  5 6 22 23 22 16 17 12 22 16 ...\n $ carbs   : num  50 70 45 56 52 53 61 28 38 36 ...\n $ fiber   : num  3 7 3 2 2 1 2 1 0 1 ...\n $ protein : num  12 10 10 7 6 6 5 5 2 8 ...\n\n# Check the rows which do not have any entries\nind.na &lt;- which(is.na(data0[,2]))\nlength(ind.na) # 0 NA values\n\n[1] 0\n\ndata &lt;- data0\n\n\nNow that we have the data ready, let us look at the histogram each of the variables namely calories, fat, carbs, fiber, protein and sodium\n\n\n# Data for histogram\nmelted_data &lt;- melt(data, id.vars=\"name\")\n\n# Plot the histogram of all the variables\nggplot(melted_data,aes(value))+\n  geom_histogram(bins = 20)+\n  facet_grid2(~variable, scales=\"free\")\n\n\n\n\n\n\n\n\n\nHistogram does not give much information. Let us look at the correlation plot to get an idea of how the variables are correlated with each other.\n\n\n# correlation plot of all the variables\ncorr &lt;- round(cor(data[,-1]), 1)\np.mat &lt;- cor_pmat(mtcars) # correlation p-value\n# Barring the no significant coefficient\nggcorrplot(corr, hc.order = TRUE,\n           type = \"lower\", p.mat = p.mat)\n\n\n\n\n\n\n\n# All positive correlation\n\n\nAll the variables are positively correlated (which indicates when one variable increases, the other variable will increase as well. ) which is not a surprising. Most important part is the correlation of calories with all the other variables are considerably high. Next we look at the pairs plot which will show the bivariate scatter plots as well as the correlation between each variables.\n\n\nggpairs(data,columns = 2:ncol(data),\n        lower = list(continuous = \"smooth\"))\n\n\n\n\n\n\n\n\n\nMost of the bivariate scatter plots indicate a linear relationship between the variables. The most important result according to us is the relationship between calories with all the other variables. We can now use the dataset for predictions where we try to predict the calories based on the fat, carb, fiber and protein content using multiple linear regression.\n\n\n# split the data into training and testing data\nseed &lt;- 23\nset.seed(seed)\n\nind &lt;- sample(floor(0.8*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train &lt;- data[ind,-1]\n# Testing dataset\ndata_test &lt;- data[-ind,-1]\n\n# Multiple linear regression using raw data\nmodel &lt;- lm(calories ~ fat + carbs + fiber + protein , data = data_train)\nsummary(model)\n\n\nCall:\nlm(formula = calories ~ fat + carbs + fiber + protein, data = data_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.675  -4.029  -0.109   3.102  27.055 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.14909    2.73597   0.054   0.9567    \nfat          8.80325    0.10889  80.842   &lt;2e-16 ***\ncarbs        4.01725    0.06185  64.955   &lt;2e-16 ***\nfiber       -0.90654    0.46687  -1.942   0.0555 .  \nprotein      4.16692    0.14242  29.258   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.6 on 85 degrees of freedom\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.996 \nF-statistic:  5508 on 4 and 85 DF,  p-value: &lt; 2.2e-16\n\n# Prediction on the testing dataset\ny_pred &lt;- predict(model, data_test)\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred,data_test$calories))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+geom_abline()\n\n\n\n\n\n\n\n# Calculate RMSE\nrmse &lt;- (y_pred-data_test$calories)^2 |&gt; sum() |&gt; sqrt()\nrmse\n\n[1] 70.16698\n\n# Check the variance inflation factor\nvif_values &lt;- vif(model)\nvif_values\n\n     fat    carbs    fiber  protein \n1.237161 1.298276 1.880520 1.762035 \n\n\n\nThe model is decent with RMSE 70.17 and the observed vs.¬†predicted plot also looks decent with all the points just around the line. The variation inflation factor (VIF) is also below 2 for all the variables. We will look at the residual plots to check if all the assumptions of multiple linear regression are satisfied.\n\n\n# Check the assumptions of the regression model\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\nNothing in the residual plots indicate a cause of concern regarding the model."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Credit approval\n\n\n\nanalysis\n\n\nR\n\n\n\n\n\n\n\nAditya Ranade\n\n\nMar 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive model for house price using Gaussian Process regression\n\n\n\nanalysis\n\n\nR\n\n\n\n\n\n\n\nAditya Ranade\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRating of cereals based on nutritional information\n\n\n\nanalysis\n\n\npython\n\n\n\n\n\n\n\nAditya Ranade\n\n\nFeb 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCereals nutritional information\n\n\n\nanalysis\n\n\npython\n\n\n\n\n\n\n\nAditya Ranade\n\n\nFeb 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarbucks food nutritional information\n\n\n\nanalysis\n\n\npython\n\n\n\n\n\n\n\nAditya Ranade\n\n\nJan 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarbucks beverages nutritional information\n\n\n\nanalysis\n\n\nR\n\n\n\n\n\n\n\nAditya Ranade\n\n\nJan 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCherry Blossom prediction using Time Series\n\n\n\nanalysis\n\n\nR\n\n\n\n\n\n\n\nAditya Ranade\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoes the best team win the Indian Premier League ?\n\n\n\nanalysis\n\n\n\n\n\n\n\nAditya Ranade\n\n\nNov 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing batting averages of the fabulous four players of Indian test cricket by opposition\n\n\n\nanalysis\n\n\n\n\n\n\n\nAditya Ranade\n\n\nOct 30, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/starbucks/index.html",
    "href": "portfolio/starbucks/index.html",
    "title": "Starbucks beverages nutritional information",
    "section": "",
    "text": "Starbucks is one of the most valued coffee chain in the world. A lot of people like to consume the beverages available at starbucks. But how good are they in terms of the nutritional value?\n\n\nI found this dataset on Kaggle which gives the nutritional information about their beverages. We will look at the exploratory data analysis first and later try some simple prediction models. First let us access and process the data through R.\n\n\n# Load the packages\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(car) # to calculate the VIF values\nlibrary(GGally) # for pairs plot using ggplot framework\n\n\n# Get starbucks data from github repo\npath = \"https://raw.githubusercontent.com//adityaranade//portfolio//refs//heads//main//starbucks//starbucks-menu-nutrition-drinks.csv\"\ndata0 &lt;- read.csv(path, header = TRUE)\n\n# Data processing\n# change the column names\ncolnames(data0) &lt;- c(\"name\", \"calories\", \"fat\", \n                     \"carbs\", \"fiber\",\"protein\", \n                     \"sodium\")\n\n# Check the first 6 rows of the dataset\ndata0 |&gt; head()\n\n                                                name calories fat carbs fiber\n1           Cool Lime Starbucks Refreshers‚Ñ¢ Beverage       45   0    11     0\n2                                   Ombr√© Pink Drink        -   -     -     -\n3                                         Pink Drink        -   -     -     -\n4     Strawberry Acai Starbucks Refreshers‚Ñ¢ Beverage       80   0    18     1\n5 Very Berry Hibiscus Starbucks Refreshers‚Ñ¢ Beverage       60   0    14     1\n6                                       Violet Drink        -   -     -     -\n  protein sodium\n1       0     10\n2       -      -\n3       -      -\n4       0     10\n5       0     10\n6       -      -\n\n# Check the type of data\ndata0 |&gt; str()\n\n'data.frame':   177 obs. of  7 variables:\n $ name    : chr  \"Cool Lime Starbucks Refreshers‚Ñ¢ Beverage\" \"Ombr√© Pink Drink\" \"Pink Drink\" \"Strawberry Acai Starbucks Refreshers‚Ñ¢ Beverage\" ...\n $ calories: chr  \"45\" \"-\" \"-\" \"80\" ...\n $ fat     : chr  \"0\" \"-\" \"-\" \"0\" ...\n $ carbs   : chr  \"11\" \"-\" \"-\" \"18\" ...\n $ fiber   : chr  \"0\" \"-\" \"-\" \"1\" ...\n $ protein : chr  \"0\" \"-\" \"-\" \"0\" ...\n $ sodium  : chr  \"10\" \"-\" \"-\" \"10\" ...\n\n\n\nThe data from second column should be numeric but shows as character. So we first convert it into numeric form and also exclude the rows with missing information\n\n\n# convert the data to numeric second row onwards\ndata0$calories &lt;- as.numeric(data0$calories)\ndata0$fat &lt;- as.numeric(data0$fat)\ndata0$carbs &lt;- as.numeric(data0$carbs)\ndata0$fiber &lt;- as.numeric(data0$fiber)\ndata0$protein &lt;- as.numeric(data0$protein)\ndata0$sodium &lt;- as.numeric(data0$sodium)\n\n# Check the type of data again\ndata0 |&gt; str()\n\n'data.frame':   177 obs. of  7 variables:\n $ name    : chr  \"Cool Lime Starbucks Refreshers‚Ñ¢ Beverage\" \"Ombr√© Pink Drink\" \"Pink Drink\" \"Strawberry Acai Starbucks Refreshers‚Ñ¢ Beverage\" ...\n $ calories: num  45 NA NA 80 60 NA NA NA 110 0 ...\n $ fat     : num  0 NA NA 0 0 NA NA NA 0 0 ...\n $ carbs   : num  11 NA NA 18 14 NA NA NA 28 0 ...\n $ fiber   : num  0 NA NA 1 1 NA NA NA 0 0 ...\n $ protein : num  0 NA NA 0 0 NA NA NA 0 0 ...\n $ sodium  : num  10 NA NA 10 10 NA NA NA 5 0 ...\n\n# Check the rows which do not have any entries\nind.na &lt;- which(is.na(data0[,2]))\nlength(ind.na) # 85 NA values\n\n[1] 85\n\n# exclude the rows which has NA values \ndata &lt;- data0[-ind.na,]\n\n\nNow that we have the data ready, let us look at the histogram each of the variables namely calories, fat, carbs, fiber, protein and sodium\n\n\n# Data for histogram\nmelted_data &lt;- melt(data, id.vars=\"name\")\n\n# Plot the histogram of all the variables\nggplot(melted_data,aes(value))+\n  # geom_histogram(aes(y = after_stat(density)),bins = 20)+\n  geom_histogram(bins = 20)+\n  facet_grid2(~variable, scales=\"free\")\n\n\n\n\n\n\n\n\n\nHistogram does not give much information. Let us look at the correlation plot to get an idea of how the variables are correlated with each other.\n\n\n# correlation plot of all the variables\ncorr &lt;- round(cor(data[,-1]), 1)\np.mat &lt;- cor_pmat(mtcars) # correlation p-value\n# Barring the no significant coefficient\nggcorrplot(corr, hc.order = TRUE,\n           type = \"lower\", p.mat = p.mat)\n\n\n\n\n\n\n\n# All positive correlation\n\n\nAll the variables are positively correlated (which indicates when one variable increases, the other variable will increase as well. ) which is not a surprising. Most important part is the correlation of calories with all the other variables are considerably high. Next we look at the pairs plot which will show the bivariate scatter plots as well as the correlation between each variables.\n\n\nggpairs(data,columns = 2:ncol(data),\n        lower = list(continuous = \"smooth\"))\n\n\n\n\n\n\n\n\n\nMost of the bivariate scatter plots indicate a linear relationship between the variables. The most important result according to us is the relationship between calories with all the other variables. We can now use the dataset for predictions where we try to predict the calories based o the fat, carb, fiber, protein and sodium content using multiple linear regression.\n\n\n# split the data into training and testing data\nseed &lt;- 23\nset.seed(seed)\n\nind &lt;- sample(floor(0.8*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train &lt;- data[ind,-1]\n# Testing dataset\ndata_test &lt;- data[-ind,-1]\n\n# Multiple linear regression using raw data\nmodel &lt;- lm(calories ~ fat + carbs + fiber + protein + sodium, data = data_train)\nsummary(model)\n\n\nCall:\nlm(formula = calories ~ fat + carbs + fiber + protein + sodium, \n    data = data_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.2823  -1.7523   0.0195   2.1092   8.1259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.75158    0.94777   1.848   0.0690 .  \nfat          7.65711    0.25133  30.466  &lt; 2e-16 ***\ncarbs        3.79744    0.04057  93.599  &lt; 2e-16 ***\nfiber        2.51563    0.96443   2.608   0.0112 *  \nprotein      0.75585    0.31682   2.386   0.0199 *  \nsodium       0.32290    0.03265   9.889 1.01e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.131 on 67 degrees of freedom\nMultiple R-squared:  0.9976,    Adjusted R-squared:  0.9974 \nF-statistic:  5493 on 5 and 67 DF,  p-value: &lt; 2.2e-16\n\n# Prediction on the testing dataset\ny_pred &lt;- predict(model, data_test)\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred,data_test$calories))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+geom_abline()\n\n\n\n\n\n\n\n# Calculate RMSE\nrmse &lt;- (y_pred-data_test$calories)^2 |&gt; sum() |&gt; sqrt()\nrmse\n\n[1] 102.3342\n\n# Check the variance inflation factor\nvif_values &lt;- vif(model)\nvif_values\n\n      fat     carbs     fiber   protein    sodium \n 3.849325  1.266592  3.382455 10.380709 11.704360 \n\n# Check the assumptions of the regression model\n# par(mfrow = c(2, 2))\n# plot(model)\n\n\nThe model is decent with RMSE 102.33 and the observed vs.¬†predicted plot also looks decent. However the variation inflation factor (VIF) value for protein and sodium is higher than 10 which indicates that these two variables are highly correlated with at least one other input variable and hence the variation of these variables is inflated. This might lead to unreliable models. One way to mitigate the multicollinearity problem is to use principal components in place of the correlated variables.\n\n\nWe will create principal components and look how much variation is explained by each of the principal components.\n\n\npc &lt;- prcomp(data[,-(1:2)],\n             center = TRUE,\n             scale. = TRUE)\nattributes(pc)\n\n$names\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n$class\n[1] \"prcomp\"\n\n# Check the factor loading of the principal components\nprint(pc)\n\nStandard deviations (1, .., p=5):\n[1] 1.7690827 0.8908878 0.7580688 0.5812294 0.4051785\n\nRotation (n x k) = (5 x 5):\n              PC1        PC2        PC3        PC4         PC5\nfat     0.4610104  0.3810085  0.1092277  0.7933177  0.03190861\ncarbs   0.3784887 -0.5448572 -0.7329252  0.1443775 -0.04304364\nfiber   0.3758452 -0.6263993  0.6433047 -0.0153384  0.22866603\nprotein 0.5161622  0.1647649  0.1161063 -0.3649765 -0.74815813\nsodium  0.4863462  0.3720747 -0.1535201 -0.4651440  0.62056454\n\n# Check the summary of the principal components\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5\nStandard deviation     1.7691 0.8909 0.7581 0.58123 0.40518\nProportion of Variance 0.6259 0.1587 0.1149 0.06757 0.03283\nCumulative Proportion  0.6259 0.7847 0.8996 0.96717 1.00000\n\n\n\nThe first four principal components explain around 96.72 % of the variation in the data. We will use the first four principal components for the regression model.\n\n\ndata_pc &lt;- cbind(data[,1:2],pc$x)\n# training data\ndata_pc_train &lt;- data_pc[ind,-1]\n# testing data\ndata_pc_test &lt;- data_pc[-ind,-1]\n\n# Multiple linear regression using PC\nmodel_pc &lt;- lm(calories ~ PC1 + PC2 + PC3 + PC4, data = data_pc_train)\nsummary(model_pc)\n\n\nCall:\nlm(formula = calories ~ PC1 + PC2 + PC3 + PC4, data = data_pc_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3780  -1.5028   0.7182   2.2345   7.4206 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 135.7766     0.5642  240.67   &lt;2e-16 ***\nPC1          49.2547     0.3746  131.47   &lt;2e-16 ***\nPC2         -12.8205     0.9443  -13.58   &lt;2e-16 ***\nPC3         -40.1362     0.9209  -43.59   &lt;2e-16 ***\nPC4          22.9143     0.9862   23.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.442 on 68 degrees of freedom\nMultiple R-squared:  0.9971,    Adjusted R-squared:  0.997 \nF-statistic:  5935 on 4 and 68 DF,  p-value: &lt; 2.2e-16\n\n# Prediction on the testing dataset\ny_pred_pc &lt;- predict(model_pc, data_pc_test)\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred_pc,data_test$calories))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+geom_abline()\n\n\n\n\n\n\n\n# Calculate RMSE\nrmse &lt;- (y_pred_pc-data_pc_test$calories)^2 |&gt; sum() |&gt; sqrt()\nrmse\n\n[1] 63.72554\n\n# Check the variance inflation factor\nvif_values_pc &lt;- vif(model_pc)\nvif_values_pc\n\n     PC1      PC2      PC3      PC4 \n1.129946 1.471886 1.373721 1.160650 \n\n# Check the assumptions of the regression model\n# par(mfrow = c(2, 2))\n# plot(model_pc)\n\n\nRMSE for the regression model using the first four principal components is 63.73. The variation inflation factor is less than 1.5 for all the principal components. So using less variables with principal components gives much better predictions."
  },
  {
    "objectID": "portfolio/fab4/index.html",
    "href": "portfolio/fab4/index.html",
    "title": "Comparing batting averages of the fabulous four players of Indian test cricket by opposition",
    "section": "",
    "text": "Sachin Tendulkar, Rahul Dravid, Sourav Ganguly and VVS Laxman are usually dubbed the faboulous four of Indian cricket team. In the late 1990‚Äôs and early 2000‚Äôs, these players were at their peak and were the backbone of Indian Test Cricket team from the batting perspective.\nWe will have a look at the career batting average of each of the player against the nine test playing nations and try to understand which of the player has been a consistent performer across teams. Sachin Tendulkar made his test cricket debut in 1989 whereas Rahul Dravid, Sourav Ganguly and VVS Laxman made their debuts in the year 1996. Sourav Ganguly was the first to retire in 2008 followed by Rahul Dravid, VVS Laxman both of whom retired in the year 2012 and lastly Sachin Tendulkar retired in the year 2013.\nThe career summary of the four players under consideration can be seen in table below\n\n\n\nCareer details for players\n\n\nSince all the players have played more that 100 matches (and more that 150 innings), we can compare the batting averages for them against all the nine test playing nations. However, we will get a little adventurous and use a Bayesian approach to calculate the credible intervals for the batting average for each of the player.\nIn case you are wondering what a Bayesian method means, it is simply reconciling our belief (prior) with the data (likelihood) to obtain updated belief (posterior). And the credible interval gives us the plausible values for the batting average based on the data we have. Along with the average against each opponent, we are incorporating the variation in the scoring as well which is nothing but considering how consistently or inconsistently the runs were scored.\nWe will incorporate another layer of uncertainty in the sense that we let the parameter at the first level have their own uncertainty and assign a distribution of their own. This is done through a Bayesian hierarchical model\nThe Bayesian Hierarchical model we will use for each player is as follows\n\\[Y_{i} \\overset{\\mathrm{i.i.d}}\\sim N(\\mu_{opposition[i]}, \\sigma_{opposition[i]}^2)\\]\n\\[\\mu_{opposition} \\overset{ind}{\\sim} N(\\theta,\\tau^2)\\]\n\\[\\theta \\sim N(\\theta_{0}, \\tau_{0}^2)\\]\n\\[\\tau \\sim Ca^{+}(0,1)\\]\n\\[\\sigma_{opposition} \\sim Uniform(10,200)\\]\nwhere \\(\\theta_{0} = 50\\) and \\(\\tau_{0} = 10\\) has been used which is consistent with each of the player‚Äôs career average.\nThe model was run using a JAGS (Just Another Gibbs Sampler) which samples from the posterior distribution of the parameters. We will not go into much details about the individual parameters but use the parameters to come up with the credible intervals for the average runs scored per innings for all the players against each of the test playing nation. We than combine the credible intervals for all the players into a single plot which can be found below.\n{{&lt; figure \"./fab4.png\" \"Credible intervals for average runs scored per inning against each of the opposition\" \"true\" &gt;}}\n\n\n\nCredible intervals for the average runs scored per inning against each of the opposition\n\n\nWe have the credible intervals for each of the four players against the nine test playing nations. Opposition is indicated on the x axis and the horizontal lines corresponding to any of the opposition is the credible interval for average against that particular opposition. We use different colors to indicate which player the interval corresponds to. Purple colour corresponds to intervals for Sachin Tendulkar, red colour corresponds to intervals for Rahul Dravid, green colour corresponds to intervals for Sourav Ganguly and blue colour corresponds to intervals for VVS Laxman.\nSince all the players have played against each of the opposition, we have intervals for all the players against all the opposition. However, all the players have batted less than 10 innings against Bangladesh and Zimbabwe and hence the intervals are not much meaningful.\nThere are two characteristics which can be understood looking at any of the interval. The first is the width of interval which indicates how consistent the player was against that particular opposition. If the width of the interval is big, that indicates the player scored runs inconsistently against the particular opposition. So we ideally want the interval to be as narrow as possible.\nThe second characteristic is the location of the interval which indicates the average of the player againt a particular opposition. If the average is higher, the location of the inteval will be towards the right and if the average is on the lower side the interval will lie towards the left side. Ideally we want a interval to be towards the right side.\nNow that we have understood how to read the intervals, let us start comparing the intervals for players against each of the opposition. In professional sports, every player seems to have a ‚Äòfavored‚Äô opposition against whom the player plays extraordinarily well which will be indicated in the plot by a narrow interval towards the right side. Similarly every player has an opposition who they do not enjoy playing which will be indicated in the plot by a wide interval and/or interval towards the left side.\nAgainst SENA countries\nFor teams in the Indian subcontinent, test cricket against SENA countries (South Africa, England, New Zealand and Australia) are considered an good indicator of how good a player is.\nAustralia was the most dominant opposition during the playing career of the four players. Australia tour of India in 2001 was a legendary series which was the turning point of Indian test cricket. The second test match in the series at Kolkata which India won while following on (an overwhelmingly difficult task) was nothing short of a legendary match. It is well know that Australia is VVS Laxman‚Äôs favoured opposition. However, if we look at the interval‚Äôs corresponding to Australia, we can see that Sachin Tendulkar has a better average than the other three players against Australia followed by VVS Laxman, Rahul Dravid and lastly Sourav Ganguly.\nAgainst New Zealand, Rahul Dravid seems to have the higher average followed by VVS Laxman, Sachin Tendulkar and lastly Sourav Ganguly.\nAgainst England, Rahul Dravid seems to have the higher average followed by Sourav Ganguly, Sachin Tendulkar and lastly VVS Laxman. VVS Laxman seems to have unusually low interval for average against England.\nAll the players seem to have performed below par against South Africa. Among the four players, Sachin Tendulkar seems to have performed relatively better than then the other three players who seems to have a similar record against South Africa.\nAgainst other countries\nAgainst Pakistan, Rahul Dravid has the highest average followed by Sourav Ganguly, Sachin Tendulkar and lastly VVS Laxman.\nAgainst Sri Lanka, Sachin Tendulkar has the highest average followed by Rahul Dravid, Sourav Ganguly and lastly VVS Laxman.\nAgainst West Indies, Rahul Dravid has the highest average followed by Sachin Tendulkar, VVS Laxman and lastly Sourav Ganguly. Sourav Ganguly has an unusually low interval for average against West Indies.\nAll the players have played less than 10 innings against Zimbabwe and Bangladesh and hence the scores have more variation in them which can be seen by the wide intervals. So comparing the intervals for the players against these two teams is not meaningful."
  },
  {
    "objectID": "portfolio/cherry_blossom_prediction/index.html",
    "href": "portfolio/cherry_blossom_prediction/index.html",
    "title": "Cherry Blossom prediction using Time Series",
    "section": "",
    "text": "Cherry Blossom is one of the most scenic visuals one can experience. Cherry blossom season marks the arrival of spring season and can be considered as transition from winter to summer. People try to make plans travel to enjoy this phenomenon. So how about using some simple statistical techniques to try and forecast / predict the peak cherry blossom time ?\n\n\nAlong with some of my fellow PhD classmates, I participated in the International Cherry Blossom Prediction Competition hosted by George Mason university. We explored a lot of models and I am going to show a very basic model which I tried during the early stages. The model is the Autogegressive (AR) model. The notation of this model is AR(1) model is as follows\n\n\\[\nY_{t} = \\beta_{1} Y_{t-1} + \\epsilon_{t}\n\\] where $Y_{t}$ is the bloom day for year $t$, \\(\\beta_{i}\\) is the model parameter and \\(\\epsilon_{t}\\) is the white noise\n\nThis simply means the present value of the response variable $Y$ (in our case the bloom day for this year) is influenced by previous value of the response variable (in our case the bloom day of the previous year). If you are aware of the simple linear regression, think of this as the explanatory variable being the same as the predictor variable in rough sense. In the competition, we tried to predict the bloom date for multiple location across the world based on the data available provide by the university. However, for the purpose of this post, I will show the analysis only for one location, Kyoto in Japan.\n\n\nLet us start with first reading in the dataset and loading the R packages required for the analysis\n\n\n# Load the packages\nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(fpp2)\nlibrary(dplyr)\nlibrary(vars)\n\n# Load the dataset\nkyoto &lt;- read.csv(\"https://raw.githubusercontent.com/GMU-CherryBlossomCompetition/peak-bloom-prediction/main/data/kyoto.csv\",header=T)\n\n# Plot of the bloom date over the years\nggplot(kyoto,aes(x=year,y=bloom_doy))+\n  geom_point()+\n  labs(x=\"Year\",y=\"Bloom Day\")+\n  ggtitle(\"Bloom Day by Year\")\n\n\n\n\n\n\n\n\n\nAs we can see from the plot, towards the later end (in the recent past), the bloom day has started to go down. This means in the recent past, the bloom day is happening earlier than before. Let us look at the plot only from the year 1950.\n\n\n# Filter data only for year since 1951\nkyoto_new &lt;- kyoto %&gt;% filter(year&gt;1950)\n\n# Plot of the bloom date over the years\nggplot(kyoto_new,aes(x=year,y=bloom_doy))+\n  geom_point()+\n  labs(x=\"Year\",y=\"Bloom Day\")+\n  ggtitle(\"Bloom Day by Year\")\n\n\n\n\n\n\n\n\n\nAs we can see from the plot for year 1951 onward, there seems to be a downward trend which indicates the bloom date is in general arriving earlier.\n\n\nWe will use the data from 1951 to 2022 to predict the bloom date for year 2023 and compare that to actual bloom date. For this, we will use the bloom day as response and the year as the predictor\n\n\n# Prepare the data for ARIMA(1,0,0) model\ny_kyoto &lt;- kyoto_new$bloom_doy # bloom day as the response\n\nExclude the year 2023 from response and explanatory variable to test the model on year 2023\n\n# First test on 2023 model\nytest &lt;- y_kyoto[-length(y_kyoto)] # exclude the bloom day for 2023 year\n\n# Model based on year 1951 to 2022\nfit_kyoto_test &lt;- Arima(ytest, order=c(1,0,0)) # order=c(1,0,0) indicates AR(1) model\nfit_kyoto_test\n\nSeries: ytest \nARIMA(1,0,0) with non-zero mean \n\nCoefficients:\n         ar1     mean\n      0.2821  97.2885\ns.e.  0.1182   0.7483\n\nsigma^2 = 21.84:  log likelihood = -215.16\nAIC=436.32   AICc=436.67   BIC=443.19\n\n#Forecast\nfcast_kyoto_test &lt;- forecast(fit_kyoto_test)\nfcast_kyoto_test\n\n   Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n74       93.53975 87.55120  99.5283 84.38105 102.6984\n75       96.23094 90.00866 102.4532 86.71479 105.7471\n76       96.99013 90.74963 103.2306 87.44611 106.5342\n77       97.20430 90.96235 103.4463 87.65806 106.7505\n78       97.26472 91.02265 103.5068 87.71830 106.8111\n79       97.28176 91.03969 103.5238 87.73533 106.8282\n80       97.28657 91.04450 103.5286 87.74014 106.8330\n81       97.28793 91.04585 103.5300 87.74150 106.8344\n82       97.28831 91.04624 103.5304 87.74188 106.8347\n83       97.28842 91.04634 103.5305 87.74199 106.8349\n\n# Check actual bloom date for 2023\ny_kyoto[length(y_kyoto)] \n\n[1] 95\n\n\n\nThe AR(1) model predicts 96 as the bloom day for year 2023 whereas the actual bloom day was 84 for year 2023 which is a difference of 12 days. Considering its a basic model, this does not seem to be too bad.\n\n\nNow we check the performance of the model using some charts where we first check the prediction plot, then the Regression and model errors.\n\n\n# Plot the prediction\nautoplot(fcast_kyoto_test) + xlab(\"Year\") +\n  ylab(\"Percentage change\")\n\n\n\n\n\n\n\n# recover estimates of nu(t) and epsilon(t) \ncbind(\"Regression Errors\" = residuals(fit_kyoto_test, type=\"regression\"),\n      \"ARIMA errors\" = residuals(fit_kyoto_test, type=\"innovation\")) %&gt;%\n  autoplot(facets=TRUE)\n\n\n\n\n\n\n\n\n\nThere does not seem to be any issues with either of the plots. Now we check the residuals to see if they are normally distributed.\n\n\n# Check the residuals\ncheckresiduals(fit_kyoto_test)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,0) with non-zero mean\nQ* = 13.92, df = 9, p-value = 0.1252\n\nModel df: 1.   Total lags used: 10\n\n\n\nThe residuals seem to be normally distributed and the Ljung-Box test indicates we have little evidence against the null hypothesis of independently distributed errors.\n\n\nNow we will use the data from 1951 upto 2023 to predict the bloom date for the year 2024. Basically its the same model with one extra data point available\n\n\n# Now use data upto 2023 to predict 2024\nfit_kyoto &lt;- Arima(y_kyoto, order=c(1,0,0)) # order=c(1,0,0) indicates AR(1) model\nfit_kyoto\n\nSeries: y_kyoto \nARIMA(1,0,0) with non-zero mean \n\nCoefficients:\n         ar1     mean\n      0.2707  97.3162\ns.e.  0.1111   0.7264\n\nsigma^2 = 21.56:  log likelihood = -217.65\nAIC=441.3   AICc=441.64   BIC=448.21\n\n#Forecast\nfcast_kyoto &lt;- forecast(fit_kyoto)\nfcast_kyoto\n\n   Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n75       96.68923 90.73853 102.6399 87.58842 105.7900\n76       97.14647 90.98163 103.3113 87.71816 106.5748\n77       97.27023 91.08999 103.4505 87.81837 106.7221\n78       97.30373 91.12237 103.4851 87.85015 106.7573\n79       97.31280 91.13135 103.4943 87.85909 106.7665\n80       97.31526 91.13380 103.4967 87.86154 106.7690\n81       97.31592 91.13447 103.4974 87.86220 106.7696\n82       97.31610 91.13465 103.4976 87.86238 106.7698\n83       97.31615 91.13469 103.4976 87.86243 106.7699\n84       97.31616 91.13471 103.4976 87.86244 106.7699\n\n\n\nThe model predicts the cherry blossom to bloom on day 93 which is 2nd April 2024 (due to 2024 being a leap year) for Kyoto, Tokyo. Now lets look at the diagnostics of the model to see if the model is reasonable.\n\n\n# Plot the forecast\nautoplot(fcast_kyoto) + xlab(\"Year\") +\n  ylab(\"Percentage change\")\n\n\n\n\n\n\n\n# recover estimates of nu(t) and epsilon(t) \ncbind(\"Regression Errors\" = residuals(fit_kyoto, type=\"regression\"),\n      \"ARIMA errors\" = residuals(fit_kyoto, type=\"innovation\")) %&gt;%\n  autoplot(facets=TRUE)\n\n\n\n\n\n\n\n# Check the residuals\ncheckresiduals(fit_kyoto)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,0) with non-zero mean\nQ* = 13.9, df = 9, p-value = 0.1259\n\nModel df: 1.   Total lags used: 10\n\n\n\nAgain, the residuals seem to be normally distributed and the Ljung-Box test indicates we have little evidence against the null hypothesis of independently distributed errors."
  },
  {
    "objectID": "portfolio/ipl_best_team/index.html",
    "href": "portfolio/ipl_best_team/index.html",
    "title": "Does the best team win the Indian Premier League ?",
    "section": "",
    "text": "Indian Premier League (IPL) is an annual cricket league which is played under the T20 format. It was started in the year 2008 and has quickly gained popularity among cricket playing nations and has probably become one of the biggest sporting event in the Indian sports calendar.\nThe tournament is played between various teams based out of different cities in India. IPL is currently in its 14th season where the tournament has been played with 8 teams from 2008 - 2010 and 2014-2021. It was played with 10 teams in 2011 and 9 teams in 2012 and 2013. From 2022, it has been expanded again to 10 teams.\nThe tournament followed a double round-robin format from 2008 - 2021 (except for the year 2011) where each team plays the other team twice (once at each team‚Äôs ‚Äòhome‚Äô ground). From the year 2022, the teams are divided into two groups and every team plays 14 games (twice against the teams in the same group, twice against one team from other group and once against the other teams from the other group). At the conclusion of the league stage, the top four teams qualify for the playoffs. For the first 3 years (2008-2010), the traditional semifinals and final approach was considered. From 2011, a playoff structure was introduced to award the top two teams with an additional chance to reach the finals. This ensured the competition stays relevant till the last game of the league stage as teams vie for top 2 position. The figure below explains the playoff structure.\n\n\n\nIPL Playoff Structure\n\n\nThe top two teams of the league stage have two chances to reach the final. The thought behind this playoff structure was that the top two teams should not be out of the tournament due to one bad day in the semi-finals.\nA question that pops up in the mind is ‚ÄúDoes the best team win the tournament‚Äù. In other words, does the team which finishes the league stage at the top of the table have the best chance to win the league. The team which finishes the league stage position 1 has won the most number of points (and in case of tied points, has a superior net run rate) and hence is considered the ‚Äúbest‚Äù team in the league. We would be tempted to say yes. But let‚Äôs find out what the data says.\nWe have the data available for 11 years (2011 - 2021) for the league stage standings and the playoff results for the team topping the league stage from IPL official website which can be seen below\n\n\n\nTeam finishing first during league stage\n\n\nWe can observe, the team which tops the league stage has won the tournament only three 3 of 11 times which corresponds to approximately 27.27%. However, this is just the observation from 11 years so cannot be considered as the best indicator of probability. We will consider a Bayesian statistical model to calculate the probability of the team finishing the tournament as winner, runners up and third place.\nIn simple terms, a Bayesian model is reconciliation of our belief ( ‚Äòprior‚Äô distribution) with the observed data (likelihood) to give the updated belief (‚Äòposterior‚Äô distribution). Here the information (prior, likelihood and posterior) is in the form of mathematical distribution functions.\nIf we consider the random variable ‚Äòresult‚Äô which indicates the position of the team at the end of the tournament. The team which finishes the league stage at the top of the table can finish at any position from 1 to 3 at the end of the tournament where 1 stands for winner, 2 stands for runner‚Äôs up and 3 stands for 3rd place. Since the variable can take only 1 out of the 3 values, it follows a categorical distribution which is denoted as result ~ categorical(\\(p_{1}, p_{2}, p_{3}\\)) where \\(p_{1}\\), \\(p_{2}\\) and \\(p_{3}\\) indicates the probability of finishing at each of the position at the end of the tournament.\nWe will consider the probabilities as a random variable as well. We have to assign a distribution to the variable in such a way that our beliefs are reflected in the distribution. We will consider the team has an equal chance at finishing at each of the positions. This is ensured by assigning a Dirichlet(1,1,1) distribution to (\\(p_{1}\\), \\(p_{2}\\), \\(p_{3}\\))\nSo the model is as follows\n\\[result \\overset{\\mathrm{i.i.d.}}{\\sim} Categorical(p_{1}, p_{2}, p_{3}) \\]\n\\[(p_{1}, p_{2}, p_{3}) \\sim Dirichlet(\\alpha_{1}, \\alpha_{2}, \\alpha_{3}) \\]\nWe run this model on 11 years of the data available (2011 - 2021) and we get the credible interval for the posterior probabilities of finishing at each of the position\n\n\n\nCredible intervals for probabilities for team finishing first during league stage\n\n\nThe horizontal line indicates the plausible values for the probability of team finishing in a particular position and black point indicates the estimated value of the probability from the available data and model used.\nWe can see the probability of being runner up is higher than the probability of finishing as winner or 3rd place which seem to have almost the same probability. This means the team finishing at the top of the table at the end of the league will more often go on to lose in the playoffs (finish as runner up or \\(3^{rd}\\) place) at the end of the tournament.\nIsn‚Äôt it surprising? It surely is. Let us look at some of the reasons this might be the case. More often than not, the team which finishes at the top of the table during the league stage accumulates most of the points during the early stages and tends to lose steam towards the business end of the tournament. It might be a case of peaking too early. There might be some other psychological impact that might be worthwhile to study.\nSo probably the best team in the league (team finishing first during league stage) does not win the tournament.!\nLet us perform the same analysis for the team which finishes at second position in the league stage. We will use the same model used for the team finishing first at the league stage. Using data from 11 years, we get the following plot\n\n\n\nCredible intervals for probabilities for team finishing second during league stage\n\n\nWe can see the probability of being winner is higher than the probability of finishing as runner‚Äôs up or 3rd place. This means the team finishing second in the league stage will more often go on to win the tournament. Surely a surprising and fascinating result!"
  },
  {
    "objectID": "portfolio/starbucks_food_python/index.html",
    "href": "portfolio/starbucks_food_python/index.html",
    "title": "Starbucks food nutritional information",
    "section": "",
    "text": "Starbucks is one of the most valued coffee chain in the world. A lot of people like to consume the food available at starbucks. But how good are they in terms of the nutritional value?\n\n\nI found this dataset on Kaggle which gives the nutritional information about their food products. In my precious post, I built a multiple linear regression model to predict the calories in beverage based on the nutritional contents of the beverage. Now we will try to do the same for the food products.\nFirst, we look at the exploratory data analysis and later try some simple regression models. First let us access and process the data through python\n\n\n# Load Libraries\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom plotnine import * # for plots\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nimport random\nfrom scipy.stats import pearsonr\n\n# Get starbucks data from github repo\npath = \"https://raw.githubusercontent.com//adityaranade//portfolio//refs//heads//main//starbucks//starbucks-menu-nutrition-food.csv\"\ndf0=pd.read_csv(path, encoding='unicode_escape')\n\ndf0.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nCalories\nFat (g)\nCarb. (g)\nFiber (g)\nProtein (g)\n\n\n\n\n0\nChonga Bagel\n300\n5.0\n50\n3\n12\n\n\n1\n8-Grain Roll\n380\n6.0\n70\n7\n10\n\n\n2\nAlmond Croissant\n410\n22.0\n45\n3\n10\n\n\n3\nApple Fritter\n460\n23.0\n56\n2\n7\n\n\n4\nBanana Nut Bread\n420\n22.0\n52\n2\n6\n\n\n\n\n\n\n\n\n#modify the column names\ndf0.columns = ['name', 'calories','fat','carbs','fiber','protein']\ndf0.head()\n\n#convert data type to float for all the columns except name\nfor i in df0.columns[1:]:\n    df0[i]=df0[i].astype(\"float\")\n# df0.info()\n\n\ndf = df0\n# Use melt function for the histograms\ndf2 = pd.melt(df, id_vars=['name'])\n# df2.head()\n\n\nNow that we have the data ready, let us look at the histogram of each variables namely calories, fat, carbs, fiber, protein and sodium\n\n\np = (\n    ggplot(df2, aes(\"value\"))\n    + geom_histogram(bins=15)\n    + facet_grid(\". ~ variable\", scales='free_x')\n    )\n\np.show()\n\n\n\n\n\n\n\n\nThe histogram of each of the variables do not show any problems as all the plots look decent. We will look at the correlation plot.\n\n# Check the correlation between the variables\n# plt.figure(figsize=(20,7))\nsns.heatmap(df.iloc[:,1:].corr(),annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nCorrelation plot indicates positive association between all the variables which is desired. Now we will look at the pairs plot which will show the pairwise histogram.\n\n\n# Pairs plot\ng = sns.PairGrid(df.iloc[:,1:])\ng.map_diag(sns.histplot)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe scatterplots of each variable with calories which can be seen in the upper triangular plots in the very first row. It seems there is a linear association between calories and fat, carbs and protein. However, it does not seem to have a linear association with fiber.\n\n\n# Split data into train and test set\nindices = range(len(df)) # Create a list of indices\n\n# Get 75% random indices for training data\nrandom.seed(23) # for repreducible example\nrandom_indices = random.sample(indices, round(0.75*len(df)))\n\n# Training dataset\ndata_train = df.iloc[random_indices,]\n\n# Testing dataset\ndata_test = df.iloc[df.index.difference(random_indices),]\n\n\n# Build a multiple linear regression model to predict calories using other variables using training data\nresult = smf.ols(\"calories ~ fat + carbs + fiber + protein\", data = data_train).fit()\n# check the summary\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ncalories\nR-squared:\n0.995\n\n\nModel:\nOLS\nAdj. R-squared:\n0.995\n\n\nMethod:\nLeast Squares\nF-statistic:\n4351.\n\n\nDate:\nSun, 25 May 2025\nProb (F-statistic):\n1.06e-92\n\n\nTime:\n11:08:25\nLog-Likelihood:\n-304.54\n\n\nNo. Observations:\n85\nAIC:\n619.1\n\n\nDf Residuals:\n80\nBIC:\n631.3\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-2.8394\n3.006\n-0.945\n0.348\n-8.821\n3.142\n\n\nfat\n8.8760\n0.129\n68.933\n0.000\n8.620\n9.132\n\n\ncarbs\n4.0291\n0.066\n61.478\n0.000\n3.899\n4.160\n\n\nfiber\n-1.1151\n0.402\n-2.772\n0.007\n-1.916\n-0.315\n\n\nprotein\n4.3474\n0.156\n27.915\n0.000\n4.037\n4.657\n\n\n\n\n\n\n\n\nOmnibus:\n8.990\nDurbin-Watson:\n2.042\n\n\nProb(Omnibus):\n0.011\nJarque-Bera (JB):\n12.887\n\n\nSkew:\n0.426\nProb(JB):\n0.00159\n\n\nKurtosis:\n4.707\nCond. No.\n151.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nNow let us make prediction on the testing data and plot the observed vs.¬†predicted plot\n\n\n# Make predictions using testing data\npredictions = result.predict(data_test)\n\n# Observed vs. Predicted plot\nplt.figure(figsize=(20,7))\nplt.scatter(predictions, data_test[\"calories\"])\nplt.ylabel(\"Observed calories\")\nplt.xlabel(\"Predicted calories\")\n# Create the abline\nx_line = np.linspace(min(data_test[\"calories\"]), max(data_test[\"calories\"]), 100)\ny_line = 1 * x_line + 1\nplt.plot(x_line, y_line, color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe observed vs.¬†predicted looks good. However there is low number of data points and hence we should take this with a grain of salt. Let us check some evaluation metrics like the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nprint(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"calories\"],predictions),2))\nprint(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"calories\"],predictions))** 0.5,2))\n\nMean Absolute Error: 7.54\nRoot Mean Squared Error: 10.7\n\n\n\nRoot Mean Squared Error (RMSE) of 7.54 and Mean Absolute Error (MAE) of 10.7 is decent and indicates model is performing fairly well."
  },
  {
    "objectID": "portfolio/cereals/index.html",
    "href": "portfolio/cereals/index.html",
    "title": "Cereals nutritional information",
    "section": "",
    "text": "Cereals are commonly consumed for breakfast. But how good are they in terms of the nutritional value? Can we predict the calories based on the nutritional contents ?\n\n\nI found this dataset on Kaggle which gives the nutritional information about their cereals. First, we look at the exploratory data analysis and later try some simple regression models. First let us access and process the data through python\n\n\n# Load Libraries\n\n# Load Libraries\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom plotnine import *\nimport numpy as np # linear algebra\n# import statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom scipy.stats import pearsonr\n\n# Get starbucks data from github repo\n\npath = \"https://raw.githubusercontent.com//adityaranade//portfolio//refs//heads//main//cereals//cereal.csv\"\n\ndf0=pd.read_csv(path, encoding='unicode_escape')\n\ndf0.head()\n\n\n\n\n\n\n\n\nname\nmfr\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843\n\n\n\n\n\n\n\n\n# modify the column names\ndf0.columns = ['name', 'manufacturer','type','calories','protein','fat','sodium','fiber','carbohydrates','sugar','potassium','vitamins','shelf','weight','cups', 'rating']\ndf0.head()\n\n\n\n\n\n\n\n\nname\nmanufacturer\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbohydrates\nsugar\npotassium\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843\n\n\n\n\n\n\n\n\n# select data for the histogram\ndf = df0[[\"calories\", \"protein\", \"fat\", \"sodium\", \"fiber\", \"carbohydrates\", \"sugar\",\"potassium\",\"name\"]]\ndf.head()\n\n# Use melt function for the histograms of variables \ndf2 = pd.melt(df, id_vars=['name'])\n# df2.head()\n\n\nNow that we have the data ready, let us look at the histogram of each variables namely nutritional contents, specifically calories, protein, fat, sodium, fiber, carbo, sugars and potassium\n\n\np = (\n    ggplot(df2, aes(\"value\"))\n    + geom_histogram(bins=10)\n    + facet_grid(\". ~ variable\", scales='free_x')\n    + theme(figure_size=(12, 3))\n    )\n\n# If we want the density on y axis\n# p = (\n#     ggplot(df2, aes(\"value\", after_stat(\"density\")))\n#     + geom_histogram(bins=10)\n#     + facet_grid(\". ~ variable\", scales='free_x')\n#     + theme(figure_size=(12, 3))\n#     )\n\np.show()\n\n\n\n\n\n\n\n\nThe histogram of each of the variables do not show any problems as all the plots look decent. We will look at the correlation plot.\n\n# Check the correlation between the variables\nplt.figure(figsize=(20,10))\nsns.heatmap(df.iloc[:,:-1].corr(),annot=True,cmap=\"viridis\")\nplt.show()\n\n\n\n\n\n\n\n\n\nCalories variable has significant positive correlation with all the variables except fiber and potassium. This seems logical and will be useful when we build a regression model for the same. Next we take a look at the pairs plot which will give us idea about relationship between each pair of variables. Most important from the point of prediction is the first row where calories is the y axis and each of the variable is x axis.\n\n\n# Pairs plot\ng = sns.PairGrid(df.iloc[:,1:])\ng.map_diag(sns.histplot)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe scatterplots of each variable with calories which can be seen in the upper triangular plots in the very first row. It seems there is a linear association between calories and fat, carbs and protein. However, it does not seem to have a linear association with fiber.\n\n\n# Split data into train and test set\nindices = range(len(df)) # Create a list of indices\n\n# Get 75% random indices\nrandom.seed(23) # for reproducible example\nrandom_indices = random.sample(indices, round(0.75*len(df)))\n\n# Training dataset\ndata_train = df.iloc[random_indices,:-1]\n\n# Testing dataset\ndata_test = df.iloc[df.index.difference(random_indices),:-1]\n\n# Build a multiple linear regression model to predict calories using other variables using training data\nresult = smf.ols(\"calories ~ protein + fat + sodium + fiber + carbohydrates + sugar + potassium\", data = data_train).fit()\n# check the summary\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ncalories\nR-squared:\n0.783\n\n\nModel:\nOLS\nAdj. R-squared:\n0.753\n\n\nMethod:\nLeast Squares\nF-statistic:\n25.78\n\n\nDate:\nSun, 25 May 2025\nProb (F-statistic):\n1.59e-14\n\n\nTime:\n11:07:25\nLog-Likelihood:\n-205.34\n\n\nNo. Observations:\n58\nAIC:\n426.7\n\n\nDf Residuals:\n50\nBIC:\n443.2\n\n\nDf Model:\n7\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n27.1211\n8.231\n3.295\n0.002\n10.588\n43.654\n\n\nprotein\n5.8767\n1.433\n4.102\n0.000\n2.999\n8.754\n\n\nfat\n8.7752\n1.482\n5.920\n0.000\n5.798\n11.753\n\n\nsodium\n0.0062\n0.016\n0.381\n0.705\n-0.026\n0.039\n\n\nfiber\n0.1062\n1.480\n0.072\n0.943\n-2.866\n3.078\n\n\ncarbohydrates\n2.5884\n0.380\n6.811\n0.000\n1.825\n3.352\n\n\nsugar\n3.1356\n0.365\n8.587\n0.000\n2.402\n3.869\n\n\npotassium\n-0.0489\n0.052\n-0.937\n0.353\n-0.154\n0.056\n\n\n\n\n\n\n\n\nOmnibus:\n29.387\nDurbin-Watson:\n2.071\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n80.990\n\n\nSkew:\n1.410\nProb(JB):\n2.59e-18\n\n\nKurtosis:\n8.055\nCond. No.\n1.39e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.39e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\np-value for sodium, fiber and potassium is considerably high and hence these variables do not add help predict the calories. This might be due to multicollinearity (the predictor variables are have high correlation within themselves). If we look at the correlation plot, fiber and potassium has 0.9 correlation which is high. One way to tackle multicollinearity is to consider principal component analysis (PCA). We will look at it in a while but let us first try to make predictions and look at the evaluation metrics.\n\n\nNow let us make prediction on the testing data and plot the observed vs.¬†predicted plot\n\n\n# Make predictions using testing data\npredictions = result.predict(data_test)\n\n# Observed vs. Predicted plot\nplt.figure(figsize=(20,7))\nplt.scatter(predictions, data_test[\"calories\"])\nplt.ylabel(\"Observed calories\")\nplt.xlabel(\"Predicted calories\")\n# Create the abline\nx_line = np.linspace(min(data_test[\"calories\"]), max(data_test[\"calories\"]), 100)\ny_line = 1 * x_line + 1\nplt.plot(x_line, y_line, color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe observed vs.¬†predicted looks good. However there is low number of data points and hence we should take this with a grain of salt. Let us check some evaluation metrics like the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nprint(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"calories\"],predictions),2))\nprint(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"calories\"],predictions))** 0.5,2))\n\nMean Absolute Error: 5.34\nRoot Mean Squared Error: 6.89\n\n\n\nRoot Mean Squared Error (RMSE) of 5.34 and Mean Absolute Error (MAE) of 6.89 is decent and indicates model is performing fairly well.\n\n\nNow, we will run regression model based on principal component analysis since it helps with multicollinearity.\n\n\n# Principal component analysis\nfrom sklearn.decomposition import PCA\n\n# separate the x and y variable for the training data first\ny_train = data_train.iloc[:,:1]\nX0_train = data_train.iloc[:,1:]\n\n# Standardize the predictor data first\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n# training data\nX_train_scaled = sc.fit_transform(X0_train)\n\n# Now calculate the principal components\nfrom sklearn.decomposition import PCA\npca = PCA()\nprincipalComponents = pca.fit_transform(X_train_scaled)\n# Training data\nX_train_pca = pd.DataFrame(data = principalComponents,\n             columns=['PC{}'.format(i+1)\n                      for i in range(principalComponents.shape[1])])\n\n\n\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance\n\narray([0.37089562, 0.24123216, 0.14712018, 0.12790205, 0.05655818,\n       0.0483467 , 0.0079451 ])\n\n\n\nThe first six principal components explain around 99% of the data, so we will use the first 6 principal components to build a regression model.\n\n\nX_train_pca = pd.DataFrame(data = principalComponents,\n             columns=['PC{}'.format(i+1)\n                      for i in range(principalComponents.shape[1])])\n\n# combine the X and Y for the training data\ndata_train_pca = X_train_pca\ndata_train_pca.set_index(X0_train.index,inplace = True)\ndata_train_pca['calories'] = y_train\ndata_train_pca.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\ncalories\n\n\n\n\n37\n-1.860753\n0.565043\n0.525620\n-0.850117\n-0.237526\n0.005600\n0.155876\n110\n\n\n10\n-1.341762\n1.558699\n0.886958\n0.766110\n-0.195248\n-0.671615\n-0.098032\n120\n\n\n2\n4.512654\n-0.609249\n2.068686\n-0.856303\n-0.653496\n-0.585533\n0.433959\n70\n\n\n39\n-0.253116\n-0.667314\n0.281191\n0.402756\n0.867213\n0.766825\n-0.165899\n140\n\n\n54\n-1.338187\n-0.005653\n-2.407667\n-1.446089\n-0.053030\n-1.193701\n0.316424\n50\n\n\n\n\n\n\n\n\n# Correlation plot for principal components\nplt.figure(figsize=(20,10))\nsns.heatmap(data_train_pca.corr().round(4),annot=True, cmap=\"viridis\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can observe that only calories variable has correlation with the principal components and the correlation between the principal components is practically 0. So we will use the principal components to build a regression model.\n\n\n# Now run the OLS regression model on the first five principal components\n# Fit the OLS regression\nresult_pca = smf.ols(\"calories ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6\", data = data_train_pca).fit()\n# check the summary\nresult_pca.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ncalories\nR-squared:\n0.774\n\n\nModel:\nOLS\nAdj. R-squared:\n0.747\n\n\nMethod:\nLeast Squares\nF-statistic:\n29.03\n\n\nDate:\nSun, 25 May 2025\nProb (F-statistic):\n7.97e-15\n\n\nTime:\n11:07:27\nLog-Likelihood:\n-206.59\n\n\nNo. Observations:\n58\nAIC:\n427.2\n\n\nDf Residuals:\n51\nBIC:\n441.6\n\n\nDf Model:\n6\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n107.7586\n1.194\n90.277\n0.000\n105.362\n110.155\n\n\nPC1\n-1.8336\n0.741\n-2.475\n0.017\n-3.321\n-0.346\n\n\nPC2\n3.8950\n0.919\n4.240\n0.000\n2.051\n5.739\n\n\nPC3\n4.3355\n1.176\n3.686\n0.001\n1.974\n6.697\n\n\nPC4\n10.9172\n1.261\n8.654\n0.000\n8.385\n13.450\n\n\nPC5\n9.0880\n1.897\n4.791\n0.000\n5.279\n12.896\n\n\nPC6\n12.7554\n2.052\n6.217\n0.000\n8.636\n16.875\n\n\n\n\n\n\n\n\nOmnibus:\n21.542\nDurbin-Watson:\n2.118\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n44.731\n\n\nSkew:\n1.104\nProb(JB):\n1.94e-10\n\n\nKurtosis:\n6.692\nCond. No.\n2.77\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\\(R^{2}\\) is 77.4% which is decent and all the predictor variables have a low p-value value. We make predictions using the test data and then plot the out of sample observed vs.¬†predicted. First we calculate the principal components of the testing data and then make the predictions.\n\n\n# X for testing data\nX0_test = data_test.iloc[:,1:]\n\n# scaled test data\nX_test_scaled = sc.transform(X0_test)\n\n# calculate the principal components for the testing data\nX_test = pca.transform(X_test_scaled)\nX_test_pca = pd.DataFrame(data = X_test,\n             columns=['PC{}'.format(i+1)\n                      for i in range(X_test.shape[1])])\n# calculate the predictions\npredictions_pca = result_pca.predict(X_test_pca)\n\n\nNow we plot the out of sample predictions obtained from regression model using raw data as well as the predictions obtained from model using the first six principal components on the same plot with different colors.\n\n\n# Observed vs. Predicted plot\nplt.figure(figsize=(20,7))\n\nplt.scatter(predictions, data_test[\"calories\"], label='raw', color='black', marker='o')\nplt.scatter(predictions_pca, data_test[\"calories\"],  label='PCA', color='blue', marker='o')\n# sns.regplot(y = data_test[\"calories\"],x = predictions,ci=None,line_kws={\"color\":\"red\"})\nplt.ylabel(\"Observed calories\")\nplt.xlabel(\"Predicted calories\")\nplt.legend()\n\n# Create the abline\nx_line = np.linspace(min(data_test[\"calories\"]), max(data_test[\"calories\"]), 100)\ny_line = 1 * x_line + 1\nplt.plot(x_line, y_line, color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe out of sample observed vs.¬†predicted plot looks decent with all the points just around the red line. WE look at the evaluation metrics for the model built using the principal components.\n\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nprint(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"calories\"],predictions_pca),2))\nprint(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"calories\"],predictions_pca))** 0.5,2))\n\nMean Absolute Error: 4.66\nRoot Mean Squared Error: 6.11\n\n\n\nFor the regression model using first six principal components, Root Mean Squared Error (RMSE) is 4.66 and Mean Absolute Error (MAE) is 6.11 which is an improvement from the regression model using the raw data."
  },
  {
    "objectID": "portfolio/cereals_ratings/index.html",
    "href": "portfolio/cereals_ratings/index.html",
    "title": "Rating of cereals based on nutritional information",
    "section": "",
    "text": "Cereals are commonly consumed for breakfast and there are plenty of options available for cereals. I found this dataset on Kaggle which gives the nutritional information about cereals as well as the ratings. It is not clear where the rating come from, but I think they are the average ratings from the customers. Can we predict the ratings based on the nutritional information of cereals ? First, we look at the exploratory data analysis and later try some simple regression models. First let us access and process the data through python.\n\n\n# Load Libraries\n\n# Load Libraries\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom plotnine import *\nimport numpy as np # linear algebra\n# import statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom scipy.stats import pearsonr\n\n# Get starbucks data from github repo\npath = \"https://raw.githubusercontent.com//adityaranade//portfolio//refs//heads//main//cereals//cereal.csv\"\n\ndf0=pd.read_csv(path, encoding='unicode_escape')\n\ndf0.head()\n\n\n\n\n\n\n\n\nname\nmfr\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843\n\n\n\n\n\n\n\n\n# modify the column names\ndf0.columns = ['name', 'manufacturer','type','calories','protein','fat','sodium','fiber','carbohydrates','sugar','potassium','vitamins','shelf','weight','cups', 'rating']\ndf0.head()\n\n\n\n\n\n\n\n\nname\nmanufacturer\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbohydrates\nsugar\npotassium\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843\n\n\n\n\n\n\n\n\n# select data for the histogram\ndf = df0[[\"calories\", \"protein\", \"fat\", \"sodium\", \"fiber\", \"carbohydrates\", \"sugar\",\"potassium\",\"rating\",\"name\"]]\ndf.head()\n\n\n\n\n\n\n\n\ncalories\nprotein\nfat\nsodium\nfiber\ncarbohydrates\nsugar\npotassium\nrating\nname\n\n\n\n\n0\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n68.402973\n100% Bran\n\n\n1\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n33.983679\n100% Natural Bran\n\n\n2\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n59.425505\nAll-Bran\n\n\n3\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n93.704912\nAll-Bran with Extra Fiber\n\n\n4\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n34.384843\nAlmond Delight\n\n\n\n\n\n\n\n\nNow that we have the data ready, let us look at the histogram of each variables namely nutritional contents, specifically calories, protein, fat, sodium, fiber, carbo, sugars and potassium\n\n\n# Use melt function for the histograms of variables \ndf2 = pd.melt(df, id_vars=['name'])\n# df2.head()\n\np = (\n    ggplot(df2, aes(\"value\"))\n    + geom_histogram(bins=10)\n    + facet_grid(\". ~ variable\", scales='free_x')\n    + theme(figure_size=(12, 3))\n    )\n\n# If we want the density on y axis\n# p = (\n#     ggplot(df2, aes(\"value\", after_stat(\"density\")))\n#     + geom_histogram(bins=10)\n#     + facet_grid(\". ~ variable\", scales='free_x')\n#     + theme(figure_size=(12, 3))\n#     )\n\np.show()\n\n\n\n\n\n\n\n\nThe histogram of each of the variables do not show any problems as all the plots look decent. We will look at the correlation plot, which shows the correlation between each pair of variables in a visual form.\n\n# Check the correlation between the variables\nplt.figure(figsize=(20,10))\nsns.heatmap(df.iloc[:,:-1].corr(),annot=True,cmap=\"viridis\")\nplt.show()\n\n\n\n\n\n\n\n\n\nRating variable has positive correlation with all the variables except sugar, sodium, fat and calories . This seems logical and will be useful when we build a regression model for the same. Next we take a look at the pairs plot which will give us idea about relationship between each pair of variables. Most important from the point of prediction is the last row where rating is the y axis and each of the variable is x axis.\n\n\n# Pairs plot\ng = sns.PairGrid(df.iloc[:,:-1])\ng.map_diag(sns.histplot)\ng.map_lower(sns.scatterplot)\ng.map_upper(sns.kdeplot)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe scatterplots of each variable with calories which can be seen in the upper triangular plots in the very first row. It seems there is a linear association between calories and fat, carbs and protein. However, it does not seem to have a linear association with fiber.\n\n\n# Split data into train and test set\nindices = range(len(df)) # Create a list of indices\n\n# Get 75% random indices\nrandom.seed(55) # for reproducible example\nrandom_indices = random.sample(indices, round(0.75*len(df)))\n\n# Training dataset\ndata_train = df.iloc[random_indices,:-1]\n\n# Testing dataset\ndata_test = df.iloc[df.index.difference(random_indices),:-1]\n\n# Build a multiple linear regression model to predict calories using other variables using training data\nresult = smf.ols(\"rating ~ calories + protein + fat + sodium + fiber + carbohydrates + sugar + potassium\", data = data_train).fit()\n# check the summary\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.997\n\n\nModel:\nOLS\nAdj. R-squared:\n0.997\n\n\nMethod:\nLeast Squares\nF-statistic:\n2429.\n\n\nDate:\nSun, 25 May 2025\nProb (F-statistic):\n6.35e-61\n\n\nTime:\n11:07:58\nLog-Likelihood:\n-65.696\n\n\nNo. Observations:\n58\nAIC:\n149.4\n\n\nDf Residuals:\n49\nBIC:\n167.9\n\n\nDf Model:\n8\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n54.4433\n0.868\n62.753\n0.000\n52.700\n56.187\n\n\ncalories\n-0.2160\n0.014\n-15.385\n0.000\n-0.244\n-0.188\n\n\nprotein\n3.3032\n0.152\n21.680\n0.000\n2.997\n3.609\n\n\nfat\n-1.8546\n0.196\n-9.451\n0.000\n-2.249\n-1.460\n\n\nsodium\n-0.0573\n0.001\n-38.480\n0.000\n-0.060\n-0.054\n\n\nfiber\n3.3924\n0.140\n24.228\n0.000\n3.111\n3.674\n\n\ncarbohydrates\n1.0424\n0.048\n21.634\n0.000\n0.946\n1.139\n\n\nsugar\n-0.7696\n0.054\n-14.272\n0.000\n-0.878\n-0.661\n\n\npotassium\n-0.0324\n0.005\n-6.648\n0.000\n-0.042\n-0.023\n\n\n\n\n\n\n\n\nOmnibus:\n78.045\nDurbin-Watson:\n2.069\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n801.691\n\n\nSkew:\n-3.864\nProb(JB):\n8.22e-175\n\n\nKurtosis:\n19.492\nCond. No.\n1.82e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.82e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\np-value for all the variables is low so all the variables are significantly affecting the response variable, rating. However the model output indicates there might be multicollinearity issue. Multicollinearity means the predictor variables are have high correlation among themselves. If we look at the correlation plot, fiber and potassium has 0.9 correlation which is high. One way to tackle multicollinearity is to consider principal component analysis (PCA). We will look at it in a while but let us first try to make predictions and look at the evaluation metrics.\n\n\nNow let us make prediction on the testing data and plot the observed vs.¬†predicted plot\n\n\n# Make predictions using testing data\npredictions = result.predict(data_test)\n\n# Observed vs. Predicted plot\nplt.figure(figsize=(20,7))\nplt.scatter(predictions, data_test[\"rating\"])\nplt.ylabel(\"Observed rating\")\nplt.xlabel(\"Predicted rating\")\n# Create the abline\nx_line = np.linspace(min(data_test[\"rating\"])-2, max(data_test[\"rating\"])+2, 100)\ny_line = 1 * x_line + 1\nplt.plot(x_line, y_line, color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe observed vs.¬†predicted looks good. However there is low number of data points and hence we should take this with a grain of salt. Let us check some evaluation metrics like the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nprint(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"rating\"],predictions),2))\nprint(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"rating\"],predictions))** 0.5,2))\n\nMean Absolute Error: 1.05\nRoot Mean Squared Error: 1.72\n\n\n\nRoot Mean Squared Error (RMSE) of 1.05 and Mean Absolute Error (MAE) of 1.72 is decent and indicates model is performing fairly well.\n\n\nNow, we will run regression model based on principal component analysis since it helps with multicollinearity.\n\n\n# Principal component analysis\nfrom sklearn.decomposition import PCA\n\n# separate the x and y variable for the training data first\ny_train = data_train.iloc[:,-1:]\nX0_train = data_train.iloc[:,:-1]\n\n# Standardize the predictor data first\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n# training data\nX_train_scaled = sc.fit_transform(X0_train)\n\n# Now calculate the principal components\nfrom sklearn.decomposition import PCA\npca = PCA()\nprincipalComponents = pca.fit_transform(X_train_scaled)\n# Training data\nX_train_pca = pd.DataFrame(data = principalComponents,\n             columns=['PC{}'.format(i+1)\n                      for i in range(principalComponents.shape[1])])\n\n\n\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance\n\narray([0.36079172, 0.24639   , 0.16186567, 0.1045944 , 0.06695072,\n       0.04472673, 0.00874256, 0.0059382 ])\n\n\n\nThe first six principal components explain around 98% of the data, so we will use the first six principal components to build a regression model.\n\n\nX_train_pca = pd.DataFrame(data = principalComponents,\n             columns=['PC{}'.format(i+1)\n                      for i in range(principalComponents.shape[1])])\n\n# combine the X and Y for the training data\ndata_train_pca = X_train_pca\ndata_train_pca.set_index(X0_train.index,inplace = True)\ndata_train_pca['rating'] = y_train\ndata_train_pca.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nrating\n\n\n\n\n11\n0.865400\n0.737424\n3.082256\n-0.797678\n-1.695220\n-0.565655\n-0.436512\n-0.175165\n50.764999\n\n\n25\n-1.656332\n-0.688910\n-0.969216\n1.205505\n0.111314\n-0.454199\n0.373535\n-0.159738\n31.435973\n\n\n19\n0.979620\n2.049689\n-0.266540\n-0.475020\n-0.352195\n1.032443\n-0.089196\n-0.058422\n40.448772\n\n\n38\n-0.939218\n-0.515171\n0.188656\n-0.014788\n0.070899\n0.306183\n0.027561\n0.007296\n36.523683\n\n\n10\n-2.157184\n1.069870\n-0.921099\n0.649449\n-0.525503\n0.549343\n0.091509\n-0.057116\n18.042851\n\n\n\n\n\n\n\n\n# Correlation plot for principal components\nplt.figure(figsize=(20,10))\nsns.heatmap(data_train_pca.corr().round(4),annot=True, cmap=\"viridis\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can observe that only rating variable has correlation with the principal components and the correlation between the principal components is 0. The correlation of rating with principal component 6 and 8 is considerably low and hence we will not use them in the model. So we will use the principal components 1,2,3,4,5 and 8 to build a regression model.\n\n\n# Now run the OLS regression model on the first five principal components\n# Fit the OLS regression\nresult_pca = smf.ols(\"rating ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC8\", data = data_train_pca).fit()\n# check the summary\nresult_pca.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.997\n\n\nModel:\nOLS\nAdj. R-squared:\n0.997\n\n\nMethod:\nLeast Squares\nF-statistic:\n3009.\n\n\nDate:\nSun, 25 May 2025\nProb (F-statistic):\n3.38e-63\n\n\nTime:\n11:08:00\nLog-Likelihood:\n-68.981\n\n\nNo. Observations:\n58\nAIC:\n152.0\n\n\nDf Residuals:\n51\nBIC:\n166.4\n\n\nDf Model:\n6\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n43.6488\n0.111\n392.171\n0.000\n43.425\n43.872\n\n\nPC1\n7.2019\n0.066\n109.932\n0.000\n7.070\n7.333\n\n\nPC2\n-5.1916\n0.079\n-65.488\n0.000\n-5.351\n-5.032\n\n\nPC3\n2.1011\n0.098\n21.481\n0.000\n1.905\n2.297\n\n\nPC4\n-2.6963\n0.122\n-22.160\n0.000\n-2.941\n-2.452\n\n\nPC5\n3.3596\n0.152\n22.091\n0.000\n3.054\n3.665\n\n\nPC8\n-7.8789\n0.511\n-15.429\n0.000\n-8.904\n-6.854\n\n\n\n\n\n\n\n\nOmnibus:\n59.001\nDurbin-Watson:\n2.019\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n340.488\n\n\nSkew:\n-2.843\nProb(JB):\n1.16e-74\n\n\nKurtosis:\n13.419\nCond. No.\n7.79\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\\(R^{2}\\) is 99.7% which is decent and all the predictor variables have a low p-value value. We make predictions using the test data and then plot the out of sample observed vs.¬†predicted. First we calculate the principal components of the testing data and then make the predictions.\n\n\n# X for testing data\nX0_test = data_test.iloc[:,:-1]\n\n# scaled test data\nX_test_scaled = sc.transform(X0_test)\n\n# calculate the principal components for the testing data\nX_test = pca.transform(X_test_scaled)\nX_test_pca = pd.DataFrame(data = X_test,\n             columns=['PC{}'.format(i+1)\n                      for i in range(X_test.shape[1])])\n# calculate the predictions\npredictions_pca = result_pca.predict(X_test_pca)\n\n\nNow we plot the out of sample predictions obtained from regression model using raw data as well as the predictions obtained from model using the six principal components on the same plot with different colors.\n\n\n# Observed vs. Predicted plot\nplt.figure(figsize=(20,7))\n\nplt.scatter(predictions, data_test[\"rating\"], label='raw model', color='black', marker='o')\nplt.scatter(predictions_pca, data_test[\"rating\"],  label='PCA model', color='blue', marker='o')\n# sns.regplot(y = data_test[\"calories\"],x = predictions,ci=None,line_kws={\"color\":\"red\"})\nplt.ylabel(\"Observed rating\")\nplt.xlabel(\"Predicted rating\")\nplt.legend()\n\n# Create the abline\nx_line = np.linspace(min(data_test[\"rating\"]), max(data_test[\"rating\"]), 100)\ny_line = 1 * x_line + 1\nplt.plot(x_line, y_line, color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe out of sample observed vs.¬†predicted plot looks decent with all the points just around the red line. WE look at the evaluation metrics for the model built using the principal components.\n\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nprint(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"rating\"],predictions_pca),2))\nprint(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"rating\"],predictions_pca))** 0.5,2))\n\nMean Absolute Error: 1.23\nRoot Mean Squared Error: 1.89\n\n\n\nFor the regression model using first six principal components, Root Mean Squared Error (RMSE) is 1.23 and Mean Absolute Error (MAE) is 1.89 which is slightly higher than the regression model using the raw data."
  },
  {
    "objectID": "portfolio/house_price/index.html",
    "href": "portfolio/house_price/index.html",
    "title": "Predictive model for house price using Gaussian Process regression",
    "section": "",
    "text": "Good houses are always in demand. However in the recent times, the price of house has increased exponentially. It will be interesting to identify the factors affecting the price of house. I found this dataset on UCI machine learning repository which gives the house price per unit area and related variables.\n\n\nThe idea is to build a predictive model to predict house price per unit area based on the variables like the age of house, etc. We will look at the exploratory data analysis first and later build prediction models. First let us access and process the data through R.\n\n\n# Load the packages\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(car) # to calculate the VIF values\nlibrary(GGally) # for pairs plot using ggplot framework\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(rstanarm)\nlibrary(tidyr)\n\n\n# Get starbucks data from github repo\npath = \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/real_estate/real_estate_valuation.csv\"\ndata0 &lt;- read.csv(path, header = TRUE)\n\n# Data processing\n# clean the column names\ncolnames(data0) &lt;- c(\"ID\", \"date\", \"house_age\", \"distance\", \"number_store\", \"latitude\", \"longitude\", \"price\")\n\n# Check the first 6 rows of the dataset\ndata0 |&gt; head()\n\n  ID     date house_age   distance number_store latitude longitude price\n1  1 2012.917      32.0   84.87882           10 24.98298  121.5402  37.9\n2  2 2012.917      19.5  306.59470            9 24.98034  121.5395  42.2\n3  3 2013.583      13.3  561.98450            5 24.98746  121.5439  47.3\n4  4 2013.500      13.3  561.98450            5 24.98746  121.5439  54.8\n5  5 2012.833       5.0  390.56840            5 24.97937  121.5425  43.1\n6  6 2012.667       7.1 2175.03000            3 24.96305  121.5125  32.1\n\n\n\nWe will focus on the 3 variables as follows\n\nhouse_age : age of house in years.\ndistance : distance to nearest MRT station in meters.\nnumber_store : the number of convenience stores in the living circle on foot.\nprice : Price per unit area where 1 unit is 1 ping = 3.3 sq. meter\n\nLet us look at the distribution of these 3 variables\n\n\n# Check the rows which do not have any entries\nind.na &lt;- sum(is.na(data0))\nind.na # No NA values\n\n[1] 0\n\n# Filter the data\n# column house_age, distance and price\n# data &lt;- data0 |&gt; select(c(house_age,distance,price))\ndata &lt;- data0[,c(\"house_age\",\"distance\",\"number_store\",\"price\")]\ndata |&gt; head()\n\n  house_age   distance number_store price\n1      32.0   84.87882           10  37.9\n2      19.5  306.59470            9  42.2\n3      13.3  561.98450            5  47.3\n4      13.3  561.98450            5  54.8\n5       5.0  390.56840            5  43.1\n6       7.1 2175.03000            3  32.1\n\n# Data for histogram\nmelted_data &lt;- melt(data)\n\n# Plot the histogram of all the variables\nggplot(melted_data,aes(value))+\n  geom_histogram(bins = 20)+\n  # geom_histogram(aes(y = after_stat(density)),bins = 20)+\n  facet_grid2(~variable, scales=\"free\")+theme_bw()\n\n\n\n\n\n\n\n\n\nHistogram does not give much information. Let us look at the correlation plot to get an idea of how the variables are correlated with each other.\n\n\n# correlation plot of all the variables\ncorr &lt;- round(cor(data), 2)\nggcorrplot(corr)\n\n\n\n\n\n\n\n\n\nhouse_age is not related to distance and number_store which is not surprising. distance variable is positively correlated with number of stores which again not surprising. However price is negatively correlated with distance to nearest MRT station as well as house age and positively correlated with number of stores in the vicinity which is again logical. Next we look at the pairs plot which will show the bivariate scatter plots as well as the correlation between each variables. Scatter plots in the last row is of interest as it shows the pairwise scatterplots where price is on the y axis and the other variables are on the x axis.\n\n\nggpairs(data)\n\n\n\n\n\n\n\n\n\nThe scatterplot of price vs.¬†distance does not look linear but more of curved. We will focus on predicting price based on the distance variable. We will look at a simple linear regression where we will try to predict price as a function of distance. First let us convert the price and distance variable to log scale and look at scatterplot of the same.\n\n\n# select the distance and price variable and convert to log scale\ndata2 &lt;- data |&gt; subset(,c(\"distance\", \"price\")) |&gt; log()\n\n# scatterplot of price vs. distance\nggplot(data2,aes(price,distance))+geom_point()+\n  labs(y = \"log(price)\", x=\"log(distance)\")\n\n\n\n\n\n\n\n\n\nOnce we convert the variables to log scale, the scatterplot looks more linear compared to the scatterplot where the variables are not on log scale. We will first look at simple linear regression\n\n\n# split the data into training and testing data\nseed &lt;- 23\nset.seed(seed)\n\nind &lt;- sample(floor(0.75*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train &lt;- data2[ind,]\n# Testing dataset\ndata_test &lt;- data2[-ind,]\n\n# Simple linear regression using raw data\nmodel &lt;- lm(price ~ distance, data = data_train)\nsummary(model)\n\n\nCall:\nlm(formula = price ~ distance, data = data_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.65672 -0.11925  0.00352  0.14145  0.95893 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.33983    0.08801   60.67   &lt;2e-16 ***\ndistance    -0.27701    0.01353  -20.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2608 on 308 degrees of freedom\nMultiple R-squared:  0.5763,    Adjusted R-squared:  0.5749 \nF-statistic: 418.9 on 1 and 308 DF,  p-value: &lt; 2.2e-16\n\n# Prediction on the testing dataset\ny_pred &lt;- predict(model, data_test)\n\n# Calculate residuals = observed - predicted\nresiduals &lt;- (data_test$price - y_pred)\n\n# Residual vs. predicted plot\nggplot(NULL,aes(y_pred,residuals))+geom_point()+\n  labs(y = \"Residuals\", x=\"Predicted price on log scale\")+ \n  geom_hline(yintercept = 0, colour = \"red\")\n\n\n\n\n\n\n\n\n\nThe residual plot shows a slight curved pattern which indicates the linearity assumption of the model is not satisfied. Hence our model is not reliable. This is not surprising since the scatterplot indicates a slight curved fit rather than a linear fit.\n\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred,data_test$price))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+\n  # lims(x=c(0,80),y=c(0,80))+\n  geom_abline()\n\n\n\n\n\n\n\n# Calculate RMSE\nrmse &lt;- (residuals)^2 |&gt; sum() |&gt; sqrt()\nround(rmse,2)\n\n[1] 2.43\n\n# Check the assumptions of the regression model\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\nThe model is has RMSE = 2.43 and the observed vs.¬†predicted plot is just about decent. We will now look at a Gaussian process model which can handle non linear relationships very well.\n\n\n\n\n# Read the GP STAN model\n\n# Read the STAN file\nfile_stan &lt;- \"GP.stan\"\n\n# Compile stan model\nmodel_stan &lt;- cmdstan_model(stan_file = file_stan,\n                            cpp_options = list(stan_threads = TRUE))\nmodel_stan$check_syntax()\n\n\nx_train &lt;- data2[ind,1]\ny_train &lt;- data2[ind,2]\nx_test &lt;- data2[-ind,1]\ny_test &lt;- data2[-ind,2]\n\nx_train &lt;- x_train |&gt; as.matrix()\nx_test &lt;- x_test |&gt; as.matrix()\n\nstandata &lt;- list(K = ncol(x_train),\n                 N1 = nrow(x_train),\n                 X1 = x_train,\n                 Y1 = y_train,\n                 N2 = nrow(x_test),\n                 X2 = x_test,\n                 Y2 = y_test)\n\nfit_optim &lt;- model_stan$optimize(data = standata,\n                                 seed = seed,\n                                 threads =  10)\n\nInitial log joint probability = -559.564 \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes  \n      14        256.76   0.000435624    0.00106958           1           1       18    \nOptimization terminated normally:  \n  Convergence detected: relative gradient magnitude is below tolerance \nFinished in  0.4 seconds.\n\nfsum_optim &lt;- as.data.frame(fit_optim$summary())\n\n# The optimized parameter would be \npar_ind &lt;- 2:4\nopt_pars &lt;- fsum_optim[par_ind,]\nopt_pars\n\n  variable estimate\n2   lambda 2.855100\n3    sigma 1.657640\n4      tau 0.248832\n\n# starting value of parameters\nstart_parameters &lt;- rep(list(list(lambda = opt_pars[1,2],\n                                  sigma = opt_pars[2,2],\n                                  tau = opt_pars[3,2])),4)\n\n# Run the MCMC with optimized values as the starting values\nfit &lt;- model_stan$sample(\n  data = standata,\n  init = start_parameters,\n  seed = seed,\n  iter_warmup = 1000,\n  iter_sampling = 1000,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000,\n  threads =  8,\n  save_warmup = FALSE)\n\nRunning MCMC with 4 parallel chains, with 8 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 405.5 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 408.7 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 428.2 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 439.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 420.5 seconds.\nTotal execution time: 439.8 seconds.\n\n# Summary\n# fit$summary()\n\n# Save the summary\nfsum &lt;- as.data.frame(fit$summary())\n\n# Plot posterior distribution of parameters\nbayesplot::color_scheme_set(\"gray\")\nbayesplot::mcmc_dens(fit$draws(c(\"lambda\",\"sigma\",\"tau\")))\n\n\n\n\n\n\n\n\n\nThe posterior distribution of parameters look good with unimodal distribution and the trace plots also show good mix so we can say the parameters have converged.\n\n\n# index for predictive mean and variance\npred_mean_ind &lt;- max(par_ind)+(1:length(y_test))\npred_var_ind &lt;- max(pred_mean_ind)+(1:length(y_test))\n\n# Prediction\ny_observed &lt;- y_test #observed\ny_predicted &lt;-  fsum[pred_mean_ind,c(2)] #predicted mean\ny_predicted_var &lt;-  fsum[pred_var_ind,c(2)] #predicted\nlb &lt;- y_predicted + qnorm(0.05,0,sqrt(y_predicted_var))\nub &lt;- y_predicted + qnorm(0.95,0,sqrt(y_predicted_var))  \n\n# Predictions with bounds\npred &lt;- ggplot(NULL,aes(x_test,y_predicted))+geom_line(linetype = 2)+\n  geom_line(aes(x_test,lb))+\n  geom_line(aes(x_test,ub))+\n  geom_point(aes(x_test,y_observed),col=\"red\")+\n  labs(y = \"log(price)\", x=\"log(distance)\")\npred\n\n\n\n\n\n\n\n\n\nThe black dashed line is the predictive mean price on log scale for distance on log scale. The black solid lines give the 95% upper bound and lower bound for predictions. The points indicated in red color are the observations in testing data set and we can see most of them are in the prediction bounds.\n\n\n# Observed vs predicted\novp_1d &lt;- ggplot(NULL,aes(y_predicted,y_observed))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+\n  geom_abline()\n\novp_1d\n\n\n\n\n\n\n\nrmse = sqrt(mean((y_observed-y_predicted)^2)) |&gt; round(2)\nrmse\n\n[1] 0.22\n\n\n\nObserved vs.¬†predicted plot also looks good with a few points away from the line. The RMSE improved from 2.43 to 0.22 which is a good improvement."
  },
  {
    "objectID": "portfolio/credit_approval/index.html",
    "href": "portfolio/credit_approval/index.html",
    "title": "Credit approval",
    "section": "",
    "text": "I found this dataset on UCI machine learning repository which gives the credit approval dataset for a Japanese credit agency. The variable names have been changed to generic names and the factor levels have been changed to general symbols. We will look to build a logistic regression based on the data and try to predict if credit is given or not.\n\n\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggh4x)\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(rstanarm)\n\n# Get data from github repo\npath &lt;- \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/credit/data/crx.data\"\ndata0 &lt;- read.table(path, sep=\",\")\n\n# Data processing\nhead(data0)\n\n  V1    V2    V3 V4 V5 V6 V7   V8 V9 V10 V11 V12 V13   V14 V15 V16\n1  b 30.83 0.000  u  g  w  v 1.25  t   t   1   f   g 00202   0   +\n2  a 58.67 4.460  u  g  q  h 3.04  t   t   6   f   g 00043 560   +\n3  a 24.50 0.500  u  g  q  h 1.50  t   f   0   f   g 00280 824   +\n4  b 27.83 1.540  u  g  w  v 3.75  t   t   5   t   g 00100   3   +\n5  b 20.17 5.625  u  g  w  v 1.71  t   f   0   f   s 00120   0   +\n6  b 32.08 4.000  u  g  m  v 2.50  t   f   0   t   g 00360   0   +\n\n\n\nIn this case, we will restrict ourselves to continuous response variables, namely V2, V3, V8, V11 and V15.\n\n\n# V16 is the response +/- so convert the response to 0 and 1\ndata0$response &lt;- ifelse(data0$V16 ==\"+\",1,ifelse(data0$V16 ==\"-\",0,NA))\ndata0$decision &lt;- ifelse(data0$V16 ==\"+\",\"credit_granted\",ifelse(data0$V16 ==\"-\",\"credit_not_granted\",NA)) \n\n# Check the first 6 rows of the dataset\ndata0 |&gt; head()\n\n  V1    V2    V3 V4 V5 V6 V7   V8 V9 V10 V11 V12 V13   V14 V15 V16 response\n1  b 30.83 0.000  u  g  w  v 1.25  t   t   1   f   g 00202   0   +        1\n2  a 58.67 4.460  u  g  q  h 3.04  t   t   6   f   g 00043 560   +        1\n3  a 24.50 0.500  u  g  q  h 1.50  t   f   0   f   g 00280 824   +        1\n4  b 27.83 1.540  u  g  w  v 3.75  t   t   5   t   g 00100   3   +        1\n5  b 20.17 5.625  u  g  w  v 1.71  t   f   0   f   s 00120   0   +        1\n6  b 32.08 4.000  u  g  m  v 2.50  t   f   0   t   g 00360   0   +        1\n        decision\n1 credit_granted\n2 credit_granted\n3 credit_granted\n4 credit_granted\n5 credit_granted\n6 credit_granted\n\n# Check the type of data\ndata0 |&gt; str()\n\n'data.frame':   690 obs. of  18 variables:\n $ V1      : chr  \"b\" \"a\" \"a\" \"b\" ...\n $ V2      : chr  \"30.83\" \"58.67\" \"24.50\" \"27.83\" ...\n $ V3      : num  0 4.46 0.5 1.54 5.62 ...\n $ V4      : chr  \"u\" \"u\" \"u\" \"u\" ...\n $ V5      : chr  \"g\" \"g\" \"g\" \"g\" ...\n $ V6      : chr  \"w\" \"q\" \"q\" \"w\" ...\n $ V7      : chr  \"v\" \"h\" \"h\" \"v\" ...\n $ V8      : num  1.25 3.04 1.5 3.75 1.71 ...\n $ V9      : chr  \"t\" \"t\" \"t\" \"t\" ...\n $ V10     : chr  \"t\" \"t\" \"f\" \"t\" ...\n $ V11     : int  1 6 0 5 0 0 0 0 0 0 ...\n $ V12     : chr  \"f\" \"f\" \"f\" \"t\" ...\n $ V13     : chr  \"g\" \"g\" \"g\" \"g\" ...\n $ V14     : chr  \"00202\" \"00043\" \"00280\" \"00100\" ...\n $ V15     : int  0 560 824 3 0 0 31285 1349 314 1442 ...\n $ V16     : chr  \"+\" \"+\" \"+\" \"+\" ...\n $ response: num  1 1 1 1 1 1 1 1 1 1 ...\n $ decision: chr  \"credit_granted\" \"credit_granted\" \"credit_granted\" \"credit_granted\" ...\n\n# Convert the data into appropriate factors or numbers\ndata0$V2 &lt;- data0$V2 |&gt; as.numeric()\ndata0$V3 &lt;- data0$V3 |&gt; as.numeric()\ndata0$V8 &lt;- data0$V8 |&gt; as.numeric()\ndata0$V11 &lt;- data0$V11 |&gt; as.numeric()\ndata0$V15 &lt;- data0$V15 |&gt; as.numeric()\n\n# Combine only numerical data along with the response\ndata1 &lt;- data0 |&gt; dplyr::select(response,V2,V3,V8,V11,V15)\ndata2 &lt;- data0 |&gt; dplyr::select(decision,V2,V3,V8,V11,V15)\n# data1 |&gt; str()\n\n# Check the number of NA values\nsum(is.na(data1))\n\n[1] 12\n\n# Exclude the rows which has NA values\ndata10 &lt;- na.omit(data1)\n\n\nWe look at the distribution of the continuous data variables based on if decision variable (credit given / credit not given)\n\n\n# Data for histogram\nmelted_data &lt;- melt(na.omit(data2), id=\"decision\")\n\n# Plot the histogram of all the variables\nggplot(melted_data,aes(value))+\n  geom_histogram(aes(),bins = 30)+\n  # geom_histogram(aes(y = after_stat(density)),bins = 20)+\n  facet_grid2(decision~variable, scales=\"free\")+theme_bw()\n\n\n\n\n\n\n\n\n\nThe distribution of the first two variables (V2 and V3) is similar across the decision. So we will exclude these two variables from the model as it is unlikely to have an impact on the decision.\n\n\n# Exclude V3 and V8 variables\ndata &lt;- data10[,-(2:3)]\n\n# split the data into training and testing data\nseed &lt;- 55\nset.seed(seed)\nind &lt;- sample(floor(0.75*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train &lt;- data[ind,]\n# Testing dataset\ndata_test &lt;- data[-c(ind),]\n\ndata |&gt; summary()\n\n    response            V8              V11              V15          \n Min.   :0.0000   Min.   : 0.000   Min.   : 0.000   Min.   :     0.0  \n 1st Qu.:0.0000   1st Qu.: 0.165   1st Qu.: 0.000   1st Qu.:     0.0  \n Median :0.0000   Median : 1.000   Median : 0.000   Median :     5.0  \n Mean   :0.4499   Mean   : 2.209   Mean   : 2.435   Mean   :  1021.2  \n 3rd Qu.:1.0000   3rd Qu.: 2.574   3rd Qu.: 3.000   3rd Qu.:   395.5  \n Max.   :1.0000   Max.   :28.500   Max.   :67.000   Max.   :100000.0  \n\n\n\n# Read the STAN file\nfile_stan &lt;- \"logistic_regression.stan\"\n\n# Compile stan model\nmodel_stan &lt;- cmdstan_model(stan_file = file_stan,\n                            cpp_options = list(stan_threads = TRUE))\nmodel_stan$check_syntax()\n\n\nNow that the model is compiled, we will prepare the data to supply to the model to estimate the parameters based on the training data and make predictions on the testing data.\n\n\n#Get the data in appropriate form to pass to STAN model\nx_train &lt;- data_train[,-1]\ny_train &lt;- data_train[,1] \nx_test &lt;- data_test[,-1]\ny_test &lt;- data_test[,1]\n\nx_train &lt;- x_train |&gt; as.matrix()\nx_test &lt;- x_test |&gt; as.matrix()\n\nstandata &lt;- list(K = ncol(x_train),\n                 N1 = nrow(x_train),\n                 X1 = x_train,\n                 Y1 = y_train,\n                 N2 = nrow(x_test),\n                 X2 = x_test,\n                 Y2 = y_test)\n\nfit_optim &lt;- model_stan$optimize(data = standata,\n                                 seed = seed,\n                                 threads =  10)\n\nInitial log joint probability = -93090.2 \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes  \n      99      -257.751    0.00427413       40.0854           1           1      164    \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes  \n     105      -257.738    0.00103664      0.596166           1           1      171    \nOptimization terminated normally:  \n  Convergence detected: relative gradient magnitude is below tolerance \nFinished in  0.1 seconds.\n\nfsum_optim &lt;- as.data.frame(fit_optim$summary())\n\n# The optimized parameter would be \npar_ind &lt;- 2:(ncol(x_train)+2)\nopt_pars &lt;- fsum_optim[par_ind,]\nopt_pars\n\n  variable     estimate\n2    alpha -1.319880000\n3  beta[1]  0.224366000\n4  beta[2]  0.333478000\n5  beta[3]  0.000385548\n\n# starting value of parameters\nstart_parameters &lt;- rep(list(list(alpha = opt_pars[1,2],\n                                  beta = opt_pars[-1,2])),4)\n\n# Run the MCMC with optimized values as the starting values\nfit &lt;- model_stan$sample(\n  data = standata,\n  init = start_parameters,\n  seed = seed,\n  iter_warmup = 10000,\n  iter_sampling = 10000,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 10000,\n  threads =  32,\n  save_warmup = FALSE)\n\nRunning MCMC with 4 parallel chains, with 32 thread(s) per chain...\n\nChain 1 Iteration:     1 / 20000 [  0%]  (Warmup) \nChain 2 Iteration:     1 / 20000 [  0%]  (Warmup) \nChain 3 Iteration:     1 / 20000 [  0%]  (Warmup) \nChain 4 Iteration:     1 / 20000 [  0%]  (Warmup) \nChain 2 Iteration: 10000 / 20000 [ 50%]  (Warmup) \nChain 2 Iteration: 10001 / 20000 [ 50%]  (Sampling) \nChain 1 Iteration: 10000 / 20000 [ 50%]  (Warmup) \nChain 1 Iteration: 10001 / 20000 [ 50%]  (Sampling) \nChain 3 Iteration: 10000 / 20000 [ 50%]  (Warmup) \nChain 3 Iteration: 10001 / 20000 [ 50%]  (Sampling) \nChain 4 Iteration: 10000 / 20000 [ 50%]  (Warmup) \nChain 4 Iteration: 10001 / 20000 [ 50%]  (Sampling) \nChain 2 Iteration: 20000 / 20000 [100%]  (Sampling) \nChain 2 finished in 22.2 seconds.\nChain 1 Iteration: 20000 / 20000 [100%]  (Sampling) \nChain 1 finished in 23.1 seconds.\nChain 3 Iteration: 20000 / 20000 [100%]  (Sampling) \nChain 3 finished in 24.0 seconds.\nChain 4 Iteration: 20000 / 20000 [100%]  (Sampling) \nChain 4 finished in 28.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 24.5 seconds.\nTotal execution time: 28.8 seconds.\n\n# Summary\nfit$summary()\n\n# A tibble: 175 √ó 10\n   variable       mean   median      sd     mad       q5      q95  rhat ess_bulk\n   &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lp__       -2.60e+2 -2.59e+2 1.41e+0 1.21e+0 -2.62e+2 -2.58e+2  1.00   14580.\n 2 alpha      -1.34e+0 -1.33e+0 1.48e-1 1.49e-1 -1.58e+0 -1.10e+0  1.00   15832.\n 3 beta[1]     2.28e-1  2.27e-1 4.57e-2 4.59e-2  1.55e-1  3.05e-1  1.00   16114.\n 4 beta[2]     3.40e-1  3.38e-1 5.22e-2 5.14e-2  2.56e-1  4.28e-1  1.00   17570.\n 5 beta[3]     4.08e-4  4.01e-4 1.19e-4 1.18e-4  2.25e-4  6.14e-4  1.00   38585.\n 6 Y2[1]       5.95e-1  5.95e-1 6.00e-2 6.01e-2  4.96e-1  6.94e-1  1.00   19639.\n 7 Y2[2]       3.64e-1  3.64e-1 2.51e-2 2.53e-2  3.24e-1  4.06e-1  1.00   23912.\n 8 Y2[3]       7.44e-1  7.45e-1 4.29e-2 4.30e-2  6.72e-1  8.14e-1  1.00   21883.\n 9 Y2[4]       4.08e-1  4.07e-1 2.97e-2 2.98e-2  3.60e-1  4.57e-1  1.00   27887.\n10 Y2[5]       9.41e-1  9.45e-1 2.62e-2 2.41e-2  8.92e-1  9.75e-1  1.00   19075.\n# ‚Ñπ 165 more rows\n# ‚Ñπ 1 more variable: ess_tail &lt;dbl&gt;\n\n# Save the summary\nfsum &lt;- as.data.frame(fit$summary())\n\n\nNext we look at the posterior distribution of the parameters and the trace plots. The posterior distribution of the parameters are unimodel and the trace plots indicates a good mix. So no issues with convergence.\n\n\n# Plot posterior distribution of parameters\nbayesplot::color_scheme_set(\"gray\")\nbayesplot::mcmc_dens(fit$draws(c(\"alpha\",\"beta\")))\n\n\n\n\n\n\n\n# Trace plots\nbayesplot::color_scheme_set(\"brewer-Spectral\")\nbayesplot::mcmc_trace(fit$draws(c(\"alpha\",\"beta\")))\n\n\n\n\n\n\n\n\n\nNow we check the prediction. The STAN model calculates the posterior probability. If the probability is greater than 1, we predict the response to be 1 and 0 otherwise. Based on the predictions, we will generate the confusion matrix.\n\n\n# Check the predictions\npred_ind &lt;- (max(par_ind)+1):(max(par_ind)+length(y_test))\n# predicted probability\npred_prob &lt;- fsum[pred_ind,2]\npred_outcome &lt;- ifelse(pred_prob&gt;0.5,1,0)\n\n# Generate the confusion matrix\nconf_matrix2 &lt;- caret::confusionMatrix(as.factor(pred_outcome),as.factor(y_test))\nconf_matrix2\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 100  21\n         1   9  40\n                                          \n               Accuracy : 0.8235          \n                 95% CI : (0.7578, 0.8777)\n    No Information Rate : 0.6412          \n    P-Value [Acc &gt; NIR] : 1.358e-07       \n                                          \n                  Kappa : 0.5991          \n                                          \n Mcnemar's Test P-Value : 0.04461         \n                                          \n            Sensitivity : 0.9174          \n            Specificity : 0.6557          \n         Pos Pred Value : 0.8264          \n         Neg Pred Value : 0.8163          \n             Prevalence : 0.6412          \n         Detection Rate : 0.5882          \n   Detection Prevalence : 0.7118          \n      Balanced Accuracy : 0.7866          \n                                          \n       'Positive' Class : 0               \n                                          \n\n# Accuracy\naccuracy &lt;- mean(pred_outcome == y_test)\nprint(paste('Accuracy is ',round(accuracy,4)))\n\n[1] \"Accuracy is  0.8235\"\n\n\n\nOur model has an accuracy of around 83% which indicates the model is correctly identifying the posiitve and negative cases in around 83% of the cases. Next, we look at the Receiver Operating Characteristic (ROC) curve. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR). It is the visualization of trade-off between correctly identifying positive cases and incorrectly identifying negative cases as positive. A good model ROC curve which goes from bottom left to top left which means the model is perfectly identifying positive cases and does not identify negatives as positive. On the other hand, a ROC curve which is a straight line from bottom left to to right with slope 1 indicates the model is randomly assigning the positive and negative cases. Our curve is somewhere in between these 2 extreme cases and is decent. The area under the curve (AUC) is around 79% which is also decent.\n\n\n# ROC curve\nlibrary(ROCR)\npr &lt;- prediction(pred_outcome, y_test)\nprf &lt;- performance(pr, measure = \"tpr\", x.measure = \"fpr\")\nplot(prf)\n\n\n\n\n\n\n\n# AUC \nauc &lt;- performance(pr, measure = \"auc\")\nauc &lt;- auc@y.values[[1]]\nauc\n\n[1] 0.7865844"
  }
]