---
title: "Breast Cancer Detection"
subtitle: "Breast cancer detection using Artificial intelligence"
author: "Aditya Ranade"
highlight-style:
            light: github
date: "2025-04-11"
categories: [analysis, R]
image: "./breast_cancer.jpg"
---

::: {style="text-align: justify"}
I found this [dataset](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) on UCI machine learning repository which gives the dataset for breast cancer detection. It has 10 basic variables which indicates different aspects of measurements in a medical examination. For the 10 variables, the dataset provided mean, sd and the worst measurement. We will focus on the mean measurement.
:::

```{r}
#| label: load-packages
#| echo: true
#| warning: false
#| include: true

library(reshape2)
library(ggplot2)
library(dplyr)
library(ggh4x)
library(GGally)
library(pROC)
library(glmnet)

# Get data from github repo
path <- "https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/breast_cancer/wdbc.data"
data00 <- read.table(path, sep = ",", header = FALSE)
data0 <- data00[,2:12] # Use only first 11 columns

# Data processing
head(data0)
# change column names
colnames(data0) <- c("diagnosis", "radius",
                     "texture", "perimeter",
                     "area","smoothness",
                     "compactness","concavity",
                     "concave_points","symmetry",
                     "fractal_dimension")

```

::: {style="text-align: justify"}
Since the measurements are taken on the same object, there is bound to be some correlation between the variables. For example, there will be strong correlation between radius, perimeter and area. So using all the three variables in a model will not be a good idea as it might not give a good reliable model.
:::

```{r}
#| label: EDA0
#| echo: true
#| warning: false
#| include: true
#| fig-width: 14
#| fig-height: 6

# data (select all the explanatory variables)
data <- data0 |> dplyr::select(radius, texture, perimeter,
                               area, smoothness, compactness, 
                               concavity, concave_points,
                               symmetry, fractal_dimension)

# convert the response to factor and categorical variable
data$response <- ifelse(data0$diagnosis=="B",0,
                        ifelse(data0$diagnosis=="M",1,99))

# Check the first 6 rows of the dataset
data |> head()

# Check the type of data
data |> str()

# Check the rows which do not have any entries
sum(is.na(data)) # no NA values
```

::: {style="text-align: justify"}
We look at the distribution of the continuous data variables based on if response variable is 1 which indicates the tumor is malignant (cancerous) or response variable is 0 which indicates the tumor is benign (non cancerous).
:::

```{r}
#| label: EDA1
#| echo: true
#| warning: false
#| include: true
#| fig-width: 14
#| fig-height: 6

# Data for histogram
melted_data <- melt(na.omit(data0), id="diagnosis")
melted_data$diagnosis <- ifelse(melted_data$diagnosis == "M","Malignant",
                                ifelse(melted_data$diagnosis == "B","Benign","NA"))


# Plot the histogram of all the variables
ggplot(melted_data,aes(value))+
  geom_histogram(aes(),bins = 30)+
  facet_grid2(diagnosis~variable, scales="free")+theme_bw()
```

::: {style="text-align: justify"}
There is a noticeable difference between the distribution of the variables for the two categories. Let us look at pairs plot which will help us understand the correlation between each pair of explanatory variables.
:::

```{r}
#| label: EDA3
#| echo: true
#| warning: false
#| include: true

# Pairs plot between the explanatory variables to 
# check correlation between each pair of the variables
ggpairs(data[,-ncol(data)])

```

::: {style="text-align: justify"}
As expected, there is multicollinearity in the data. One way to mitigate the effect of multicollinearity is to use $L2$ regularization (often called as ridge regression). Another way is to transform the data using principal component analysis (PCA) and use that data for regression. In this instance, we will first look at logistic regression and then logistic regression using L2 regularization. Let us look at the 
:::

::: {style="text-align: justify"}
Let us look at the results of logistic regression.
:::

```{r}
#| label: logistic_model
#| echo: true
#| warning: false
#| include: true

# split the data into training and testing data
seed <- 23
set.seed(seed)

ind <- sample(floor(0.8*nrow(data)),
              replace = FALSE)

# Training dataset
data_train <- data[ind,]

# Testing dataset
data_test <- data[-ind,]
###############################################################

# Fit an binary logistic regression model
model <- glm(response ~ ., data = data_train, family = binomial)

# Check the summary of the model
model |> summary()

# Prediction on the testing dataset
y_pred_prob <- predict(model, data_test,
                  type = "response")

y_pred <- ifelse(y_pred_prob>0.5,1,0)

# Generate the confusion matrix
conf_matrix <- caret::confusionMatrix(as.factor(y_pred),
                                      as.factor(data_test$response),
                                      positive = "1")
conf_matrix

```

::: {style="text-align: justify"}
Our logistic regression model has accuracy around 88.6% on the testing dataset. The misclassification rate on the testing data is (2+11)/114 = 0.1140. We will look at the ROC curve and AUC.
:::

```{r}
#| label: diagnostics_logistic_model
#| echo: TRUE
#| warning: false
#| include: true

# Compute ROC curve
roc_curve <- roc(data_test$response,as.vector(y_pred_prob))
# Calculate AUC
auc_value <- auc(roc_curve)

# Plot the ROC curve
plot(roc_curve, col = "blue", lwd = 3, main = "ROC Curve")
# Add AUC to the plot
legend("bottomright", legend = paste("AUC =", round(auc_value, 3)), col = "blue", lwd = 3)

```

::: {style="text-align: justify"}
We will now try the logistic regression with $L2$ regularization.
:::

```{r}
#| label: logistic_model_l2
#| echo: TRUE
#| warning: false
#| include: true

# Now try the logistic regression ridge (L2) regularization
model_l2 <- cv.glmnet(as.matrix(data_train[,-ncol(data_train)]), 
                      data_train[,ncol(data_train)], 
                      family = "binomial",
                      alpha=0)

# Plot cross-validation results 
plot(model_l2) 

# View the best lambda 
best_lambda_l2 <- model_l2$lambda.min 
print(best_lambda_l2) 

# Fit the final model with the best lambda 
final_model_l2 <- glmnet(as.matrix(data_train[,-ncol(data_train)]), 
                      data_train[,ncol(data_train)],
                      family = "binomial", 
                      alpha = 0, 
                      lambda = best_lambda_l2)

coef(final_model_l2,s = best_lambda_l2)

# Check the summary of the model
final_model_l2 |> summary()

# Prediction on the testing dataset
y_pred_prob_l2 <- predict(final_model_l2, 
                          as.matrix(data_test[,-ncol(data_test)]),
                          type = "response")

y_pred_l2 <- ifelse(y_pred_prob_l2>0.5,1,0)

# Generate the confusion matrix
conf_matrix_l2 <- caret::confusionMatrix(as.factor(y_pred_l2),as.factor(data_test$response))
conf_matrix_l2

```
::: {style="text-align: justify"}
Now the misclassification rate on the testing data is (2+4)/114 = 0.0526. We will now try the logistic regression with $L2$ regularization, which should improve the performance.
:::


```{r}
#| label: diagnostics_model_l2
#| echo: TRUE
#| warning: false
#| include: true

# Compute ROC curve
roc_curve_l2 <- roc(data_test$response,as.vector(y_pred_prob_l2))

# Plot the ROC curve
plot(roc_curve_l2, col = "blue", lwd = 3, main = "ROC Curve")

# Add AUC to the plot
auc_value_l2 <- auc(roc_curve_l2)
legend("bottomright", legend = paste("AUC =", round(auc_value_l2, 3)), col = "blue", lwd = 3)

```

::: {style="text-align: justify"}
Our model has an accuracy of around 94% which indicates the model is correctly identifying the positive and negative cases in around 94% of the cases. Next, we look at the Receiver Operating Characteristic (ROC) curve. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR). It is the visualization of trade-off between correctly identifying positive cases and incorrectly identifying negative cases as positive. A good model has ROC curve which goes from bottom left to top left which means the model is perfectly identifying positive cases and does not identify negatives as positive. On the other hand, a ROC curve which is a straight line from bottom left to to right with slope 1 indicates the model is randomly assigning the positive and negative cases. Our curve is somewhere in between these 2 extreme cases and is decent. The area under the curve (AUC) is around 99% which is also good. Let us compare the coefficients of the model parameters for the two models.
:::

```{r}
#| label: final_results
#| echo: true
#| warning: false
#| include: true
coef_model <- coef(model)
coef_model_l2 <- matrix(coef(final_model_l2,s = best_lambda_l2))

coef_combined <- data.frame(coef_model,coef_model_l2)
colnames(coef_combined) <- c("Logistic regression", "Logistic regression with L2 penalty")
coef_combined
```

::: {style="text-align: justify"}
This demonstrates that using L2 regularization in logistic regression where there is multicollinearity improves the model without any transformation. We can also try to build a model using the principal component analysis but that is for another day / dataset.
:::

```{r}
#| label: diagnostics3
#| echo: false
#| warning: false
#| include: false
# # ROC curve
# library(ROCR)
# pr <- prediction(y_pred_l2, data_test$response)
# prf <- performance(pr, measure = "tpr", x.measure = "fpr")
# 
# # AUC 
# auc <- performance(pr, measure = "auc")
# auc <- auc@y.values[[1]]
# auc
# 
# # Combine ROC curve and AUC 
# (
# plot(prf, col = "blue", lwd = 3, main = "ROC Curve")
# legend("bottomright", legend = paste("AUC =", round(auc, 3)), col = "blue", lwd = 3)
# )
```

