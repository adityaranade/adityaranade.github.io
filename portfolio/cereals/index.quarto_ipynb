{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Cereals nutritional information\"\n",
        "subtitle: \"Predicting calories cereals based on the nutritional contents\"\n",
        "author: \"Aditya Ranade\"\n",
        "highlight-style: github-light\n",
        "date: \"2025-02-13\"\n",
        "categories: [analysis, python]\n",
        "image: \"./cereals.jpg\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "::: {style=\"text-align: justify\"}\n",
        "Cereals are commonly consumed for breakfast. But how good are they in terms of the nutritional value? Can we predict the calories based on the nutritional contents ?\n",
        ":::\n",
        "\n",
        "::: {style=\"text-align: justify\"}\n",
        "I found this dataset on Kaggle which gives the nutritional information about their cereals. First, we look at the exploratory data analysis and later try some simple regression models. First let us access and process the data through python\n",
        ":::\n"
      ],
      "id": "bff9db05"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-packages\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# Load Libraries\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from plotnine import *\n",
        "import numpy as np # linear algebra\n",
        "# import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Get data from github repo\n",
        "\n",
        "path = \"https://raw.githubusercontent.com//adityaranade//portfolio//refs//heads//main//cereals//cereal.csv\"\n",
        "\n",
        "df0=pd.read_csv(path, encoding='unicode_escape')\n",
        "\n",
        "df0.head()"
      ],
      "id": "load-packages",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: data_processing1\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# modify the column names\n",
        "df0.columns = ['name', 'manufacturer','type','calories','protein','fat','sodium','fiber','carbohydrates','sugar','potassium','vitamins','shelf','weight','cups', 'rating']\n",
        "df0.head()"
      ],
      "id": "data_processing1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: data_processing2\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# select data for the histogram\n",
        "df = df0[[\"calories\", \"protein\", \"fat\", \"sodium\", \"fiber\", \"carbohydrates\", \"sugar\",\"potassium\",\"name\"]]\n",
        "df.head()\n",
        "\n",
        "# Use melt function for the histograms of variables \n",
        "df2 = pd.melt(df, id_vars=['name'])\n",
        "# df2.head()"
      ],
      "id": "data_processing2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "Now that we have the data ready, let us look at the histogram of each variables namely nutritional contents, specifically calories, protein, fat, sodium, fiber, carbo, sugars and potassium\n",
        ":::\n"
      ],
      "id": "54faf834"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: EDA\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "p = (\n",
        "    ggplot(df2, aes(\"value\"))\n",
        "    + geom_histogram(bins=10)\n",
        "    + facet_grid(\". ~ variable\", scales='free_x')\n",
        "    + theme(figure_size=(12, 3))\n",
        "    )\n",
        "\n",
        "# If we want the density on y axis\n",
        "# p = (\n",
        "#     ggplot(df2, aes(\"value\", after_stat(\"density\")))\n",
        "#     + geom_histogram(bins=10)\n",
        "#     + facet_grid(\". ~ variable\", scales='free_x')\n",
        "#     + theme(figure_size=(12, 3))\n",
        "#     )\n",
        "\n",
        "p.show()"
      ],
      "id": "EDA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The histogram of each of the variables do not show any problems as all the plots look decent. We will look at the correlation plot.\n"
      ],
      "id": "36656d23"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: EDA2\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "# Check the correlation between the variables\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(df.iloc[:,:-1].corr(),annot=True,cmap=\"viridis\")\n",
        "plt.show()"
      ],
      "id": "EDA2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "Calories variable has significant positive correlation with all the variables except fiber and potassium. This seems logical and will be useful when we build a regression model for the same. Next we take a look at the pairs plot which will give us idea about relationship between each pair of variables. Most important from the point of prediction is the first row where calories is the y axis and each of the variable is x axis.\n",
        ":::\n"
      ],
      "id": "d2bda5a0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: test\n",
        "#| echo: false\n",
        "#| warning: false\n",
        "#| include: false\n",
        "\n",
        "# def reg_coef(x,y,label=None,color=None,hue=None,**kwargs):\n",
        "#     ax = plt.gca()\n",
        "#     r,p = pearsonr(x,y)\n",
        "#     ax.annotate('r = {:.2f}'.format(r), xy=(0.5,0.5), xycoords='axes fraction', ha='center')\n",
        "#     ax.set_axis_off()\n",
        "# \n",
        "# g = sns.PairGrid(df.iloc[:,1:])\n",
        "# g.map_diag(sns.histplot)\n",
        "# g.map_upper(sns.scatterplot)\n",
        "# g.map_lower(reg_coef, hue=None)\n",
        "# g.add_legend()\n",
        "# plt.show()"
      ],
      "id": "test",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: EDA3\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# Pairs plot\n",
        "g = sns.PairGrid(df.iloc[:,1:])\n",
        "g.map_diag(sns.histplot)\n",
        "g.map_upper(sns.scatterplot)\n",
        "g.map_lower(sns.kdeplot)\n",
        "plt.show()"
      ],
      "id": "EDA3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "The scatterplots of each variable with calories which can be seen in the upper triangular plots in the very first row. It seems there is a linear association between calories and fat, carbs and protein. However, it does not seem to have a linear association with fiber.\n",
        ":::\n"
      ],
      "id": "4ab3a0d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: mlr_train\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# Split data into train and test set\n",
        "indices = range(len(df)) # Create a list of indices\n",
        "\n",
        "# Get 75% random indices\n",
        "random.seed(23) # for reproducible example\n",
        "random_indices = random.sample(indices, round(0.75*len(df)))\n",
        "\n",
        "# Training dataset\n",
        "data_train = df.iloc[random_indices,:-1]\n",
        "\n",
        "# Testing dataset\n",
        "data_test = df.iloc[df.index.difference(random_indices),:-1]\n",
        "\n",
        "# Build a multiple linear regression model to predict calories using other variables using training data\n",
        "result = smf.ols(\"calories ~ protein + fat + sodium + fiber + carbohydrates + sugar + potassium\", data = data_train).fit()\n",
        "# check the summary\n",
        "result.summary()"
      ],
      "id": "mlr_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "p-value for sodium, fiber and potassium is considerably high and hence these variables do not add help predict the calories. This might be due to multicollinearity (the predictor variables are have high correlation within themselves). If we look at the correlation plot, fiber and potassium has 0.9 correlation which is high. One way to tackle multicollinearity is to consider principal component analysis (PCA). We will look at it in a while but let us first try to make predictions and look at the evaluation metrics.\n",
        ":::\n",
        "\n",
        "::: {style=\"text-align: justify\"}\n",
        "Now let us make prediction on the testing data and plot the observed vs. predicted plot\n",
        ":::\n"
      ],
      "id": "004a37d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: prediction_test\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# Make predictions using testing data\n",
        "predictions = result.predict(data_test)\n",
        "\n",
        "# Observed vs. Predicted plot\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.scatter(predictions, data_test[\"calories\"])\n",
        "plt.ylabel(\"Observed calories\")\n",
        "plt.xlabel(\"Predicted calories\")\n",
        "# Create the abline\n",
        "x_line = np.linspace(min(data_test[\"calories\"]), max(data_test[\"calories\"]), 100)\n",
        "y_line = 1 * x_line + 1\n",
        "plt.plot(x_line, y_line, color='red')\n",
        "plt.show()"
      ],
      "id": "prediction_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "The observed vs. predicted looks good. However there is low number of data points and hence we should take this with a grain of salt. Let us check some evaluation metrics like the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n",
        ":::\n"
      ],
      "id": "d29cfab7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: evaluation_metrics\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
        "print(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"calories\"],predictions),2))\n",
        "print(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"calories\"],predictions))** 0.5,2))"
      ],
      "id": "evaluation_metrics",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "Root Mean Squared Error (RMSE) of 5.34 and Mean Absolute Error (MAE) of 6.89 is decent and indicates model is performing fairly well.\n",
        ":::\n",
        "\n",
        "::: {style=\"text-align: justify\"}\n",
        "Now, we will run regression model based on principal component analysis since it helps with multicollinearity.\n",
        ":::\n"
      ],
      "id": "1292eab6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: PCA_processing\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# Principal component analysis\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# separate the x and y variable for the training data first\n",
        "y_train = data_train.iloc[:,:1]\n",
        "X0_train = data_train.iloc[:,1:]\n",
        "\n",
        "# Standardize the predictor data first\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "# training data\n",
        "X_train_scaled = sc.fit_transform(X0_train)\n",
        "\n",
        "# Now calculate the principal components\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "principalComponents = pca.fit_transform(X_train_scaled)\n",
        "# Training data\n",
        "X_train_pca = pd.DataFrame(data = principalComponents,\n",
        "             columns=['PC{}'.format(i+1)\n",
        "                      for i in range(principalComponents.shape[1])])\n",
        "\n",
        "\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "explained_variance"
      ],
      "id": "PCA_processing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "The first six principal components explain around 99% of the data, so we will use the first 6 principal components to build a regression model.\n",
        ":::\n"
      ],
      "id": "90aebdf1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: PCA_processing2\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "X_train_pca = pd.DataFrame(data = principalComponents,\n",
        "             columns=['PC{}'.format(i+1)\n",
        "                      for i in range(principalComponents.shape[1])])\n",
        "\n",
        "# combine the X and Y for the training data\n",
        "data_train_pca = X_train_pca\n",
        "data_train_pca.set_index(X0_train.index,inplace = True)\n",
        "data_train_pca['calories'] = y_train\n",
        "data_train_pca.head()"
      ],
      "id": "PCA_processing2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: correlation_plot\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# Correlation plot for principal components\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(data_train_pca.corr().round(4),annot=True, cmap=\"viridis\")\n",
        "plt.show()"
      ],
      "id": "correlation_plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "We can observe that only calories variable has correlation with the principal components and the correlation between the principal components is practically 0. So we will use the principal components to build a regression model.\n",
        ":::\n"
      ],
      "id": "20ae4cf0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: PCA_model\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# Now run the OLS regression model on the first five principal components\n",
        "# Fit the OLS regression\n",
        "result_pca = smf.ols(\"calories ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6\", data = data_train_pca).fit()\n",
        "# check the summary\n",
        "result_pca.summary()"
      ],
      "id": "PCA_model",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "$R^{2}$ is 77.4% which is decent and all the predictor variables have a low p-value value. We make predictions using the test data and then plot the out of sample observed vs. predicted. First we calculate the principal components of the testing data and then make the predictions.\n",
        ":::\n"
      ],
      "id": "56e462b3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: PCA_test_data\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# X for testing data\n",
        "X0_test = data_test.iloc[:,1:]\n",
        "\n",
        "# scaled test data\n",
        "X_test_scaled = sc.transform(X0_test)\n",
        "\n",
        "# calculate the principal components for the testing data\n",
        "X_test = pca.transform(X_test_scaled)\n",
        "X_test_pca = pd.DataFrame(data = X_test,\n",
        "             columns=['PC{}'.format(i+1)\n",
        "                      for i in range(X_test.shape[1])])\n",
        "# calculate the predictions\n",
        "predictions_pca = result_pca.predict(X_test_pca)"
      ],
      "id": "PCA_test_data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "Now we plot the out of sample predictions obtained from regression model using raw data as well as the predictions obtained from model using the first six principal components on the same plot with different colors.\n",
        ":::\n"
      ],
      "id": "752db202"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: PCA_predicted\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "# Observed vs. Predicted plot\n",
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.scatter(predictions, data_test[\"calories\"], label='raw', color='black', marker='o')\n",
        "plt.scatter(predictions_pca, data_test[\"calories\"],  label='PCA', color='blue', marker='o')\n",
        "# sns.regplot(y = data_test[\"calories\"],x = predictions,ci=None,line_kws={\"color\":\"red\"})\n",
        "plt.ylabel(\"Observed calories\")\n",
        "plt.xlabel(\"Predicted calories\")\n",
        "plt.legend()\n",
        "\n",
        "# Create the abline\n",
        "x_line = np.linspace(min(data_test[\"calories\"]), max(data_test[\"calories\"]), 100)\n",
        "y_line = 1 * x_line + 1\n",
        "plt.plot(x_line, y_line, color='red')\n",
        "plt.show()"
      ],
      "id": "PCA_predicted",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "The out of sample observed vs. predicted plot looks decent with all the points just around the red line. WE look at the evaluation metrics for the model built using the principal components.\n",
        ":::\n"
      ],
      "id": "65e1683c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: PCA_metrics\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| include: true\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
        "print(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"calories\"],predictions_pca),2))\n",
        "print(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"calories\"],predictions_pca))** 0.5,2))"
      ],
      "id": "PCA_metrics",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {style=\"text-align: justify\"}\n",
        "For the regression model using first six principal components, Root Mean Squared Error (RMSE) is 4.66 and Mean Absolute Error (MAE) is 6.11 which is an improvement from the regression model using the raw data.\n",
        ":::"
      ],
      "id": "b4723433"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python313\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}