---
title: "Starbucks beverages nutritional information"
subtitle: "We all like starbucks beverages, but what about the nutritional value of them?"
author: "Aditya Ranade"
highlight-style:
            light: github
date: "2025-04-23"
categories: [analysis, R]
image: "./starbucks_beverages.jpg"
---

::: {style="text-align: justify"}
Starbucks is one of the most valued coffee chain in the world. A lot of people like to consume the beverages available at starbucks. But how good are they in terms of the nutritional value?
:::

::: {style="text-align: justify"}
I found this dataset on Kaggle which gives the nutritional information about their beverages. We will look at the exploratory data analysis first and later try some simple prediction models. First let us access and process the data through R.
:::

```{r}
#| label: load-packages
#| echo: true
#| warning: false
#| include: true

# Load the packages
library(reshape2)
library(ggplot2)
library(ggh4x)
library(ggcorrplot)
library(car) # to calculate the VIF values
library(GGally) # for pairs plot using ggplot framework
```

```{r}
#| label: data_processing1
#| echo: true
#| warning: false
#| include: true

# Get starbucks data from github repo
path <- "https://raw.githubusercontent.com/adityaranade/starbucks/refs/heads/main/data/starbucks-menu-nutrition-drinks.csv"
data0 <- read.csv(path, header = TRUE)

# Data processing
# change the column names
colnames(data0) <- c("name", "calories", "fat", 
                     "carbs", "fiber","protein", 
                     "sodium")

# Check the first 6 rows of the dataset
data0 |> head()

# Check the type of data
data0 |> str()

```

::: {style="text-align: justify"}
The data from second column should be numeric but shows as character. So we first convert it into numeric form and also exclude the rows with missing information
:::

```{r}
#| label: data_processing2
#| echo: true
#| warning: false
#| include: true

# convert the data to numeric second row onwards
data0$calories <- as.numeric(data0$calories)
data0$fat <- as.numeric(data0$fat)
data0$carbs <- as.numeric(data0$carbs)
data0$fiber <- as.numeric(data0$fiber)
data0$protein <- as.numeric(data0$protein)
data0$sodium <- as.numeric(data0$sodium)

# Check the type of data again
data0 |> str()

# Check the rows which do not have any entries
ind.na <- which(is.na(data0[,2]))
length(ind.na) # 85 NA values

# exclude the rows which has NA values 
data <- data0[-ind.na,]
```

::: {style="text-align: justify"}
Now that we have the data ready, let us look at the histogram each of the variables namely calories, fat, carbs, fiber, protein and sodium
:::

```{r}
#| label: EDA
#| echo: true
#| warning: false
#| include: true

# Data for histogram
melted_data <- melt(data, id.vars="name")

# Plot the histogram of all the variables
ggplot(melted_data,aes(value))+
  # geom_histogram(aes(y = after_stat(density)),bins = 20)+
  geom_histogram(bins = 20)+
  facet_grid2(~variable, scales="free")
```

::: {style="text-align: justify"}
Histogram does not give much information. Let us look at the correlation plot to get an idea of how the variables are correlated with each other.
:::

```{r}
#| label: correlation_plot
#| echo: true
#| warning: false
#| include: true

# correlation plot of all the variables
corr <- round(cor(data[,-1]), 1)
p.mat <- cor_pmat(mtcars) # correlation p-value
# Barring the no significant coefficient
ggcorrplot(corr, hc.order = TRUE,
           type = "lower", p.mat = p.mat)
# All positive correlation
```

::: {style="text-align: justify"}
All the variables are positively correlated (which indicates when one variable increases, the other variable will increase as well. ) which is not a surprising. Most important part is the correlation of calories with all the other variables are considerably high. Next we look at the pairs plot which will show the bivariate scatter plots as well as the correlation between each variables.
:::

```{r}
#| label: pairplots
#| echo: true
#| warning: false
#| include: true

ggpairs(data,columns = 2:ncol(data),
        lower = list(continuous = "smooth"))

```

::: {style="text-align: justify"}
Most of the bivariate scatter plots indicate a linear relationship between the variables. The most important result according to us is the relationship between calories with all the other variables. We can now use the dataset for predictions where we try to predict the calories based o the fat, carb, fiber, protein and sodium content using multiple linear regression.
:::

```{r}
#| label: MLR_raw
#| echo: true
#| warning: false
#| include: true

# split the data into training and testing data
seed <- 23
set.seed(seed)

ind <- sample(floor(0.8*nrow(data)),
              replace = FALSE)

# Training dataset
data_train <- data[ind,-1]
# Testing dataset
data_test <- data[-ind,-1]

# Multiple linear regression using raw data
model <- lm(calories ~ fat + carbs + fiber + protein + sodium, data = data_train)
summary(model)

# Prediction on the testing dataset
y_pred <- predict(model, data_test)

# Create a observed vs. predicted plot
ggplot(NULL,aes(y_pred,data_test$calories))+geom_point()+
  labs(y = "Observed", x="Predicted")+geom_abline()

# Calculate RMSE
rmse <- (y_pred-data_test$calories)^2 |> sum() |> sqrt()
rmse

# Check the variance inflation factor
vif_values <- vif(model)
vif_values

# Check the assumptions of the regression model
# par(mfrow = c(2, 2))
# plot(model)
```

::: {style="text-align: justify"}
The model is decent with RMSE 102.33 and the observed vs. predicted plot also looks decent. However the variation inflation factor (VIF) value for protein and sodium is higher than 10 which indicates that these two variables are highly correlated with at least one other input variable and hence the variation of these variables is inflated. This might lead to unreliable models. One way to mitigate the multicollinearity problem is to use principal components in place of the correlated variables.
:::

::: {style="text-align: justify"}
We will create principal components and look how much variation is explained by each of the principal components.
:::

```{r}
#| label: MLR_PC
#| echo: true
#| warning: false
#| include: true

pc <- prcomp(data[,-(1:2)],
             center = TRUE,
             scale. = TRUE)
attributes(pc)

# Check the factor loading of the principal components
print(pc)
# Check the summary of the principal components
summary(pc)
```

::: {style="text-align: justify"}
The first four principal components explain around 96.72 % of the variation in the data. We will use the first four principal components for the regression model.
:::

```{r}
data_pc <- cbind(data[,1:2],pc$x)
# training data
data_pc_train <- data_pc[ind,-1]
# testing data
data_pc_test <- data_pc[-ind,-1]

# Multiple linear regression using PC
model_pc <- lm(calories ~ PC1 + PC2 + PC3 + PC4, data = data_pc_train)
summary(model_pc)

# Prediction on the testing dataset
y_pred_pc <- predict(model_pc, data_pc_test)

# Create a observed vs. predicted plot
ggplot(NULL,aes(y_pred_pc,data_test$calories))+geom_point()+
  labs(y = "Observed", x="Predicted")+geom_abline()

# Calculate RMSE
rmse <- (y_pred_pc-data_pc_test$calories)^2 |> sum() |> sqrt()
rmse

# Check the variance inflation factor
vif_values_pc <- vif(model_pc)
vif_values_pc

# Check the assumptions of the regression model
# par(mfrow = c(2, 2))
# plot(model_pc)
```

::: {style="text-align: justify"}
RMSE for the regression model using the first four principal components is 63.73. The variation inflation factor is less than 1.5 for all the principal components. So using less variables with principal components gives much better predictions.
:::
