{
  "hash": "650bf652456d9d022cf885a1ecdbd009",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Starbucks beverages nutritional information\"\nsubtitle: \"We all like starbucks beverages, but what about the nutritional value of them?\"\nauthor: \"Aditya Ranade\"\nhighlight-style:\n            light: github\ndate: \"2025-04-23\"\ncategories: [analysis]\nimage: \"./starbucks_beverages.jpg\"\n---\n\n\n\n::: {style=\"text-align: justify\"}\nStarbucks is one of the most valued coffee chain in the world. A lot of people like to consume the beverages available at starbucks. But how good are they in terms of the nutritional value?\n:::\n\n::: {style=\"text-align: justify\"}\nI found this dataset on Kaggle which gives the nutritional information about their beverages. We will look at the exploratory data analysis first and later try some simple prediction models. First let us access and process the data through R.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the packages\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(car) # to calculate the VIF values\nlibrary(GGally) # for pairs plot using ggplot framework\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get starbucks data from github repo\npath <- \"https://raw.githubusercontent.com/adityaranade/starbucks/refs/heads/main/data/starbucks-menu-nutrition-drinks.csv\"\ndata0 <- read.csv(path, header = TRUE)\n\n# Data processing\n# change the column names\ncolnames(data0) <- c(\"name\", \"calories\", \"fat\", \n                     \"carbs\", \"fiber\",\"protein\", \n                     \"sodium\")\n\n# Check the first 6 rows of the dataset\ndata0 |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                                name calories fat carbs fiber\n1           Cool Lime Starbucks Refreshers™ Beverage       45   0    11     0\n2                                   Ombré Pink Drink        -   -     -     -\n3                                         Pink Drink        -   -     -     -\n4     Strawberry Acai Starbucks Refreshers™ Beverage       80   0    18     1\n5 Very Berry Hibiscus Starbucks Refreshers™ Beverage       60   0    14     1\n6                                       Violet Drink        -   -     -     -\n  protein sodium\n1       0     10\n2       -      -\n3       -      -\n4       0     10\n5       0     10\n6       -      -\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the type of data\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t177 obs. of  7 variables:\n $ name    : chr  \"Cool Lime Starbucks Refreshers™ Beverage\" \"Ombré Pink Drink\" \"Pink Drink\" \"Strawberry Acai Starbucks Refreshers™ Beverage\" ...\n $ calories: chr  \"45\" \"-\" \"-\" \"80\" ...\n $ fat     : chr  \"0\" \"-\" \"-\" \"0\" ...\n $ carbs   : chr  \"11\" \"-\" \"-\" \"18\" ...\n $ fiber   : chr  \"0\" \"-\" \"-\" \"1\" ...\n $ protein : chr  \"0\" \"-\" \"-\" \"0\" ...\n $ sodium  : chr  \"10\" \"-\" \"-\" \"10\" ...\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe data from second column should be numeric but shows as character. So we first convert it into numeric form and also exclude the rows with missing information\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert the data to numeric second row onwards\ndata0$calories <- as.numeric(data0$calories)\ndata0$fat <- as.numeric(data0$fat)\ndata0$carbs <- as.numeric(data0$carbs)\ndata0$fiber <- as.numeric(data0$fiber)\ndata0$protein <- as.numeric(data0$protein)\ndata0$sodium <- as.numeric(data0$sodium)\n\n# Check the type of data again\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t177 obs. of  7 variables:\n $ name    : chr  \"Cool Lime Starbucks Refreshers™ Beverage\" \"Ombré Pink Drink\" \"Pink Drink\" \"Strawberry Acai Starbucks Refreshers™ Beverage\" ...\n $ calories: num  45 NA NA 80 60 NA NA NA 110 0 ...\n $ fat     : num  0 NA NA 0 0 NA NA NA 0 0 ...\n $ carbs   : num  11 NA NA 18 14 NA NA NA 28 0 ...\n $ fiber   : num  0 NA NA 1 1 NA NA NA 0 0 ...\n $ protein : num  0 NA NA 0 0 NA NA NA 0 0 ...\n $ sodium  : num  10 NA NA 10 10 NA NA NA 5 0 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the rows which do not have any entries\nind.na <- which(is.na(data0[,2]))\nlength(ind.na) # 85 NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 85\n```\n\n\n:::\n\n```{.r .cell-code}\n# exclude the rows which has NA values \ndata <- data0[-ind.na,]\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNow that we have the data ready, let us look at the histogram each of the variables namely calories, fat, carbs, fiber, protein and sodium\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data for histogram\nmelted_data <- melt(data, id.vars=\"name\")\n\n# Plot the histogram of all the variables\nggplot(melted_data,aes(value))+\n  # geom_histogram(aes(y = after_stat(density)),bins = 20)+\n  geom_histogram(bins = 20)+\n  facet_grid2(~variable, scales=\"free\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/EDA-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nHistogram does not give much information. Let us look at the correlation plot to get an idea of how the variables are correlated with each other.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# correlation plot of all the variables\ncorr <- round(cor(data[,-1]), 1)\np.mat <- cor_pmat(mtcars) # correlation p-value\n# Barring the no significant coefficient\nggcorrplot(corr, hc.order = TRUE,\n           type = \"lower\", p.mat = p.mat)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/correlation_plot-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# All positive correlation\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nAll the variables are positively correlated (which indicates when one variable increases, the other variable will increase as well. ) which is not a surprising. Most important part is the correlation of calories with all the other variables are considerably high. Next we look at the pairs plot which will show the bivariate scatter plots as well as the correlation between each variables.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggpairs(data,columns = 2:ncol(data),\n        lower = list(continuous = \"smooth\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/pairplots-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nMost of the bivariate scatter plots indicate a linear relationship between the variables. The most important result according to us is the relationship between calories with all the other variables. We can now use the dataset for predictions where we try to predict the calories based o the fat, carb, fiber, protein and sodium content using multiple linear regression.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split the data into training and testing data\nseed <- 23\nset.seed(seed)\n\nind <- sample(floor(0.8*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train <- data[ind,-1]\n# Testing dataset\ndata_test <- data[-ind,-1]\n\n# Multiple linear regression using raw data\nmodel <- lm(calories ~ fat + carbs + fiber + protein + sodium, data = data_train)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = calories ~ fat + carbs + fiber + protein + sodium, \n    data = data_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.2823  -1.7523   0.0195   2.1092   8.1259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.75158    0.94777   1.848   0.0690 .  \nfat          7.65711    0.25133  30.466  < 2e-16 ***\ncarbs        3.79744    0.04057  93.599  < 2e-16 ***\nfiber        2.51563    0.96443   2.608   0.0112 *  \nprotein      0.75585    0.31682   2.386   0.0199 *  \nsodium       0.32290    0.03265   9.889 1.01e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.131 on 67 degrees of freedom\nMultiple R-squared:  0.9976,\tAdjusted R-squared:  0.9974 \nF-statistic:  5493 on 5 and 67 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred <- predict(model, data_test)\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred,data_test$calories))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+geom_abline()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/MLR_raw-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate RMSE\nrmse <- (y_pred-data_test$calories)^2 |> sum() |> sqrt()\nrmse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 102.3342\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the variance inflation factor\nvif_values <- vif(model)\nvif_values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      fat     carbs     fiber   protein    sodium \n 3.849325  1.266592  3.382455 10.380709 11.704360 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the assumptions of the regression model\n# par(mfrow = c(2, 2))\n# plot(model)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe model is decent with RMSE 102.33 and the observed vs. predicted plot also looks decent. However the variation inflation factor (VIF) value for protein and sodium is higher than 10 which indicates that these two variables are highly correlated with at least one other input variable and hence the variation of these variables is inflated. This might lead to unreliable models. One way to mitigate the multicollinearity problem is to use principal components in place of the correlated variables.\n:::\n\n::: {style=\"text-align: justify\"}\nWe will create principal components and look how much variation is explained by each of the principal components.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npc <- prcomp(data[,-(1:2)],\n             center = TRUE,\n             scale. = TRUE)\nattributes(pc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$names\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n$class\n[1] \"prcomp\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the factor loading of the principal components\nprint(pc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard deviations (1, .., p=5):\n[1] 1.7690827 0.8908878 0.7580688 0.5812294 0.4051785\n\nRotation (n x k) = (5 x 5):\n              PC1        PC2        PC3        PC4         PC5\nfat     0.4610104  0.3810085  0.1092277  0.7933177  0.03190861\ncarbs   0.3784887 -0.5448572 -0.7329252  0.1443775 -0.04304364\nfiber   0.3758452 -0.6263993  0.6433047 -0.0153384  0.22866603\nprotein 0.5161622  0.1647649  0.1161063 -0.3649765 -0.74815813\nsodium  0.4863462  0.3720747 -0.1535201 -0.4651440  0.62056454\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the summary of the principal components\nsummary(pc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5\nStandard deviation     1.7691 0.8909 0.7581 0.58123 0.40518\nProportion of Variance 0.6259 0.1587 0.1149 0.06757 0.03283\nCumulative Proportion  0.6259 0.7847 0.8996 0.96717 1.00000\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe first four principal components explain around 96.72 % of the variation in the data. We will use the first four principal components for the regression model.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_pc <- cbind(data[,1:2],pc$x)\n# training data\ndata_pc_train <- data_pc[ind,-1]\n# testing data\ndata_pc_test <- data_pc[-ind,-1]\n\n# Multiple linear regression using PC\nmodel_pc <- lm(calories ~ PC1 + PC2 + PC3 + PC4, data = data_pc_train)\nsummary(model_pc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = calories ~ PC1 + PC2 + PC3 + PC4, data = data_pc_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3780  -1.5028   0.7182   2.2345   7.4206 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 135.7766     0.5642  240.67   <2e-16 ***\nPC1          49.2547     0.3746  131.47   <2e-16 ***\nPC2         -12.8205     0.9443  -13.58   <2e-16 ***\nPC3         -40.1362     0.9209  -43.59   <2e-16 ***\nPC4          22.9143     0.9862   23.23   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.442 on 68 degrees of freedom\nMultiple R-squared:  0.9971,\tAdjusted R-squared:  0.997 \nF-statistic:  5935 on 4 and 68 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_pc <- predict(model_pc, data_pc_test)\n\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred_pc,data_test$calories))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+geom_abline()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate RMSE\nrmse <- (y_pred_pc-data_pc_test$calories)^2 |> sum() |> sqrt()\nrmse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 63.72554\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the variance inflation factor\nvif_values_pc <- vif(model_pc)\nvif_values_pc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     PC1      PC2      PC3      PC4 \n1.129946 1.471886 1.373721 1.160650 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the assumptions of the regression model\n# par(mfrow = c(2, 2))\n# plot(model_pc)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nRMSE for the regression model using the first four principal components is 63.73. The variation inflation factor is less than 1.5 for all the principal components. So using less variables with principal components gives much better predictions.\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}