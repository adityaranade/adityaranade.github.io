{
  "hash": "16f757f0a366974b894f96ead04f3e8d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Rating of cereals based on nutritional information\"\nsubtitle: \"Can we predict the rating of cereals based on nutritional information?\"\nauthor: \"Aditya Ranade\"\nhighlight-style: github-light\ndate: \"2025-04-10\"\ncategories: [analysis, python]\nimage: \"./cereals.jpg\"\njupyter: python3\n---\n\n\n::: {style=\"text-align: justify\"}\nCereals are commonly consumed for breakfast and there are plenty of options available for cereals. I found this dataset on Kaggle which gives the nutritional information about cereals as well as the ratings. It is not clear where the rating come from, but I think they are the average ratings from the customers. Can we predict the ratings based on the nutritional information of cereals ? First, we look at the exploratory data analysis and later try some simple regression models. First let us access and process the data through python.\n:::\n\n::: {#cell-load-packages .cell execution_count=1}\n``` {.python .cell-code}\n# Load Libraries\n\n# Load Libraries\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom plotnine import *\nimport numpy as np # linear algebra\n# import statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom scipy.stats import pearsonr\n\n# Get starbucks data from github repo\ndf0=pd.read_csv(\"https://raw.githubusercontent.com//adityaranade//cereals//refs//heads//main//cereal.csv\", encoding='unicode_escape')\ndf0.head()\n```\n\n::: {#load-packages .cell-output .cell-output-display execution_count=69}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>mfr</th>\n      <th>type</th>\n      <th>calories</th>\n      <th>protein</th>\n      <th>fat</th>\n      <th>sodium</th>\n      <th>fiber</th>\n      <th>carbo</th>\n      <th>sugars</th>\n      <th>potass</th>\n      <th>vitamins</th>\n      <th>shelf</th>\n      <th>weight</th>\n      <th>cups</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100% Bran</td>\n      <td>N</td>\n      <td>C</td>\n      <td>70</td>\n      <td>4</td>\n      <td>1</td>\n      <td>130</td>\n      <td>10.0</td>\n      <td>5.0</td>\n      <td>6</td>\n      <td>280</td>\n      <td>25</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.33</td>\n      <td>68.402973</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100% Natural Bran</td>\n      <td>Q</td>\n      <td>C</td>\n      <td>120</td>\n      <td>3</td>\n      <td>5</td>\n      <td>15</td>\n      <td>2.0</td>\n      <td>8.0</td>\n      <td>8</td>\n      <td>135</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>1.00</td>\n      <td>33.983679</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>All-Bran</td>\n      <td>K</td>\n      <td>C</td>\n      <td>70</td>\n      <td>4</td>\n      <td>1</td>\n      <td>260</td>\n      <td>9.0</td>\n      <td>7.0</td>\n      <td>5</td>\n      <td>320</td>\n      <td>25</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.33</td>\n      <td>59.425505</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>All-Bran with Extra Fiber</td>\n      <td>K</td>\n      <td>C</td>\n      <td>50</td>\n      <td>4</td>\n      <td>0</td>\n      <td>140</td>\n      <td>14.0</td>\n      <td>8.0</td>\n      <td>0</td>\n      <td>330</td>\n      <td>25</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.50</td>\n      <td>93.704912</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Almond Delight</td>\n      <td>R</td>\n      <td>C</td>\n      <td>110</td>\n      <td>2</td>\n      <td>2</td>\n      <td>200</td>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>8</td>\n      <td>-1</td>\n      <td>25</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.75</td>\n      <td>34.384843</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#cell-data_processing1 .cell execution_count=2}\n``` {.python .cell-code}\n# modify the column names\ndf0.columns = ['name', 'manufacturer','type','calories','protein','fat','sodium','fiber','carbohydrates','sugar','potassium','vitamins','shelf','weight','cups', 'rating']\ndf0.head()\n```\n\n::: {#data_processing1 .cell-output .cell-output-display execution_count=70}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>manufacturer</th>\n      <th>type</th>\n      <th>calories</th>\n      <th>protein</th>\n      <th>fat</th>\n      <th>sodium</th>\n      <th>fiber</th>\n      <th>carbohydrates</th>\n      <th>sugar</th>\n      <th>potassium</th>\n      <th>vitamins</th>\n      <th>shelf</th>\n      <th>weight</th>\n      <th>cups</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100% Bran</td>\n      <td>N</td>\n      <td>C</td>\n      <td>70</td>\n      <td>4</td>\n      <td>1</td>\n      <td>130</td>\n      <td>10.0</td>\n      <td>5.0</td>\n      <td>6</td>\n      <td>280</td>\n      <td>25</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.33</td>\n      <td>68.402973</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100% Natural Bran</td>\n      <td>Q</td>\n      <td>C</td>\n      <td>120</td>\n      <td>3</td>\n      <td>5</td>\n      <td>15</td>\n      <td>2.0</td>\n      <td>8.0</td>\n      <td>8</td>\n      <td>135</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>1.00</td>\n      <td>33.983679</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>All-Bran</td>\n      <td>K</td>\n      <td>C</td>\n      <td>70</td>\n      <td>4</td>\n      <td>1</td>\n      <td>260</td>\n      <td>9.0</td>\n      <td>7.0</td>\n      <td>5</td>\n      <td>320</td>\n      <td>25</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.33</td>\n      <td>59.425505</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>All-Bran with Extra Fiber</td>\n      <td>K</td>\n      <td>C</td>\n      <td>50</td>\n      <td>4</td>\n      <td>0</td>\n      <td>140</td>\n      <td>14.0</td>\n      <td>8.0</td>\n      <td>0</td>\n      <td>330</td>\n      <td>25</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.50</td>\n      <td>93.704912</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Almond Delight</td>\n      <td>R</td>\n      <td>C</td>\n      <td>110</td>\n      <td>2</td>\n      <td>2</td>\n      <td>200</td>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>8</td>\n      <td>-1</td>\n      <td>25</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.75</td>\n      <td>34.384843</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#cell-data_processing2 .cell execution_count=3}\n``` {.python .cell-code}\n# select data for the histogram\ndf = df0[[\"calories\", \"protein\", \"fat\", \"sodium\", \"fiber\", \"carbohydrates\", \"sugar\",\"potassium\",\"rating\",\"name\"]]\ndf.head()\n```\n\n::: {#data_processing2 .cell-output .cell-output-display execution_count=71}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>calories</th>\n      <th>protein</th>\n      <th>fat</th>\n      <th>sodium</th>\n      <th>fiber</th>\n      <th>carbohydrates</th>\n      <th>sugar</th>\n      <th>potassium</th>\n      <th>rating</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>70</td>\n      <td>4</td>\n      <td>1</td>\n      <td>130</td>\n      <td>10.0</td>\n      <td>5.0</td>\n      <td>6</td>\n      <td>280</td>\n      <td>68.402973</td>\n      <td>100% Bran</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>120</td>\n      <td>3</td>\n      <td>5</td>\n      <td>15</td>\n      <td>2.0</td>\n      <td>8.0</td>\n      <td>8</td>\n      <td>135</td>\n      <td>33.983679</td>\n      <td>100% Natural Bran</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70</td>\n      <td>4</td>\n      <td>1</td>\n      <td>260</td>\n      <td>9.0</td>\n      <td>7.0</td>\n      <td>5</td>\n      <td>320</td>\n      <td>59.425505</td>\n      <td>All-Bran</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>50</td>\n      <td>4</td>\n      <td>0</td>\n      <td>140</td>\n      <td>14.0</td>\n      <td>8.0</td>\n      <td>0</td>\n      <td>330</td>\n      <td>93.704912</td>\n      <td>All-Bran with Extra Fiber</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>110</td>\n      <td>2</td>\n      <td>2</td>\n      <td>200</td>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>8</td>\n      <td>-1</td>\n      <td>34.384843</td>\n      <td>Almond Delight</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nNow that we have the data ready, let us look at the histogram of each variables namely nutritional contents, specifically calories, protein, fat, sodium, fiber, carbo, sugars and potassium\n:::\n\n::: {#cell-EDA .cell execution_count=4}\n``` {.python .cell-code}\n# Use melt function for the histograms of variables \ndf2 = pd.melt(df, id_vars=['name'])\n# df2.head()\n\np = (\n    ggplot(df2, aes(\"value\"))\n    + geom_histogram(bins=10)\n    + facet_grid(\". ~ variable\", scales='free_x')\n    + theme(figure_size=(12, 3))\n    )\n\n# If we want the density on y axis\n# p = (\n#     ggplot(df2, aes(\"value\", after_stat(\"density\")))\n#     + geom_histogram(bins=10)\n#     + facet_grid(\". ~ variable\", scales='free_x')\n#     + theme(figure_size=(12, 3))\n#     )\n\np.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/eda-output-1.png){#eda width=1152 height=288}\n:::\n:::\n\n\nThe histogram of each of the variables do not show any problems as all the plots look decent. We will look at the correlation plot, which shows the correlation between each pair of variables in a visual form.\n\n::: {#cell-EDA2 .cell execution_count=5}\n``` {.python .cell-code}\n# Check the correlation between the variables\nplt.figure(figsize=(20,10))\nsns.heatmap(df.iloc[:,:-1].corr(),annot=True,cmap=\"viridis\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/eda2-output-1.png){#eda2}\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nRating variable has positive correlation with all the variables except sugar, sodium, fat and calories . This seems logical and will be useful when we build a regression model for the same. Next we take a look at the pairs plot which will give us idea about relationship between each pair of variables. Most important from the point of prediction is the last row where rating is the y axis and each of the variable is x axis.\n:::\n\n\n::: {#cell-EDA3 .cell execution_count=7}\n``` {.python .cell-code}\n# Pairs plot\ng = sns.PairGrid(df.iloc[:,:-1])\ng.map_diag(sns.histplot)\ng.map_lower(sns.scatterplot)\ng.map_upper(sns.kdeplot)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/eda3-output-1.png){#eda3}\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nThe scatterplots of each variable with calories which can be seen in the upper triangular plots in the very first row. It seems there is a linear association between calories and fat, carbs and protein. However, it does not seem to have a linear association with fiber.\n:::\n\n::: {#cell-mlr_train .cell execution_count=8}\n``` {.python .cell-code}\n# Split data into train and test set\nindices = range(len(df)) # Create a list of indices\n\n# Get 75% random indices\nrandom.seed(55) # for reproducible example\nrandom_indices = random.sample(indices, round(0.75*len(df)))\n\n# Training dataset\ndata_train = df.iloc[random_indices,:-1]\n\n# Testing dataset\ndata_test = df.iloc[df.index.difference(random_indices),:-1]\n\n# Build a multiple linear regression model to predict calories using other variables using training data\nresult = smf.ols(\"rating ~ calories + protein + fat + sodium + fiber + carbohydrates + sugar + potassium\", data = data_train).fit()\n# check the summary\nresult.summary()\n```\n\n::: {#mlr_train .cell-output .cell-output-display execution_count=76}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>         <td>rating</td>      <th>  R-squared:         </th> <td>   0.997</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.997</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2429.</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Fri, 16 May 2025</td> <th>  Prob (F-statistic):</th> <td>6.35e-61</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>19:02:32</td>     <th>  Log-Likelihood:    </th> <td> -65.696</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    58</td>      <th>  AIC:               </th> <td>   149.4</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    49</td>      <th>  BIC:               </th> <td>   167.9</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>     <td>   54.4433</td> <td>    0.868</td> <td>   62.753</td> <td> 0.000</td> <td>   52.700</td> <td>   56.187</td>\n</tr>\n<tr>\n  <th>calories</th>      <td>   -0.2160</td> <td>    0.014</td> <td>  -15.385</td> <td> 0.000</td> <td>   -0.244</td> <td>   -0.188</td>\n</tr>\n<tr>\n  <th>protein</th>       <td>    3.3032</td> <td>    0.152</td> <td>   21.680</td> <td> 0.000</td> <td>    2.997</td> <td>    3.609</td>\n</tr>\n<tr>\n  <th>fat</th>           <td>   -1.8546</td> <td>    0.196</td> <td>   -9.451</td> <td> 0.000</td> <td>   -2.249</td> <td>   -1.460</td>\n</tr>\n<tr>\n  <th>sodium</th>        <td>   -0.0573</td> <td>    0.001</td> <td>  -38.480</td> <td> 0.000</td> <td>   -0.060</td> <td>   -0.054</td>\n</tr>\n<tr>\n  <th>fiber</th>         <td>    3.3924</td> <td>    0.140</td> <td>   24.228</td> <td> 0.000</td> <td>    3.111</td> <td>    3.674</td>\n</tr>\n<tr>\n  <th>carbohydrates</th> <td>    1.0424</td> <td>    0.048</td> <td>   21.634</td> <td> 0.000</td> <td>    0.946</td> <td>    1.139</td>\n</tr>\n<tr>\n  <th>sugar</th>         <td>   -0.7696</td> <td>    0.054</td> <td>  -14.272</td> <td> 0.000</td> <td>   -0.878</td> <td>   -0.661</td>\n</tr>\n<tr>\n  <th>potassium</th>     <td>   -0.0324</td> <td>    0.005</td> <td>   -6.648</td> <td> 0.000</td> <td>   -0.042</td> <td>   -0.023</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>78.045</td> <th>  Durbin-Watson:     </th> <td>   2.069</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 801.691</td> \n</tr>\n<tr>\n  <th>Skew:</th>          <td>-3.864</td> <th>  Prob(JB):          </th> <td>8.22e-175</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td>19.492</td> <th>  Cond. No.          </th> <td>1.82e+03</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.82e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\np-value for all the variables is low so all the variables are significantly affecting the response variable, rating. However the model output indicates there might be multicollinearity issue. Multicollinearity means the predictor variables are have high correlation among themselves. If we look at the correlation plot, fiber and potassium has 0.9 correlation which is high. One way to tackle multicollinearity is to consider principal component analysis (PCA). We will look at it in a while but let us first try to make predictions and look at the evaluation metrics.\n:::\n\n::: {style=\"text-align: justify\"}\nNow let us make prediction on the testing data and plot the observed vs. predicted plot\n:::\n\n::: {#cell-prediction_test .cell execution_count=9}\n``` {.python .cell-code}\n# Make predictions using testing data\npredictions = result.predict(data_test)\n\n# Observed vs. Predicted plot\nplt.figure(figsize=(20,7))\nplt.scatter(predictions, data_test[\"rating\"])\nplt.ylabel(\"Observed rating\")\nplt.xlabel(\"Predicted rating\")\n# Create the abline\nx_line = np.linspace(min(data_test[\"rating\"])-2, max(data_test[\"rating\"])+2, 100)\ny_line = 1 * x_line + 1\nplt.plot(x_line, y_line, color='red')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/prediction_test-output-1.png){#prediction_test}\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nThe observed vs. predicted looks good. However there is low number of data points and hence we should take this with a grain of salt. Let us check some evaluation metrics like the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n:::\n\n::: {#evaluation_metrics .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nprint(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"rating\"],predictions),2))\nprint(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"rating\"],predictions))** 0.5,2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Absolute Error: 1.05\nRoot Mean Squared Error: 1.72\n```\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nRoot Mean Squared Error (RMSE) of 1.05 and Mean Absolute Error (MAE) of 1.72 is decent and indicates model is performing fairly well.\n:::\n\n::: {style=\"text-align: justify\"}\nNow, we will run regression model based on principal component analysis since it helps with multicollinearity.\n:::\n\n::: {#cell-PCA_processing .cell execution_count=11}\n``` {.python .cell-code}\n# Principal component analysis\nfrom sklearn.decomposition import PCA\n\n# separate the x and y variable for the training data first\ny_train = data_train.iloc[:,-1:]\nX0_train = data_train.iloc[:,:-1]\n\n# Standardize the predictor data first\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n# training data\nX_train_scaled = sc.fit_transform(X0_train)\n\n# Now calculate the principal components\nfrom sklearn.decomposition import PCA\npca = PCA()\nprincipalComponents = pca.fit_transform(X_train_scaled)\n# Training data\nX_train_pca = pd.DataFrame(data = principalComponents,\n             columns=['PC{}'.format(i+1)\n                      for i in range(principalComponents.shape[1])])\n\n\n\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance\n```\n\n::: {#pca_processing .cell-output .cell-output-display execution_count=79}\n```\narray([0.36079172, 0.24639   , 0.16186567, 0.1045944 , 0.06695072,\n       0.04472673, 0.00874256, 0.0059382 ])\n```\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nThe first six principal components explain around 98% of the data, so we will use the first six principal components to build a regression model.\n:::\n\n::: {#cell-PCA_processing2 .cell execution_count=12}\n``` {.python .cell-code}\nX_train_pca = pd.DataFrame(data = principalComponents,\n             columns=['PC{}'.format(i+1)\n                      for i in range(principalComponents.shape[1])])\n\n# combine the X and Y for the training data\ndata_train_pca = X_train_pca\ndata_train_pca.set_index(X0_train.index,inplace = True)\ndata_train_pca['rating'] = y_train\ndata_train_pca.head()\n```\n\n::: {#pca_processing2 .cell-output .cell-output-display execution_count=80}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PC1</th>\n      <th>PC2</th>\n      <th>PC3</th>\n      <th>PC4</th>\n      <th>PC5</th>\n      <th>PC6</th>\n      <th>PC7</th>\n      <th>PC8</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11</th>\n      <td>0.865400</td>\n      <td>0.737424</td>\n      <td>3.082256</td>\n      <td>-0.797678</td>\n      <td>-1.695220</td>\n      <td>-0.565655</td>\n      <td>-0.436512</td>\n      <td>-0.175165</td>\n      <td>50.764999</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>-1.656332</td>\n      <td>-0.688910</td>\n      <td>-0.969216</td>\n      <td>1.205505</td>\n      <td>0.111314</td>\n      <td>-0.454199</td>\n      <td>0.373535</td>\n      <td>-0.159738</td>\n      <td>31.435973</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.979620</td>\n      <td>2.049689</td>\n      <td>-0.266540</td>\n      <td>-0.475020</td>\n      <td>-0.352195</td>\n      <td>1.032443</td>\n      <td>-0.089196</td>\n      <td>-0.058422</td>\n      <td>40.448772</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>-0.939218</td>\n      <td>-0.515171</td>\n      <td>0.188656</td>\n      <td>-0.014788</td>\n      <td>0.070899</td>\n      <td>0.306183</td>\n      <td>0.027561</td>\n      <td>0.007296</td>\n      <td>36.523683</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-2.157184</td>\n      <td>1.069870</td>\n      <td>-0.921099</td>\n      <td>0.649449</td>\n      <td>-0.525503</td>\n      <td>0.549343</td>\n      <td>0.091509</td>\n      <td>-0.057116</td>\n      <td>18.042851</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#cell-correlation_plot .cell execution_count=13}\n``` {.python .cell-code}\n# Correlation plot for principal components\nplt.figure(figsize=(20,10))\nsns.heatmap(data_train_pca.corr().round(4),annot=True, cmap=\"viridis\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/correlation_plot-output-1.png){#correlation_plot}\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nWe can observe that only rating variable has correlation with the principal components and the correlation between the principal components is 0. The correlation of rating with principal component 6 and 8 is considerably low and hence we will not use them in the model. So we will use the principal components 1,2,3,4,5 and 8 to build a regression model.\n:::\n\n::: {#cell-PCA_model .cell execution_count=14}\n``` {.python .cell-code}\n# Now run the OLS regression model on the first five principal components\n# Fit the OLS regression\nresult_pca = smf.ols(\"rating ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC8\", data = data_train_pca).fit()\n# check the summary\nresult_pca.summary()\n```\n\n::: {#pca_model .cell-output .cell-output-display execution_count=82}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>         <td>rating</td>      <th>  R-squared:         </th> <td>   0.997</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.997</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3009.</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Fri, 16 May 2025</td> <th>  Prob (F-statistic):</th> <td>3.38e-63</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>19:02:32</td>     <th>  Log-Likelihood:    </th> <td> -68.981</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    58</td>      <th>  AIC:               </th> <td>   152.0</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    51</td>      <th>  BIC:               </th> <td>   166.4</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   43.6488</td> <td>    0.111</td> <td>  392.171</td> <td> 0.000</td> <td>   43.425</td> <td>   43.872</td>\n</tr>\n<tr>\n  <th>PC1</th>       <td>    7.2019</td> <td>    0.066</td> <td>  109.932</td> <td> 0.000</td> <td>    7.070</td> <td>    7.333</td>\n</tr>\n<tr>\n  <th>PC2</th>       <td>   -5.1916</td> <td>    0.079</td> <td>  -65.488</td> <td> 0.000</td> <td>   -5.351</td> <td>   -5.032</td>\n</tr>\n<tr>\n  <th>PC3</th>       <td>    2.1011</td> <td>    0.098</td> <td>   21.481</td> <td> 0.000</td> <td>    1.905</td> <td>    2.297</td>\n</tr>\n<tr>\n  <th>PC4</th>       <td>   -2.6963</td> <td>    0.122</td> <td>  -22.160</td> <td> 0.000</td> <td>   -2.941</td> <td>   -2.452</td>\n</tr>\n<tr>\n  <th>PC5</th>       <td>    3.3596</td> <td>    0.152</td> <td>   22.091</td> <td> 0.000</td> <td>    3.054</td> <td>    3.665</td>\n</tr>\n<tr>\n  <th>PC8</th>       <td>   -7.8789</td> <td>    0.511</td> <td>  -15.429</td> <td> 0.000</td> <td>   -8.904</td> <td>   -6.854</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>59.001</td> <th>  Durbin-Watson:     </th> <td>   2.019</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 340.488</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-2.843</td> <th>  Prob(JB):          </th> <td>1.16e-74</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td>13.419</td> <th>  Cond. No.          </th> <td>    7.79</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\n$R^{2}$ is 99.7% which is decent and all the predictor variables have a low p-value value. We make predictions using the test data and then plot the out of sample observed vs. predicted. First we calculate the principal components of the testing data and then make the predictions.\n:::\n\n::: {#pca_test_data .cell execution_count=15}\n``` {.python .cell-code}\n# X for testing data\nX0_test = data_test.iloc[:,:-1]\n\n# scaled test data\nX_test_scaled = sc.transform(X0_test)\n\n# calculate the principal components for the testing data\nX_test = pca.transform(X_test_scaled)\nX_test_pca = pd.DataFrame(data = X_test,\n             columns=['PC{}'.format(i+1)\n                      for i in range(X_test.shape[1])])\n# calculate the predictions\npredictions_pca = result_pca.predict(X_test_pca)\n```\n:::\n\n\n::: {style=\"text-align: justify\"}\nNow we plot the out of sample predictions obtained from regression model using raw data as well as the predictions obtained from model using the six principal components on the same plot with different colors.\n:::\n\n::: {#cell-PCA_predicted .cell execution_count=16}\n``` {.python .cell-code}\n# Observed vs. Predicted plot\nplt.figure(figsize=(20,7))\n\nplt.scatter(predictions, data_test[\"rating\"], label='raw model', color='black', marker='o')\nplt.scatter(predictions_pca, data_test[\"rating\"],  label='PCA model', color='blue', marker='o')\n# sns.regplot(y = data_test[\"calories\"],x = predictions,ci=None,line_kws={\"color\":\"red\"})\nplt.ylabel(\"Observed rating\")\nplt.xlabel(\"Predicted rating\")\nplt.legend()\n\n# Create the abline\nx_line = np.linspace(min(data_test[\"rating\"]), max(data_test[\"rating\"]), 100)\ny_line = 1 * x_line + 1\nplt.plot(x_line, y_line, color='red')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/pca_predicted-output-1.png){#pca_predicted}\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nThe out of sample observed vs. predicted plot looks decent with all the points just around the red line. WE look at the evaluation metrics for the model built using the principal components.\n:::\n\n::: {#pca_metrics .cell execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nprint(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"rating\"],predictions_pca),2))\nprint(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"rating\"],predictions_pca))** 0.5,2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Absolute Error: 1.23\nRoot Mean Squared Error: 1.89\n```\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nFor the regression model using first six principal components, Root Mean Squared Error (RMSE) is 1.23 and Mean Absolute Error (MAE) is 1.89 which is slightly higher than the regression model using the raw data.\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}