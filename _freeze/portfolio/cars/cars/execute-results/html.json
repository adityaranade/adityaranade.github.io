{
  "hash": "34740ff4be0cf90ed526a09a3b23a229",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting fuel consumption (MPG) of cars\"\nsubtitle: \"ML models to predict fuel conumption of cars (MPG) using car information\"\nauthor: \"Aditya Ranade\"\nhighlight-style:\n  light: github\ndate: \"2025-05-15\"\ncategories: [analysis, R]\nimage: \"./car_fuel.jpg\"\n---\n\n\n\n::: {style=\"text-align: justify\"}\nI found this [dataset](https://archive.ics.uci.edu/dataset/9/auto+mpg) on UCI machine learning repository which gives the dataset regarding the car features along with fuel consumption. The goal is to predict the fuel consumption indicated by the variable mpg based on other features of the car like horespower, displacement, weight, etc. of car. We will compare multiple Machine Learning models for the same.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(GGally) # for pairs plot using ggplot framework\nlibrary(dplyr)\nlibrary(glmnet)\nlibrary(knitr)\n\n# Get cars data from github repo\npath <- \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/cars/autompg.data\"\ndata0 <- read.table(path, fill = TRUE, header = FALSE)\n\ncolnames(data0) <- c(\"mpg\",\"cylinders\",\"displacement\",\n                     \"horsepower\",\"weight\",\"acceleration\",\n                     \"model_year\",\"origin\",\"car_name\")\n\n# Check the type of data\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t398 obs. of  9 variables:\n $ mpg         : num  18 15 18 16 17 15 14 14 14 15 ...\n $ cylinders   : int  8 8 8 8 8 8 8 8 8 8 ...\n $ displacement: num  307 350 318 304 302 429 454 440 455 390 ...\n $ horsepower  : chr  \"130.0\" \"165.0\" \"150.0\" \"150.0\" ...\n $ weight      : num  3504 3693 3436 3433 3449 ...\n $ acceleration: num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n $ model_year  : int  70 70 70 70 70 70 70 70 70 70 ...\n $ origin      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ car_name    : chr  \"chevrolet chevelle malibu\" \"buick skylark 320\" \"plymouth satellite\" \"amc rebel sst\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Convert horsepower to numeric\ndata0$horsepower <- as.numeric(data0$horsepower)\n\n# Check the type of data again\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t398 obs. of  9 variables:\n $ mpg         : num  18 15 18 16 17 15 14 14 14 15 ...\n $ cylinders   : int  8 8 8 8 8 8 8 8 8 8 ...\n $ displacement: num  307 350 318 304 302 429 454 440 455 390 ...\n $ horsepower  : num  130 165 150 150 140 198 220 215 225 190 ...\n $ weight      : num  3504 3693 3436 3433 3449 ...\n $ acceleration: num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n $ model_year  : int  70 70 70 70 70 70 70 70 70 70 ...\n $ origin      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ car_name    : chr  \"chevrolet chevelle malibu\" \"buick skylark 320\" \"plymouth satellite\" \"amc rebel sst\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the rows which do not have any entries\nsum(is.na(data0)) # 6 NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Exclude the rows with missing information\ndata1 <- na.omit(data0)\nsum(is.na(data1)) # no NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the first 6 rows of the dataset\ndata1 |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mpg cylinders displacement horsepower weight acceleration model_year origin\n1  18         8          307        130   3504         12.0         70      1\n2  15         8          350        165   3693         11.5         70      1\n3  18         8          318        150   3436         11.0         70      1\n4  16         8          304        150   3433         12.0         70      1\n5  17         8          302        140   3449         10.5         70      1\n6  15         8          429        198   4341         10.0         70      1\n                   car_name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n5               ford torino\n6          ford galaxie 500\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe distributions of the continuous variables on the original scale indicates some non linear relationships between the response variable mpg and the other variables. So we convert the data to log scale and the relationships become close to linear. Hence we will use the data on log scale for predictions. The distribution of the data on the log scale is as follows\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Transform the data to log scale\n# exclude the last column which is car name\ndata <- data1[,-ncol(data1)]\n\n# Pairs plot between the explanatory variables to \n# check correlation between each pair of the variables\nggpairs(data) \n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/EDA01-1.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe response variable, mpg is correlated with all the variables which is good. However, the explanatory variables are correlated within themselves which is not a good indication. This indicates there is some multicollinearity. This means two variables give similar information about the response variable. One way to mitigate the effect is to consider the principal components and then use the principal components for the models. Another way is to use some regularization to mitigate the effect of multicollinearity.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Transform the data to log scale\ndata <- data1[,-ncol(data1)] |> log()\n\n# Pairs plot between the explanatory variables to \n# check correlation between each pair of the variables\nggpairs(data) \n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/EDA00-1.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe response variable, mpg is correlated with all the variables which is good. However, the explanatory variables are correlated within themselves which is not a good indication. This indicates there is some multicollinearity. This means two variables give similar information about the response variable. One way to mitigate the effect is to consider the principal components and then use the principal components for the models. Another way is to use some regularization to mitigate the effect of multicollinearity.\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split the data into training and testing data\nseed <- 23\nset.seed(seed)\n\nind <- sample(floor(0.8*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train <- data[ind,-ncol(data)]\n# Testing dataset\ndata_test <- data[-ind,-ncol(data)]\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nFirst, we will look at a multiple linear regression model\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a multiple linear regression model\nmodel_lm <- glm(mpg ~ ., data = data_train)\n\n# Check the summary of the model\nmodel_lm |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = mpg ~ ., data = data_train)\n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.49694    0.74566   2.008 0.045573 *  \ncylinders    -0.06673    0.06434  -1.037 0.300499    \ndisplacement -0.04162    0.05169  -0.805 0.421340    \nhorsepower   -0.25381    0.05798  -4.378 1.65e-05 ***\nweight       -0.59676    0.08376  -7.125 7.51e-12 ***\nacceleration -0.22290    0.06073  -3.670 0.000286 ***\nmodel_year    1.94813    0.16281  11.966  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.01097963)\n\n    Null deviance: 30.4993  on 312  degrees of freedom\nResidual deviance:  3.3598  on 306  degrees of freedom\nAIC: -514.99\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_lm <- predict(model_lm, data_test)\n\n# Data frame for observed vs predicted\ndf_pred_mlr <- data.frame(predicted = y_pred_lm, \n                    observed = data_test$mpg)\ndf_pred_mlr$model <- \"mlr\"\n\n# Evaluation metrics\nrmse_lm <- sqrt(sum(data_test$mpg-y_pred_lm)^2)\nmae_lm <- mean(abs(data_test$mpg-y_pred_lm))\nr2_lm   <- 1 - sum((data_test$mpg - y_pred_lm)^2) / sum((data_test$mpg - mean(data_test$mpg))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the lasso regression which uses the $L^1$ penalty.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Lasso regression (L1 penalty)\nmodel_l1_cv <- cv.glmnet(as.matrix(data_train[,-1]),\n                         as.matrix(data_train[,1]),\n                         alpha = 0)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda_l1 <- model_l1_cv$lambda.min\nbest_lambda_l1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02813141\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_l1 <- glmnet(as.matrix(data_train[,-1]),\n                   as.matrix(data_train[,1]),\n                   alpha = 0, \n                   lambda = best_lambda_l1)\n\n# Coefficients of the lasso regression model \ncoef(model_l1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept)   1.1562832\ncylinders    -0.1119517\ndisplacement -0.1014371\nhorsepower   -0.2373315\nweight       -0.4181040\nacceleration -0.1806215\nmodel_year    1.7415423\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_l1 <- predict(model_l1,  s = best_lambda_l1,\n                     newx= as.matrix(data_test[,-1]))\n\n# Data frame for observed vs predicted\ndf_pred_l1 <- data.frame(predicted = as.vector(y_pred_l1), \n                    observed = data_test$mpg)\ndf_pred_l1$model <- \"lasso\"\n\n# Evaluation metrics\nrmse_l1 <- sqrt(sum(data_test$mpg-y_pred_l1)^2)\nmae_l1 <- mean(abs(data_test$mpg-y_pred_l1))\nr2_l1   <- 1 - sum((data_test$mpg - y_pred_l1)^2) / sum((data_test$mpg - mean(data_test$mpg))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the ridge regression which uses the $L^2$ penalty.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ridge regression (L2 penalty)\nmodel_l2_cv <- cv.glmnet(as.matrix(data_train[,-1]),\n                         as.matrix(data_train[,1]),\n                         alpha = 1)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda <- model_l2_cv$lambda.min\nbest_lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0006060729\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_l2 <- glmnet(as.matrix(data_train[,-1]),\n                   as.matrix(data_train[,1]),\n                   alpha = 1, \n                   lambda = best_lambda)\n\n# Coefficients of the ridge regression model \ncoef(model_l2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept)   1.53082771\ncylinders    -0.06263596\ndisplacement -0.04075770\nhorsepower   -0.24240969\nweight       -0.60741542\nacceleration -0.20551149\nmodel_year    1.93416769\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_l2 <- predict(model_l2,  s = best_lambda,\n                     newx= as.matrix(data_test[,-1]))\n\n# Data frame for observed vs predicted\ndf_pred_l2 <- data.frame(predicted = as.vector(y_pred_l2), \n                    observed = data_test$mpg)\ndf_pred_l2$model <- \"ridge\"\n\n# Evaluation metrics\nrmse_l2 <- sqrt(sum(data_test$mpg-y_pred_l2)^2)\nmae_l2 <- mean(abs(data_test$mpg-y_pred_l2))\nr2_l2   <- 1 - sum((data_test$mpg - y_pred_l2)^2) / sum((data_test$mpg - mean(data_test$mpg))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the elastic net regression which is a combination of lasso ($L^1$ penalty) and ridge ($L^2$ penalty) regression.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Elastic net\nmodel_en_cv <- cv.glmnet(as.matrix(data_train[,-1]),\n                         as.matrix(data_train[,1]),\n                         alpha = 0.5)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda_en <- model_en_cv$lambda.min\nbest_lambda_en\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001212146\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_en <- glmnet(as.matrix(data_train[,-1]),\n                   as.matrix(data_train[,1]),\n                   alpha = 0.5, \n                   lambda = best_lambda_en)\ncoef(model_en)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept)   1.51253268\ncylinders    -0.06214793\ndisplacement -0.04589088\nhorsepower   -0.24457980\nweight       -0.59592496\nacceleration -0.20734991\nmodel_year    1.92659723\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_en |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Length Class     Mode   \na0        1      -none-    numeric\nbeta      6      dgCMatrix S4     \ndf        1      -none-    numeric\ndim       2      -none-    numeric\nlambda    1      -none-    numeric\ndev.ratio 1      -none-    numeric\nnulldev   1      -none-    numeric\nnpasses   1      -none-    numeric\njerr      1      -none-    numeric\noffset    1      -none-    logical\ncall      5      -none-    call   \nnobs      1      -none-    numeric\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_en <- predict(model_en,  s = best_lambda_en,\n                     newx= as.matrix(data_test[,-1]))\n\n# Data frame for observed vs predicted\ndf_pred_en <- data.frame(predicted = as.vector(y_pred_en), \n                    observed = data_test$mpg)\ndf_pred_en$model <- \"elastic_net\"\n\n# Evaluation metrics\nrmse_en <- sqrt(sum(data_test$mpg-y_pred_en)^2)\nmae_en <- mean(abs(data_test$mpg-y_pred_en))\nr2_en   <- 1 - sum((data_test$mpg - y_pred_en)^2) / sum((data_test$mpg - mean(data_test$mpg))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the tree based approach.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tree approach\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Fit regression tree\nmodel_tree <- rpart(mpg ~ ., data = data_train, method = \"anova\")\n\n# summary(model_tree)\n\n# Plot\nrpart.plot(model_tree, type = 3, extra = 101, fallen.leaves = TRUE)\n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/tree-1.png){width=672}\n:::\n\n```{.r .cell-code}\ny_pred_tree <- predict(model_tree, data_test)\n\n# Data frame for observed vs predicted\ndf_pred_tree <- data.frame(predicted = y_pred_tree, \n                    observed = data_test$mpg)\ndf_pred_tree$model <- \"tree\"\n\n# Evaluation metrics\nrmse_tree <- sqrt(sum(data_test$mpg-y_pred_tree)^2)\nmae_tree <- mean(abs(data_test$mpg-y_pred_tree))\nr2_tree   <- 1 - sum((data_test$mpg - y_pred_tree)^2) / sum((data_test$mpg - mean(data_test$mpg))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the random forest approach. In random forest approach, we build multiple trees and then average the predictions of all the trees.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random forest\nlibrary(randomForest)\nmodel_rf <- randomForest(mpg ~ ., data = data_train)\n\ny_pred_rf <- predict(model_rf, data_test)\n\n# Data frame for observed vs predicted\ndf_pred_rf <- data.frame(predicted = y_pred_rf, \n                    observed = data_test$mpg)\ndf_pred_rf$model <- \"random_forest\"\n\n# Evaluation metrics\nrmse_rf <- sqrt(sum(data_test$mpg-y_pred_rf)^2)\nmae_rf <- mean(abs(data_test$mpg-y_pred_rf))\nr2_rf   <- 1 - sum((data_test$mpg - y_pred_rf)^2) / sum((data_test$mpg - mean(data_test$mpg))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the support vector machine (SVM) approach.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(e1071)\nmodel_svm <- svm(mpg ~ ., data = data_train, \n                 kernel = \"radial\", cost = 10, \n                 gamma = 0.1)\n\n# Predict on test data\ny_pred_svm <- predict(model_svm, data_test)\n\n# Data frame for observed vs predicted\ndf_pred_svm <- data.frame(predicted = y_pred_svm, \n                    observed = data_test$mpg)\ndf_pred_svm$model <- \"svm\"\n\n# Evaluation metrics\nrmse_svm <- sqrt(sum(data_test$mpg-y_pred_svm)^2)\nmae_svm <- mean(abs(data_test$mpg-y_pred_svm))\nr2_svm   <- 1 - sum((data_test$mpg - y_pred_svm)^2) / sum((data_test$mpg - mean(data_test$mpg))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe observed vs. predicted for all the models side by side can be seen in the plot below\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot observed vs. predicted for all the models\ndf_pred <- rbind(df_pred_mlr,df_pred_l1,df_pred_l2,\n                 df_pred_en, df_pred_tree, df_pred_rf,\n                 df_pred_svm)\n\n# Create a observed vs. predicted plot combined for all the models\nggplot(df_pred,aes(predicted,observed))+geom_point()+\n  lims(x = c(2.5,4) , y = c(2.5,4))+\n  labs(y = \"Observed\", x=\"Predicted\")+\n  facet_grid(~model, scales=\"free\")+\n  geom_abline()+\n  theme_bw(base_size = 15)\n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/ovp_plot-1.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nCombined evaluation metrics to compare all the models can be seen in the table below.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Evaluation metrics for all the models\nmetrics_all <- data.frame(Model = c(\"linear_model\", \"lasso\", \"ridge\",\n                                 \"elastic_net\", \"tree\", \"random_forest\",\n                                 \"svm\"),\n                       RMSE = c(rmse_lm,rmse_l1, rmse_l2, rmse_en, \n                                rmse_tree, rmse_rf, rmse_svm),\n                       MAE = c(mae_lm,mae_l1, mae_l2, mae_en, \n                                mae_tree, mae_rf, mae_svm),\n                       R_squared = c(r2_lm, r2_l1, r2_l2, r2_en, \n                                r2_tree, r2_rf, r2_svm))\n\n# Print evaluation metrics\nkable(metrics_all, digits = 2, caption = \"Model Performance Metrics\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Model Performance Metrics\n\n|Model         |  RMSE|  MAE| R_squared|\n|:-------------|-----:|----:|---------:|\n|linear_model  |  5.20| 0.12|      0.38|\n|lasso         |  6.61| 0.12|      0.33|\n|ridge         |  5.36| 0.12|      0.38|\n|elastic_net   |  5.40| 0.12|      0.38|\n|tree          | 14.31| 0.19|     -0.52|\n|random_forest |  8.85| 0.14|      0.17|\n|svm           |  0.14| 0.11|      0.45|\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nVisualization of the combined evaluation metrics can be seen in the plot below.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# To plot RMSE and MAE on the same plot\nmetrics_long <- melt(metrics_all, id.vars = \"Model\", \n                     variable.name = \"Metric\", \n                     value.name = \"Value\")\n\nggplot(metrics_long, aes(x = Model, y = Value, fill = Model)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(aes(label = round(Value, 2)), \n            #position = position_dodge(width = 0.8), \n            size = 3) +\n  coord_flip(clip = \"off\") +  # horizontal bars, no clipping of text\n  facet_grid2(~Metric, scales=\"free\")+\n  labs(title = \"Model Performance Comparison\", \n       y = \"Error Value\", x = \"Model\") +\n  theme_bw(base_size = 10) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/combine3-1.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nBased on the evaluation metrics and observed vs. predicted plot, support vector machine model seems to be the best model.\n:::\n\n\n\n\n",
    "supporting": [
      "cars_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}