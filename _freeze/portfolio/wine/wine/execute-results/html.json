{
  "hash": "47623f7166572f225b7b9082e18501ba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting Wine Quality\"\nsubtitle: \"Predicting wine quality using multiple ML models\"\nauthor: \"Aditya Ranade\"\nhighlight-style:\n  light: github\ndate: \"2025-05-23\"\ncategories: [analysis, R]\nimage: \"./wine.jpg\"\n---\n\n\n\n::: {style=\"text-align: justify\"}\nI found this [dataset](https://archive.ics.uci.edu/dataset/186/wine+quality) on UCI machine learning repository which gives the dataset related to red and white variants of the Portuguese \"Vinho Verde\" wine which has physicochemical variables (input) and sensory variables (output) available. Due to privacy, the wine brand, type of grape and selling price is not provided. In this post, we will try to predict the quality based on the physicochemical variables.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(GGally) # for pairs plot using ggplot framework\n\n# Load the data\npath <- \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/wine/winequality-red.csv\"\ndata0 <- read.csv(path, header = TRUE)\n\n# Data processing\n# Check the type of data\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t1599 obs. of  12 variables:\n $ fixed.acidity       : num  7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...\n $ volatile.acidity    : num  0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...\n $ citric.acid         : num  0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...\n $ residual.sugar      : num  1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...\n $ chlorides           : num  0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...\n $ free.sulfur.dioxide : num  11 25 15 17 11 13 15 15 9 17 ...\n $ total.sulfur.dioxide: num  34 67 54 60 34 40 59 21 18 102 ...\n $ density             : num  0.998 0.997 0.997 0.998 0.998 ...\n $ pH                  : num  3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...\n $ sulphates           : num  0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...\n $ alcohol             : num  9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...\n $ quality             : int  5 5 5 6 5 5 5 7 7 5 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the rows which do not have any entries\nsum(is.na(data0)) # no NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# data (no need to exclude anything)\ndata <- data0\n\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.4             0.70        0.00            1.9     0.076\n2           7.8             0.88        0.00            2.6     0.098\n3           7.8             0.76        0.04            2.3     0.092\n4          11.2             0.28        0.56            1.9     0.075\n5           7.4             0.70        0.00            1.9     0.076\n6           7.4             0.66        0.00            1.8     0.075\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n1                  11                   34  0.9978 3.51      0.56     9.4\n2                  25                   67  0.9968 3.20      0.68     9.8\n3                  15                   54  0.9970 3.26      0.65     9.8\n4                  17                   60  0.9980 3.16      0.58     9.8\n5                  11                   34  0.9978 3.51      0.56     9.4\n6                  13                   40  0.9978 3.51      0.56     9.4\n  quality\n1       5\n2       5\n3       5\n4       6\n5       5\n6       5\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlation plot\nggcorrplot(round(cor(data), 2))\n```\n\n::: {.cell-output-display}\n![](wine_files/figure-html/EDA0-1.png){width=1344}\n:::\n\n```{.r .cell-code}\n# Pairs plot between the explanatory variables to \n# check correlation between each pair of the variables\nggpairs(data[,-ncol(data)])\n```\n\n::: {.cell-output-display}\n![](wine_files/figure-html/EDA0-2.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe correlation plot indicates moderate multicolinearity. The histogram of the response variable quality can be seen below. It can be seen most of the entries take the value 5, 6 or 7. Very few entries have value 3, 4 or 8.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check the histogram of the response variable\nggplot(data,aes(quality))+\n  geom_histogram()+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](wine_files/figure-html/EDA1-1.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThis can be treated as a count data and we can use the Poisson regression. However in Poisson regression, the mean and variance of the response variable quality should be same.\n:::\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check if the mean and variance of response variable is same.\n\n# Mean\ndata$quality %>% mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.636023\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance\ndata$quality %>% var\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6521684\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe mean is considerably greater than variance. Hence Poisson regression model will not work well. An alternative is to use a quasi Poisson model or Negative Binomial model. We will start with a basic multiple linear regression model. First we process the data and standardize all the variables.\n:::\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the data\n# Scale everything except the response variable (last column)\ndata2 <- data\ndata2[, -ncol(data)] <- scale(data[, -ncol(data)])\n\n# split the data into training and testing data\nseed <- 23\nset.seed(seed)\n\nind <- sample(1:nrow(data2),\n              floor(0.8*nrow(data2)),\n              replace = FALSE)\n\n# Training dataset\ndata_train <- data2[ind,] |> as.data.frame()\n# Testing dataset\ndata_test <- data2[-ind,] |> as.data.frame()\ndata_test |> dim()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 320  12\n```\n\n\n:::\n:::\n\n\n\n\n::: {style=\"text-align: justify\"}\nNow we run a linear regression model with quality being predicted as a response to volatile acidity, chlorides, total sulfur dioxide, pH, sulphates and alcohol. These variables have been selected after trying multiple combinations of predictor variables. Once we build the model and make predictions, we will round the response variable and make a Predicted vs. actual table. This will help us understand how many correct values were predicted.\n:::\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(quality ~ volatile.acidity + chlorides + total.sulfur.dioxide \n            + pH + sulphates + alcohol,\n            data = data_train)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = quality ~ volatile.acidity + chlorides + total.sulfur.dioxide + \n    pH + sulphates + alcohol, data = data_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.33867 -0.36947 -0.04921  0.47513  1.95783 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           5.63636    0.01818 310.079  < 2e-16 ***\nvolatile.acidity     -0.18389    0.02001  -9.191  < 2e-16 ***\nchlorides            -0.09564    0.02038  -4.692 2.99e-06 ***\ntotal.sulfur.dioxide -0.07486    0.01908  -3.923 9.22e-05 ***\npH                   -0.07111    0.01990  -3.573 0.000366 ***\nsulphates             0.15764    0.02120   7.436 1.90e-13 ***\nalcohol               0.30503    0.01989  15.333  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6499 on 1272 degrees of freedom\nMultiple R-squared:  0.3589,\tAdjusted R-squared:  0.3559 \nF-statistic: 118.7 on 6 and 1272 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred <- predict(model, data_test, type = \"response\")\n\n# # Create a observed vs. predicted plot\n# ggplot(NULL,aes(y_pred,data_test$quality))+geom_point()+\n#   labs(y = \"Observed\", x=\"Predicted\")+theme_minimal()+geom_abline()\n\ntable_lm <- table(Predicted = as.factor(round(y_pred)),\n                  Actual = as.factor(data_test$quality))\ntable_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Actual\nPredicted   3   4   5   6   7   8\n        5   0   9 102  29   3   0\n        6   1   4  37  91  31   2\n        7   0   0   1   4   5   1\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nSince there is some multicolinearity in the data, we can try to use L2  regularization which is also called as ridge regression. This helps shrink the coefficients of the model towards zero.\n:::\n  \n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'glmnet' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n```{.r .cell-code}\n#| label: mlr\n#| echo: true\n#| warning: false\n#| include: true\n\nmodel_l2_cv <- cv.glmnet(as.matrix(data_train[,-ncol(data_train)]),\n                         as.matrix(data_train[,ncol(data_train)]),\n                         alpha = 1)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda <- model_l2_cv$lambda.min\nbest_lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.004486534\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_l2 <- glmnet(as.matrix(data_train[,-ncol(data_train)]),\n                   as.matrix(data_train[,ncol(data_train)]),\n                   alpha = 1, \n                   lambda = best_lambda)\ncoef(model_l2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n12 x 1 sparse Matrix of class \"dgCMatrix\"\n                              s0\n(Intercept)           5.63622382\nfixed.acidity         0.01712355\nvolatile.acidity     -0.19348099\ncitric.acid          -0.03035695\nresidual.sugar        0.01923240\nchlorides            -0.08408944\nfree.sulfur.dioxide   0.02921787\ntotal.sulfur.dioxide -0.09162867\ndensity               .         \npH                   -0.06792325\nsulphates             0.15171519\nalcohol               0.30355610\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_l2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Length Class     Mode   \na0         1     -none-    numeric\nbeta      11     dgCMatrix S4     \ndf         1     -none-    numeric\ndim        2     -none-    numeric\nlambda     1     -none-    numeric\ndev.ratio  1     -none-    numeric\nnulldev    1     -none-    numeric\nnpasses    1     -none-    numeric\njerr       1     -none-    numeric\noffset     1     -none-    logical\ncall       5     -none-    call   \nnobs       1     -none-    numeric\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_l2 <- predict(model_l2,  s = best_lambda,\n                     newx= as.matrix(data_test[,-ncol(data_test)]))\n\ntable_l2 <- table(Predicted = as.factor(round(y_pred_l2)),\n                  Actual = as.factor(data_test$quality))\ntable_l2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Actual\nPredicted   3   4   5   6   7   8\n        5   0   9 101  29   2   0\n        6   1   4  38  91  32   3\n        7   0   0   1   4   5   0\n```\n\n\n:::\n:::\n\n\n  \n::: {style=\"text-align: justify\"}\nNow, we will try the support vector machines regression model.\n:::\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# SVM regression\nlibrary(e1071)\nmodel_svm <- svm(quality ~ volatile.acidity + chlorides + total.sulfur.dioxide \n                 + pH + sulphates + alcohol,\n                 data = data_train,\n                 type = \"eps-regression\", kernel = \"radial\")\n\ny_pred_svm <- predict(model_svm, data_test)\n\ntable(Predicted = round(y_pred_svm),\n      Actual = data_test$quality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Actual\nPredicted   3   4   5   6   7   8\n        5   0  10 117  36   3   0\n        6   1   3  23  85  28   2\n        7   0   0   0   3   8   1\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNow, we will try Bootstrap Aggregating (Bagging) model.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bagging\nlibrary(ipred)\nmodel_bagging <- bagging(quality ~ volatile.acidity + chlorides + total.sulfur.dioxide \n                         + pH + sulphates + alcohol, data = data_train, nbagg = 100)\n\n\ny_pred_bagging <- predict(model_bagging, data_test)\n\ntable(Predicted = round(y_pred_bagging),\n      Actual = data_test$quality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Actual\nPredicted   3   4   5   6   7   8\n        5   0  11 114  36   4   0\n        6   1   2  26  84  24   2\n        7   0   0   0   4  11   1\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNow, we will try the Boosting model.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boosting\nlibrary(gbm)\nmodel_boosting <- gbm(quality ~ volatile.acidity + chlorides + total.sulfur.dioxide \n                      + pH + sulphates + alcohol, \n                      data = data_train, \n                      distribution = \"gaussian\", \n                      n.trees = 100, interaction.depth = 3, \n                      shrinkage = 0.01, cv.folds = 5)\n\nbest_trees <- gbm.perf(model_boosting, method = \"cv\")\n```\n\n::: {.cell-output-display}\n![](wine_files/figure-html/boosting-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbest_trees\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 100\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predictions\ny_pred_boosting <- predict(model_boosting, data_test, n.trees = best_trees)\ntable(data_test$quality, round(y_pred_boosting))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   \n      5   6\n  3   0   1\n  4  10   3\n  5  95  45\n  6  23 101\n  7   3  36\n  8   0   3\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNow, we will try Xgboot (Xtreme Gradient Boosting) model.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# XGBOOST\nlibrary(xgboost)\n# Convert to matrix (xgboost requires the data as matrix or DMatrix format)\ndata_train_matrix <- as.matrix(data_train[,-ncol(data_train)])  \ntarget <- data_train[,ncol(data_train)]\n\n# Create a DMatrix object (this is how xgboost stores and handles data)\ndtrain <- xgb.DMatrix(data = data_train_matrix, label = target)\n\nparams <- list(\n  objective = \"reg:squarederror\",  # Objective function (regression)\n  eta = 0.1,                      # Learning rate\n  max_depth = 6,                   # Maximum depth of trees\n  colsample_bytree = 0.8,          # Subsample fraction of features\n  subsample = 0.8,                 # Subsample fraction of data\n  alpha = 0.1                      # L2 regularization\n)\n\nxgb_model <- xgb.train(\n  params = params, \n  data = dtrain, \n  nrounds = 100,         # Number of boosting rounds (iterations)\n  watchlist = list(train = dtrain),  # To monitor the training process\n  print_every_n = 10     # Print every 10 iterations\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]\ttrain-rmse:4.696259 \n[11]\ttrain-rmse:1.751696 \n[21]\ttrain-rmse:0.787100 \n[31]\ttrain-rmse:0.499741 \n[41]\ttrain-rmse:0.407594 \n[51]\ttrain-rmse:0.368031 \n[61]\ttrain-rmse:0.331699 \n[71]\ttrain-rmse:0.300139 \n[81]\ttrain-rmse:0.273557 \n[91]\ttrain-rmse:0.246384 \n[100]\ttrain-rmse:0.225383 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Make predictions on the testing data\ndata_test_matrix <- as.matrix(data_test[,-ncol(data_test)])  \npredictions <- predict(xgb_model, data_test_matrix)\n\ntable(round(predictions),data_test$quality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   \n      3   4   5   6   7   8\n  4   0   0   1   0   1   0\n  5   1  10 118  24   2   0\n  6   0   3  21  97  21   2\n  7   0   0   0   3  15   1\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nLastly, we will try k nearest neighbour (knn) model.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# KNN\nlibrary(caret)\n# Apply KNN with k = 5 (5 nearest neighbors)\ntrain_control <- trainControl(method = \"cv\", number = 5)  # 5-fold cross-validation\nmodel_knn <- train(quality ~ volatile.acidity + chlorides + total.sulfur.dioxide \n                   + pH + sulphates + alcohol, \n                   data = data_train, \n                   method = \"knn\", \n                   trControl = train_control, \n                   tuneLength = 10)\n\n# Check best K and accuracy\nprint(model_knn$bestTune)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   k\n5 13\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(model_knn$results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    k      RMSE  Rsquared       MAE     RMSESD RsquaredSD      MAESD\n1   5 0.6798989 0.3220740 0.5143110 0.02943495 0.04376877 0.02301630\n2   7 0.6731686 0.3253252 0.5172371 0.03212803 0.04431381 0.02571618\n3   9 0.6616948 0.3417998 0.5145655 0.03065211 0.04027093 0.02510235\n4  11 0.6552774 0.3519570 0.5150387 0.03275645 0.04130796 0.02957085\n5  13 0.6491360 0.3632288 0.5129367 0.03298546 0.03808877 0.02952243\n6  15 0.6497144 0.3608939 0.5157876 0.03146983 0.03170699 0.02620662\n7  17 0.6502542 0.3600491 0.5175585 0.02916799 0.02787033 0.02289953\n8  19 0.6508248 0.3600546 0.5184179 0.02817991 0.02841868 0.02239239\n9  21 0.6503630 0.3611082 0.5177740 0.02473004 0.02581504 0.02087044\n10 23 0.6504593 0.3611569 0.5187199 0.02345332 0.02202820 0.01893757\n```\n\n\n:::\n\n```{.r .cell-code}\ny_pred_knn <- predict(model_knn, data_test)\n\ntable(round(y_pred_knn),data_test$quality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   \n      3   4   5   6   7   8\n  5   0   9 103  30   6   0\n  6   1   4  37  85  24   2\n  7   0   0   0   9   9   1\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nBased on the multiple models, the Extreme Gradiant Boosting (xgboost) model is able to get the most accuracy in out case. It got the ratings correctly for 230 out of 320 cases in the testing data set which translates to an accuracy of around 71.88%.\n:::",
    "supporting": [
      "wine_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}