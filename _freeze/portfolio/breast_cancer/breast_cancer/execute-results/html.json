{
  "hash": "166f6de7723cb3c7a37625a07c0405b4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Breast Cancer Detection\"\nsubtitle: \"Breast cancer detection using Artificial intelligence\"\nauthor: \"Aditya Ranade\"\nhighlight-style:\n            light: github\ndate: \"2025-04-11\"\ncategories: [analysis, R]\nimage: \"./breast_cancer.jpg\"\n---\n\n\n\n::: {style=\"text-align: justify\"}\nI found this [dataset](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) on UCI machine learning repository which gives the dataset for breast cancer detection. It has 10 basic variables which indicates different aspects of measurements in a medical examination. For the 10 variables, the dataset provided mean, sd and the worst measurement. We will focus on the mean measurement.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggh4x)\nlibrary(GGally)\nlibrary(pROC)\nlibrary(glmnet)\n\n# Get data from github repo\npath <- \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/breast_cancer/wdbc.data\"\ndata00 <- read.table(path, sep = \",\", header = FALSE)\ndata0 <- data00[,2:12] # Use only first 11 columns\n\n# Data processing\nhead(data0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  V2    V3    V4     V5     V6      V7      V8     V9     V10    V11     V12\n1  M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871\n2  M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667\n3  M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999\n4  M 11.42 20.38  77.58  386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744\n5  M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883\n6  M 12.45 15.70  82.57  477.1 0.12780 0.17000 0.1578 0.08089 0.2087 0.07613\n```\n\n\n:::\n\n```{.r .cell-code}\n# change column names\ncolnames(data0) <- c(\"diagnosis\", \"radius\",\n                     \"texture\", \"perimeter\",\n                     \"area\",\"smoothness\",\n                     \"compactness\",\"concavity\",\n                     \"concave_points\",\"symmetry\",\n                     \"fractal_dimension\")\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nSince the measurements are taken on the same object, there is bound to be some correlation between the variables. For example, there will be strong correlation between radius, perimeter and area. So using all the three variables in a model will not be a good idea as it might not give a good reliable model.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data (select all the explanatory variables)\ndata <- data0 |> dplyr::select(radius, texture, perimeter,\n                               area, smoothness, compactness, \n                               concavity, concave_points,\n                               symmetry, fractal_dimension)\n\n# convert the response to factor and categorical variable\ndata$response <- ifelse(data0$diagnosis==\"B\",0,\n                        ifelse(data0$diagnosis==\"M\",1,99))\n\n# Check the first 6 rows of the dataset\ndata |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  radius texture perimeter   area smoothness compactness concavity\n1  17.99   10.38    122.80 1001.0    0.11840     0.27760    0.3001\n2  20.57   17.77    132.90 1326.0    0.08474     0.07864    0.0869\n3  19.69   21.25    130.00 1203.0    0.10960     0.15990    0.1974\n4  11.42   20.38     77.58  386.1    0.14250     0.28390    0.2414\n5  20.29   14.34    135.10 1297.0    0.10030     0.13280    0.1980\n6  12.45   15.70     82.57  477.1    0.12780     0.17000    0.1578\n  concave_points symmetry fractal_dimension response\n1        0.14710   0.2419           0.07871        1\n2        0.07017   0.1812           0.05667        1\n3        0.12790   0.2069           0.05999        1\n4        0.10520   0.2597           0.09744        1\n5        0.10430   0.1809           0.05883        1\n6        0.08089   0.2087           0.07613        1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the type of data\ndata |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t569 obs. of  11 variables:\n $ radius           : num  18 20.6 19.7 11.4 20.3 ...\n $ texture          : num  10.4 17.8 21.2 20.4 14.3 ...\n $ perimeter        : num  122.8 132.9 130 77.6 135.1 ...\n $ area             : num  1001 1326 1203 386 1297 ...\n $ smoothness       : num  0.1184 0.0847 0.1096 0.1425 0.1003 ...\n $ compactness      : num  0.2776 0.0786 0.1599 0.2839 0.1328 ...\n $ concavity        : num  0.3001 0.0869 0.1974 0.2414 0.198 ...\n $ concave_points   : num  0.1471 0.0702 0.1279 0.1052 0.1043 ...\n $ symmetry         : num  0.242 0.181 0.207 0.26 0.181 ...\n $ fractal_dimension: num  0.0787 0.0567 0.06 0.0974 0.0588 ...\n $ response         : num  1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the rows which do not have any entries\nsum(is.na(data)) # no NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nWe look at the distribution of the continuous data variables based on if response variable is 1 which indicates the tumor is malignant (cancerous) or response variable is 0 which indicates the tumor is benign (non cancerous).\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data for histogram\nmelted_data <- melt(na.omit(data0), id=\"diagnosis\")\nmelted_data$diagnosis <- ifelse(melted_data$diagnosis == \"M\",\"Malignant\",\n                                ifelse(melted_data$diagnosis == \"B\",\"Benign\",\"NA\"))\n\n\n# Plot the histogram of all the variables\nggplot(melted_data,aes(value))+\n  geom_histogram(aes(),bins = 30)+\n  facet_grid2(diagnosis~variable, scales=\"free\")+theme_bw()\n```\n\n::: {.cell-output-display}\n![](breast_cancer_files/figure-html/EDA1-1.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThere is a noticeable difference between the distribution of the variables for the two categories. Let us look at pairs plot which will help us understand the correlation between each pair of explanatory variables.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pairs plot between the explanatory variables to \n# check correlation between each pair of the variables\nggpairs(data[,-ncol(data)])\n```\n\n::: {.cell-output-display}\n![](breast_cancer_files/figure-html/EDA3-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nAs expected, there is multicollinearity in the data. One way to mitigate the effect of multicollinearity is to use $L2$ regularization (often called as ridge regression). Another way is to transform the data using principal component analysis (PCA) and use that data for regression. In this instance, we will first look at logistic regression and then logistic regression using L2 regularization. Let us look at the \n:::\n\n::: {style=\"text-align: justify\"}\nLet us look at the results of logistic regression.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split the data into training and testing data\nseed <- 23\nset.seed(seed)\n\nind <- sample(floor(0.8*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train <- data[ind,]\n\n# Testing dataset\ndata_test <- data[-ind,]\n###############################################################\n\n# Fit an binary logistic regression model\nmodel <- glm(response ~ ., data = data_train, family = binomial)\n\n# Check the summary of the model\nmodel |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = response ~ ., family = binomial, data = data_train)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -10.90839   14.88757  -0.733  0.46373    \nradius             -2.64293    4.05814  -0.651  0.51487    \ntexture             0.47784    0.08329   5.737 9.63e-09 ***\nperimeter           0.02482    0.57887   0.043  0.96580    \narea                0.04017    0.01981   2.027  0.04263 *  \nsmoothness        107.08151   39.67654   2.699  0.00696 ** \ncompactness        -2.66325   22.90510  -0.116  0.90744    \nconcavity           6.57732    9.57425   0.687  0.49210    \nconcave_points     56.61819   32.77129   1.728  0.08405 .  \nsymmetry            2.79133   12.47816   0.224  0.82299    \nfractal_dimension -40.94988   92.09458  -0.445  0.65657    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 615.54  on 454  degrees of freedom\nResidual deviance: 112.51  on 444  degrees of freedom\nAIC: 134.51\n\nNumber of Fisher Scoring iterations: 9\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_prob <- predict(model, data_test,\n                  type = \"response\")\n\ny_pred <- ifelse(y_pred_prob>0.5,1,0)\n\n# Generate the confusion matrix\nconf_matrix <- caret::confusionMatrix(as.factor(y_pred),\n                                      as.factor(data_test$response),\n                                      positive = \"1\")\nconf_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 77  2\n         1 11 24\n                                          \n               Accuracy : 0.886           \n                 95% CI : (0.8129, 0.9379)\n    No Information Rate : 0.7719          \n    P-Value [Acc > NIR] : 0.001462        \n                                          \n                  Kappa : 0.7113          \n                                          \n Mcnemar's Test P-Value : 0.026500        \n                                          \n            Sensitivity : 0.9231          \n            Specificity : 0.8750          \n         Pos Pred Value : 0.6857          \n         Neg Pred Value : 0.9747          \n             Prevalence : 0.2281          \n         Detection Rate : 0.2105          \n   Detection Prevalence : 0.3070          \n      Balanced Accuracy : 0.8990          \n                                          \n       'Positive' Class : 1               \n                                          \n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nOur logistic regression model has accuracy around 88.6% on the testing dataset. The misclassification rate on the testing data is (2+11)/114 = 0.1140. We will look at the ROC curve and AUC.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute ROC curve\nroc_curve <- roc(data_test$response,as.vector(y_pred_prob))\n# Calculate AUC\nauc_value <- auc(roc_curve)\n\n# Plot the ROC curve\nplot(roc_curve, col = \"blue\", lwd = 3, main = \"ROC Curve\")\n# Add AUC to the plot\nlegend(\"bottomright\", legend = paste(\"AUC =\", round(auc_value, 3)), col = \"blue\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](breast_cancer_files/figure-html/diagnostics_logistic_model-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nWe will now try the logistic regression with $L2$ regularization.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now try the logistic regression ridge (L2) regularization\nmodel_l2 <- cv.glmnet(as.matrix(data_train[,-ncol(data_train)]), \n                      data_train[,ncol(data_train)], \n                      family = \"binomial\",\n                      alpha=0)\n\n# Plot cross-validation results \nplot(model_l2) \n```\n\n::: {.cell-output-display}\n![](breast_cancer_files/figure-html/logistic_model_l2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# View the best lambda \nbest_lambda_l2 <- model_l2$lambda.min \nprint(best_lambda_l2) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03789937\n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit the final model with the best lambda \nfinal_model_l2 <- glmnet(as.matrix(data_train[,-ncol(data_train)]), \n                      data_train[,ncol(data_train)],\n                      family = \"binomial\", \n                      alpha = 0, \n                      lambda = best_lambda_l2)\n\ncoef(final_model_l2,s = best_lambda_l2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                             s1\n(Intercept)       -14.254070800\nradius              0.174447577\ntexture             0.189604875\nperimeter           0.025035861\narea                0.001661682\nsmoothness         34.526223296\ncompactness         5.508633703\nconcavity           5.821321243\nconcave_points     17.322503544\nsymmetry            5.139945976\nfractal_dimension -29.003877807\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the summary of the model\nfinal_model_l2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Length Class     Mode     \na0          1     -none-    numeric  \nbeta       10     dgCMatrix S4       \ndf          1     -none-    numeric  \ndim         2     -none-    numeric  \nlambda      1     -none-    numeric  \ndev.ratio   1     -none-    numeric  \nnulldev     1     -none-    numeric  \nnpasses     1     -none-    numeric  \njerr        1     -none-    numeric  \noffset      1     -none-    logical  \nclassnames  2     -none-    character\ncall        6     -none-    call     \nnobs        1     -none-    numeric  \n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_prob_l2 <- predict(final_model_l2, \n                          as.matrix(data_test[,-ncol(data_test)]),\n                          type = \"response\")\n\ny_pred_l2 <- ifelse(y_pred_prob_l2>0.5,1,0)\n\n# Generate the confusion matrix\nconf_matrix_l2 <- caret::confusionMatrix(as.factor(y_pred_l2),as.factor(data_test$response))\nconf_matrix_l2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 84  2\n         1  4 24\n                                         \n               Accuracy : 0.9474         \n                 95% CI : (0.889, 0.9804)\n    No Information Rate : 0.7719         \n    P-Value [Acc > NIR] : 3.302e-07      \n                                         \n                  Kappa : 0.8545         \n                                         \n Mcnemar's Test P-Value : 0.6831         \n                                         \n            Sensitivity : 0.9545         \n            Specificity : 0.9231         \n         Pos Pred Value : 0.9767         \n         Neg Pred Value : 0.8571         \n             Prevalence : 0.7719         \n         Detection Rate : 0.7368         \n   Detection Prevalence : 0.7544         \n      Balanced Accuracy : 0.9388         \n                                         \n       'Positive' Class : 0              \n                                         \n```\n\n\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nNow the misclassification rate on the testing data is (2+4)/114 = 0.0526. We will now try the logistic regression with $L2$ regularization, which should improve the performance.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute ROC curve\nroc_curve_l2 <- roc(data_test$response,as.vector(y_pred_prob_l2))\n\n# Plot the ROC curve\nplot(roc_curve_l2, col = \"blue\", lwd = 3, main = \"ROC Curve\")\n\n# Add AUC to the plot\nauc_value_l2 <- auc(roc_curve_l2)\nlegend(\"bottomright\", legend = paste(\"AUC =\", round(auc_value_l2, 3)), col = \"blue\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](breast_cancer_files/figure-html/diagnostics_model_l2-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nOur model has an accuracy of around 94% which indicates the model is correctly identifying the positive and negative cases in around 94% of the cases. Next, we look at the Receiver Operating Characteristic (ROC) curve. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR). It is the visualization of trade-off between correctly identifying positive cases and incorrectly identifying negative cases as positive. A good model has ROC curve which goes from bottom left to top left which means the model is perfectly identifying positive cases and does not identify negatives as positive. On the other hand, a ROC curve which is a straight line from bottom left to to right with slope 1 indicates the model is randomly assigning the positive and negative cases. Our curve is somewhere in between these 2 extreme cases and is decent. The area under the curve (AUC) is around 99% which is also good. Let us compare the coefficients of the model parameters for the two models.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef_model <- coef(model)\ncoef_model_l2 <- matrix(coef(final_model_l2,s = best_lambda_l2))\n\ncoef_combined <- data.frame(coef_model,coef_model_l2)\ncolnames(coef_combined) <- c(\"Logistic regression\", \"Logistic regression with L2 penalty\")\ncoef_combined\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  Logistic regression Logistic regression with L2 penalty\n(Intercept)              -10.90838572                       -14.254070800\nradius                    -2.64293183                         0.174447577\ntexture                    0.47783651                         0.189604875\nperimeter                  0.02482002                         0.025035861\narea                       0.04017044                         0.001661682\nsmoothness               107.08150915                        34.526223296\ncompactness               -2.66325384                         5.508633703\nconcavity                  6.57731768                         5.821321243\nconcave_points            56.61818911                        17.322503544\nsymmetry                   2.79132574                         5.139945976\nfractal_dimension        -40.94988370                       -29.003877807\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThis demonstrates that using L2 regularization in logistic regression where there is multicollinearity improves the model without any transformation. We can also try to build a model using the principal component analysis but that is for another day / dataset.\n:::\n\n\n\n\n",
    "supporting": [
      "breast_cancer_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}