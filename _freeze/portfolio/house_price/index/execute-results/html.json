{
  "hash": "15fe53214af21cc55f97ace898469190",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predictive model for house price using Gaussian Process regression\"\nsubtitle: \"Gaussian Process regression for non linear relationship to predict house price\"\nauthor: \"Aditya Ranade\"\nhighlight-style:\n            light: github\ndate: \"2025-03-03\"\ncategories: [analysis, R]\nimage: \"./house_price.jpg\"\n---\n\n\n\n::: {style=\"text-align: justify\"}\nGood houses are always in demand. However in the recent times, the price of house has increased exponentially. It will be interesting to identify the factors affecting the price of house. I found this [dataset](https://archive.ics.uci.edu/dataset/477/real+estate+valuation+data+set) on UCI machine learning repository which gives the house price per unit area and related variables.\n:::\n\n::: {style=\"text-align: justify\"}\nThe idea is to build a predictive model to predict house price per unit area based on the variables like the age of house, etc. We will look at the exploratory data analysis first and later build prediction models. First let us access and process the data through R.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the packages\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(car) # to calculate the VIF values\nlibrary(GGally) # for pairs plot using ggplot framework\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(rstanarm)\nlibrary(tidyr)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get starbucks data from github repo\npath = \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/real_estate/real_estate_valuation.csv\"\ndata0 <- read.csv(path, header = TRUE)\n\n# Data processing\n# clean the column names\ncolnames(data0) <- c(\"ID\", \"date\", \"house_age\", \"distance\", \"number_store\", \"latitude\", \"longitude\", \"price\")\n\n# Check the first 6 rows of the dataset\ndata0 |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ID     date house_age   distance number_store latitude longitude price\n1  1 2012.917      32.0   84.87882           10 24.98298  121.5402  37.9\n2  2 2012.917      19.5  306.59470            9 24.98034  121.5395  42.2\n3  3 2013.583      13.3  561.98450            5 24.98746  121.5439  47.3\n4  4 2013.500      13.3  561.98450            5 24.98746  121.5439  54.8\n5  5 2012.833       5.0  390.56840            5 24.97937  121.5425  43.1\n6  6 2012.667       7.1 2175.03000            3 24.96305  121.5125  32.1\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nWe will focus on the 3 variables as follows\n\n-   house_age : age of house in years.\n\n-   distance : distance to nearest MRT station in meters.\n\n-   number_store : the number of convenience stores in the living circle on foot.\n\n-   price : Price per unit area where 1 unit is 1 ping = 3.3 sq. meter\n\nLet us look at the distribution of these 3 variables\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check the rows which do not have any entries\nind.na <- sum(is.na(data0))\nind.na # No NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Filter the data\n# column house_age, distance and price\n# data <- data0 |> select(c(house_age,distance,price))\ndata <- data0[,c(\"house_age\",\"distance\",\"number_store\",\"price\")]\ndata |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  house_age   distance number_store price\n1      32.0   84.87882           10  37.9\n2      19.5  306.59470            9  42.2\n3      13.3  561.98450            5  47.3\n4      13.3  561.98450            5  54.8\n5       5.0  390.56840            5  43.1\n6       7.1 2175.03000            3  32.1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Data for histogram\nmelted_data <- melt(data)\n\n# Plot the histogram of all the variables\nggplot(melted_data,aes(value))+\n  geom_histogram(bins = 20)+\n  # geom_histogram(aes(y = after_stat(density)),bins = 20)+\n  facet_grid2(~variable, scales=\"free\")+theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/data_processing2-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nHistogram does not give much information. Let us look at the correlation plot to get an idea of how the variables are correlated with each other.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# correlation plot of all the variables\ncorr <- round(cor(data), 2)\nggcorrplot(corr)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/correlation_plot-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nhouse_age is not related to distance and number_store which is not surprising. distance variable is positively correlated with number of stores which again not surprising. However price is negatively correlated with distance to nearest MRT station as well as house age and positively correlated with number of stores in the vicinity which is again logical. Next we look at the pairs plot which will show the bivariate scatter plots as well as the correlation between each variables. Scatter plots in the last row is of interest as it shows the pairwise scatterplots where price is on the y axis and the other variables are on the x axis.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggpairs(data)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/pairplots-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe scatterplot of price vs. distance does not look linear but more of curved. We will focus on predicting price based on the distance variable. We will look at a simple linear regression where we will try to predict price as a function of distance. First let us convert the price and distance variable to log scale and look at scatterplot of the same.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# select the distance and price variable and convert to log scale\ndata2 <- data |> subset(,c(\"distance\", \"price\")) |> log()\n\n# scatterplot of price vs. distance\nggplot(data2,aes(price,distance))+geom_point()+\n  labs(y = \"log(price)\", x=\"log(distance)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/scatterplot_log-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nOnce we convert the variables to log scale, the scatterplot looks more linear compared to the scatterplot where the variables are not on log scale. We will first look at simple linear regression\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split the data into training and testing data\nseed <- 23\nset.seed(seed)\n\nind <- sample(floor(0.75*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train <- data2[ind,]\n# Testing dataset\ndata_test <- data2[-ind,]\n\n# Simple linear regression using raw data\nmodel <- lm(price ~ distance, data = data_train)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = price ~ distance, data = data_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.65672 -0.11925  0.00352  0.14145  0.95893 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.33983    0.08801   60.67   <2e-16 ***\ndistance    -0.27701    0.01353  -20.47   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2608 on 308 degrees of freedom\nMultiple R-squared:  0.5763,\tAdjusted R-squared:  0.5749 \nF-statistic: 418.9 on 1 and 308 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred <- predict(model, data_test)\n\n# Calculate residuals = observed - predicted\nresiduals <- (data_test$price - y_pred)\n\n# Residual vs. predicted plot\nggplot(NULL,aes(y_pred,residuals))+geom_point()+\n  labs(y = \"Residuals\", x=\"Predicted price on log scale\")+ \n  geom_hline(yintercept = 0, colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/SLR-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe residual plot shows a slight curved pattern which indicates the linearity assumption of the model is not satisfied. Hence our model is not reliable. This is not surprising since the scatterplot indicates a slight curved fit rather than a linear fit.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a observed vs. predicted plot\nggplot(NULL,aes(y_pred,data_test$price))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+\n  # lims(x=c(0,80),y=c(0,80))+\n  geom_abline()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/SLR_checks-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate RMSE\nrmse <- (residuals)^2 |> sum() |> sqrt()\nround(rmse,2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.43\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the assumptions of the regression model\npar(mfrow = c(2, 2))\nplot(model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/SLR_checks-2.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe model is has RMSE = 2.43 and the observed vs. predicted plot is just about decent. We will now look at a Gaussian process model which can handle non linear relationships very well.\n:::\n\n<!-- ::: {style=\"text-align: justify\"} -->\n\n<!-- ::: -->\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read the GP STAN model\n\n# Read the STAN file\nfile_stan <- \"GP.stan\"\n\n# Compile stan model\nmodel_stan <- cmdstan_model(stan_file = file_stan,\n                            cpp_options = list(stan_threads = TRUE))\nmodel_stan$check_syntax()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx_train <- data2[ind,1]\ny_train <- data2[ind,2]\nx_test <- data2[-ind,1]\ny_test <- data2[-ind,2]\n\nx_train <- x_train |> as.matrix()\nx_test <- x_test |> as.matrix()\n\nstandata <- list(K = ncol(x_train),\n                 N1 = nrow(x_train),\n                 X1 = x_train,\n                 Y1 = y_train,\n                 N2 = nrow(x_test),\n                 X2 = x_test,\n                 Y2 = y_test)\n\nfit_optim <- model_stan$optimize(data = standata,\n                                 seed = seed,\n                                 threads =  10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInitial log joint probability = -559.564 \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes  \n      14        256.76   0.000435624    0.00106958           1           1       18    \nOptimization terminated normally:  \n  Convergence detected: relative gradient magnitude is below tolerance \nFinished in  0.4 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\nfsum_optim <- as.data.frame(fit_optim$summary())\n\n# The optimized parameter would be \npar_ind <- 2:4\nopt_pars <- fsum_optim[par_ind,]\nopt_pars\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  variable estimate\n2   lambda 2.855100\n3    sigma 1.657640\n4      tau 0.248832\n```\n\n\n:::\n\n```{.r .cell-code}\n# starting value of parameters\nstart_parameters <- rep(list(list(lambda = opt_pars[1,2],\n                                  sigma = opt_pars[2,2],\n                                  tau = opt_pars[3,2])),4)\n\n# Run the MCMC with optimized values as the starting values\nfit <- model_stan$sample(\n  data = standata,\n  init = start_parameters,\n  seed = seed,\n  iter_warmup = 1000,\n  iter_sampling = 1000,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000,\n  threads =  8,\n  save_warmup = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains, with 8 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 405.5 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 408.7 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 428.2 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 439.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 420.5 seconds.\nTotal execution time: 439.8 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary\n# fit$summary()\n\n# Save the summary\nfsum <- as.data.frame(fit$summary())\n\n# Plot posterior distribution of parameters\nbayesplot::color_scheme_set(\"gray\")\nbayesplot::mcmc_dens(fit$draws(c(\"lambda\",\"sigma\",\"tau\")))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/GP2-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe posterior distribution of parameters look good with unimodal distribution and the trace plots also show good mix so we can say the parameters have converged.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# index for predictive mean and variance\npred_mean_ind <- max(par_ind)+(1:length(y_test))\npred_var_ind <- max(pred_mean_ind)+(1:length(y_test))\n\n# Prediction\ny_observed <- y_test #observed\ny_predicted <-  fsum[pred_mean_ind,c(2)] #predicted mean\ny_predicted_var <-  fsum[pred_var_ind,c(2)] #predicted\nlb <- y_predicted + qnorm(0.05,0,sqrt(y_predicted_var))\nub <- y_predicted + qnorm(0.95,0,sqrt(y_predicted_var))  \n\n# Predictions with bounds\npred <- ggplot(NULL,aes(x_test,y_predicted))+geom_line(linetype = 2)+\n  geom_line(aes(x_test,lb))+\n  geom_line(aes(x_test,ub))+\n  geom_point(aes(x_test,y_observed),col=\"red\")+\n  labs(y = \"log(price)\", x=\"log(distance)\")\npred\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/GP3-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe black dashed line is the predictive mean price on log scale for distance on log scale. The black solid lines give the 95% upper bound and lower bound for predictions. The points indicated in red color are the observations in testing data set and we can see most of them are in the prediction bounds.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observed vs predicted\novp_1d <- ggplot(NULL,aes(y_predicted,y_observed))+geom_point()+\n  labs(y = \"Observed\", x=\"Predicted\")+\n  geom_abline()\n\novp_1d\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/GP4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nrmse = sqrt(mean((y_observed-y_predicted)^2)) |> round(2)\nrmse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.22\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nObserved vs. predicted plot also looks good with a few points away from the line. The RMSE improved from 2.43 to 0.22 which is a good improvement.\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}