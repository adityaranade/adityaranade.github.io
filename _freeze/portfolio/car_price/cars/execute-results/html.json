{
  "hash": "bb4ea70f89cd25447cd87b979bce6661",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting car price\"\nsubtitle: \"AI models to predict car price using car features\"\nauthor: \"Aditya Ranade\"\nhighlight-style:\n  light: github\ndate: \"2025-05-15\"\ncategories: [analysis, R]\nimage: \"./car.jpg\"\n---\n\n\n\n::: {style=\"text-align: justify\"}\nI found this [dataset](https://archive.ics.uci.edu/dataset/10/automobile) on UCI machine learning repository which gives the dataset regarding the car features along with price. The goal is to predict the car price indicated by the variable price based on other features of the car like horespower, displacement, weight, etc. of car. We will compare multiple Machine Learning models for the same.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(GGally) # for pairs plot using ggplot framework\nlibrary(dplyr)\nlibrary(glmnet)\nlibrary(knitr)\n\n# Get cars data from github repo\npath <- \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/car_price/automobile.data\"\ndata0 <- read.table(path, \n                    sep = \",\", \n                    fill = TRUE, \n                    header = FALSE)\n\ncolnames(data0) <- c( \"symbolling\", \"normalized_losses\", \"make\", \"fuel_type\", \n                      \"aspiration\", \"doors\",\"body_style\", \"drive_wheels\", \n                      \"engine_location\", \"wheel_base\",\"length\", \"width\", \n                      \"height\", \"curb_weight\", \"engine_type\", \"cylinders\", \n                      \"engine_size\", \"fuel_system\", \"bore\", \"stroke\", \n                      \"compression_ratio\", \"horsepower\", \"peak_rpm\", \n                      \"city_mpg\", \"highway_mpg\", \"price\")\n\n# Check the type of data\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t205 obs. of  26 variables:\n $ symbolling       : int  3 3 1 2 2 2 1 1 1 0 ...\n $ normalized_losses: chr  \"?\" \"?\" \"?\" \"164\" ...\n $ make             : chr  \"alfa-romero\" \"alfa-romero\" \"alfa-romero\" \"audi\" ...\n $ fuel_type        : chr  \"gas\" \"gas\" \"gas\" \"gas\" ...\n $ aspiration       : chr  \"std\" \"std\" \"std\" \"std\" ...\n $ doors            : chr  \"two\" \"two\" \"two\" \"four\" ...\n $ body_style       : chr  \"convertible\" \"convertible\" \"hatchback\" \"sedan\" ...\n $ drive_wheels     : chr  \"rwd\" \"rwd\" \"rwd\" \"fwd\" ...\n $ engine_location  : chr  \"front\" \"front\" \"front\" \"front\" ...\n $ wheel_base       : num  88.6 88.6 94.5 99.8 99.4 ...\n $ length           : num  169 169 171 177 177 ...\n $ width            : num  64.1 64.1 65.5 66.2 66.4 66.3 71.4 71.4 71.4 67.9 ...\n $ height           : num  48.8 48.8 52.4 54.3 54.3 53.1 55.7 55.7 55.9 52 ...\n $ curb_weight      : int  2548 2548 2823 2337 2824 2507 2844 2954 3086 3053 ...\n $ engine_type      : chr  \"dohc\" \"dohc\" \"ohcv\" \"ohc\" ...\n $ cylinders        : chr  \"four\" \"four\" \"six\" \"four\" ...\n $ engine_size      : int  130 130 152 109 136 136 136 136 131 131 ...\n $ fuel_system      : chr  \"mpfi\" \"mpfi\" \"mpfi\" \"mpfi\" ...\n $ bore             : chr  \"3.47\" \"3.47\" \"2.68\" \"3.19\" ...\n $ stroke           : chr  \"2.68\" \"2.68\" \"3.47\" \"3.40\" ...\n $ compression_ratio: num  9 9 9 10 8 8.5 8.5 8.5 8.3 7 ...\n $ horsepower       : chr  \"111\" \"111\" \"154\" \"102\" ...\n $ peak_rpm         : chr  \"5000\" \"5000\" \"5000\" \"5500\" ...\n $ city_mpg         : int  21 21 19 24 18 19 19 19 17 16 ...\n $ highway_mpg      : int  27 27 26 30 22 25 25 25 20 22 ...\n $ price            : chr  \"13495\" \"16500\" \"16500\" \"13950\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Convert the specific data types to numeric\ndata0$bore <- as.numeric(data0$bore)\ndata0$stroke <- as.numeric(data0$stroke)\ndata0$horsepower <- as.numeric(data0$horsepower)\ndata0$peak_rpm <- as.numeric(data0$peak_rpm)\ndata0$price <- as.numeric(data0$price)\n\n# Check the type of data again\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t205 obs. of  26 variables:\n $ symbolling       : int  3 3 1 2 2 2 1 1 1 0 ...\n $ normalized_losses: chr  \"?\" \"?\" \"?\" \"164\" ...\n $ make             : chr  \"alfa-romero\" \"alfa-romero\" \"alfa-romero\" \"audi\" ...\n $ fuel_type        : chr  \"gas\" \"gas\" \"gas\" \"gas\" ...\n $ aspiration       : chr  \"std\" \"std\" \"std\" \"std\" ...\n $ doors            : chr  \"two\" \"two\" \"two\" \"four\" ...\n $ body_style       : chr  \"convertible\" \"convertible\" \"hatchback\" \"sedan\" ...\n $ drive_wheels     : chr  \"rwd\" \"rwd\" \"rwd\" \"fwd\" ...\n $ engine_location  : chr  \"front\" \"front\" \"front\" \"front\" ...\n $ wheel_base       : num  88.6 88.6 94.5 99.8 99.4 ...\n $ length           : num  169 169 171 177 177 ...\n $ width            : num  64.1 64.1 65.5 66.2 66.4 66.3 71.4 71.4 71.4 67.9 ...\n $ height           : num  48.8 48.8 52.4 54.3 54.3 53.1 55.7 55.7 55.9 52 ...\n $ curb_weight      : int  2548 2548 2823 2337 2824 2507 2844 2954 3086 3053 ...\n $ engine_type      : chr  \"dohc\" \"dohc\" \"ohcv\" \"ohc\" ...\n $ cylinders        : chr  \"four\" \"four\" \"six\" \"four\" ...\n $ engine_size      : int  130 130 152 109 136 136 136 136 131 131 ...\n $ fuel_system      : chr  \"mpfi\" \"mpfi\" \"mpfi\" \"mpfi\" ...\n $ bore             : num  3.47 3.47 2.68 3.19 3.19 3.19 3.19 3.19 3.13 3.13 ...\n $ stroke           : num  2.68 2.68 3.47 3.4 3.4 3.4 3.4 3.4 3.4 3.4 ...\n $ compression_ratio: num  9 9 9 10 8 8.5 8.5 8.5 8.3 7 ...\n $ horsepower       : num  111 111 154 102 115 110 110 110 140 160 ...\n $ peak_rpm         : num  5000 5000 5000 5500 5500 5500 5500 5500 5500 5500 ...\n $ city_mpg         : int  21 21 19 24 18 19 19 19 17 16 ...\n $ highway_mpg      : int  27 27 26 30 22 25 25 25 20 22 ...\n $ price            : num  13495 16500 16500 13950 17450 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the rows which do not have any entries\nsum(is.na(data0)) # 16 NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Exclude the rows with missing information\ndata1 <- na.omit(data0)\n\n# Check NA values again\nsum(is.na(data1)) # No NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the first 6 rows of the dataset\ndata1 |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  symbolling normalized_losses        make fuel_type aspiration doors\n1          3                 ? alfa-romero       gas        std   two\n2          3                 ? alfa-romero       gas        std   two\n3          1                 ? alfa-romero       gas        std   two\n4          2               164        audi       gas        std  four\n5          2               164        audi       gas        std  four\n6          2                 ?        audi       gas        std   two\n   body_style drive_wheels engine_location wheel_base length width height\n1 convertible          rwd           front       88.6  168.8  64.1   48.8\n2 convertible          rwd           front       88.6  168.8  64.1   48.8\n3   hatchback          rwd           front       94.5  171.2  65.5   52.4\n4       sedan          fwd           front       99.8  176.6  66.2   54.3\n5       sedan          4wd           front       99.4  176.6  66.4   54.3\n6       sedan          fwd           front       99.8  177.3  66.3   53.1\n  curb_weight engine_type cylinders engine_size fuel_system bore stroke\n1        2548        dohc      four         130        mpfi 3.47   2.68\n2        2548        dohc      four         130        mpfi 3.47   2.68\n3        2823        ohcv       six         152        mpfi 2.68   3.47\n4        2337         ohc      four         109        mpfi 3.19   3.40\n5        2824         ohc      five         136        mpfi 3.19   3.40\n6        2507         ohc      five         136        mpfi 3.19   3.40\n  compression_ratio horsepower peak_rpm city_mpg highway_mpg price\n1               9.0        111     5000       21          27 13495\n2               9.0        111     5000       21          27 16500\n3               9.0        154     5000       19          26 16500\n4              10.0        102     5500       24          30 13950\n5               8.0        115     5500       18          22 17450\n6               8.5        110     5500       19          25 15250\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select the numerical columns and then draw a pairs plot\n# Pairs plot between the explanatory variables to \n# check correlation between each pair of the variables\ndata1 |> select(where(is.numeric)) |> ggpairs()\n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/EDA01-1.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe response variable, price is correlated with all the variables which is good. However, the explanatory variables are correlated within themselves which is not a good indication. This indicates there is some multicollinearity. This means two variables give similar information about the response variable. One way to mitigate the effect is to consider the principal components and then use the principal components for the models. Another way is to use some regularization to mitigate the effect of multicollinearity.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Histogram of price variable\nggplot(data1,aes(x=price))+\n  geom_histogram(aes(y = after_stat(density)),bins = 30)+\n  labs(title = \"Distribution of Prices\", \n       x = \"price\", y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/EDA02-1.png){width=1344}\n:::\n\n```{.r .cell-code}\n# log scale\nggplot(data1, aes(x = price)) + \n  geom_histogram(aes(y = after_stat(density)),bins = 30) +\n  labs(title = \"Log-Scale Distribution of Prices\", \n       x = \"log(price)\", y = \"Density\") +\n  scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/EDA02-2.png){width=1344}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Histogram of price according to make\nggplot(data1, aes(x = price, y = make)) + \n  geom_boxplot(fill=\"skyblue\", col=\"black\") +\n  labs(title = \"Price Distribution by Make\", \n       x = \"Make\", y = \"Price\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/EDA03-1.png){width=1344}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select specific columns for the ML model\n\ndata2 <- subset(data1, select = c(wheel_base,length,width,curb_weight,\n                                 engine_size, bore,\n                                 stroke,compression_ratio,horsepower,\n                                 peak_rpm,city_mpg,highway_mpg,price))\n\n# data2 <- data1 |> select(wheel_base,length,width,curb_weight,\n#                         #fuel_type,cylinders,\n#                         engine_size,bore, stroke,\n#                         compression_ratio,horsepower,\n#                         peak_rpm,city_mpg,highway_mpg,price)\n\n# split the data into training and testing data\nseed <- 55\nset.seed(seed)\n\nind <- sample(1:nrow(data2),\n              floor(0.8*nrow(data2)),\n              replace = FALSE)\n\n# Training dataset\ndata_train <- data2[ind,]\n# Testing dataset\ndata_test <- data2[-ind,]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a linear regression model\n# Fit a multiple linear regression model\nmodel_lm <- glm(price ~ ., data = data_train)\n\n# Check the summary of the model\nmodel_lm |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = price ~ ., data = data_train)\n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -5.396e+04  1.799e+04  -2.999 0.003196 ** \nwheel_base         1.334e+02  1.095e+02   1.218 0.225256    \nlength            -9.810e+01  6.262e+01  -1.566 0.119455    \nwidth              6.062e+02  3.089e+02   1.962 0.051671 .  \ncurb_weight        4.439e+00  1.994e+00   2.227 0.027536 *  \nengine_size        1.002e+02  1.868e+01   5.364 3.21e-07 ***\nbore              -4.447e+02  1.465e+03  -0.304 0.761889    \nstroke            -2.912e+03  8.441e+02  -3.450 0.000736 ***\ncompression_ratio  2.119e+02  1.012e+02   2.092 0.038165 *  \nhorsepower         2.922e+01  2.230e+01   1.311 0.192081    \npeak_rpm           1.916e+00  7.966e-01   2.405 0.017435 *  \ncity_mpg          -3.215e+02  2.202e+02  -1.460 0.146381    \nhighway_mpg        3.579e+02  2.007e+02   1.783 0.076701 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 10295453)\n\n    Null deviance: 9190090574  on 155  degrees of freedom\nResidual deviance: 1472249800  on 143  degrees of freedom\nAIC: 2976.1\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predictions on test data\ny_pred_lm <- predict(model_lm, data_test)\n\n# data frame for plotting\ndf_pred_mlr <- data.frame(predicted = y_pred_lm, \n                    observed = data_test$price)\ndf_pred_mlr$model <- \"mlr\"\n\n# evaluation metrics\nrmse_lm <- (data_test$price-y_pred_lm)^2 |> mean() |> sqrt()\nmae_lm <- (data_test$price-y_pred_lm) |> abs() |> mean()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n# Ensure all factor variables in test have the same levels as training\nfor (col in names(data_train)) {\n  if (is.factor(data_train[[col]])) {\n    data_test[[col]] <- factor(data_test[[col]], levels = levels(data_train[[col]]))\n  }\n}\n\n# Now build matrices\nX  <- model.matrix(price ~ . - 1, data = data_train)\ny  <- data_train$price\nX.new <- model.matrix(price ~ . - 1, data = data_test)\n\nmodel_l2_cv <- cv.glmnet(X,y,alpha = 0)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda <- model_l2_cv$lambda.min\n\nmodel_l2 <- glmnet(X,y,\n                   alpha = 1,\n                   lambda = best_lambda)\n#coef(model_l2)\n\n# for predictions\nX.new <- model.matrix(price ~ . - 1,\n                      data = data_test)\n\n# Predictions on test data\ny_pred_l2 <- predict(model_l2,\n                     s = best_lambda,\n                     newx= X.new)\n\n# data frame for plotting\ndf_pred_l2 <- data.frame(predicted = as.vector(y_pred_l2),\n                         observed = data_test$price)\ndf_pred_l2$model <- \"ridge\"\n\n# evaluation metrics \nrmse_l2 <- (data_test$price-y_pred_l2)^2 |> mean() |> sqrt()\nmae_l2 <- (data_test$price-y_pred_l2) |> abs() |> mean()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_l1_cv <- cv.glmnet(as.matrix(data_train[,-ncol(data_train)]),\n                         as.matrix(data_train[,ncol(data_train)]),\n                         alpha = 0)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda_l1 <- model_l1_cv$lambda.min\nbest_lambda_l1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1174.377\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_l1 <- glmnet(as.matrix(data_train[,-ncol(data_train)]),\n                   as.matrix(data_train[,ncol(data_train)]),\n                   alpha = 0,\n                   lambda = best_lambda_l1)\n#coef(model_l1)\n\n# Predictions on test data\ny_pred_l1 <- predict(model_l1,  s = best_lambda_l1,\n                     newx= as.matrix(data_test[,-ncol(data_train)]))\n\n# data frame for plotting\ndf_pred_l1 <- data.frame(predicted = as.vector(y_pred_l1),\n                         observed = data_test$price)\ndf_pred_l1$model <- \"ridge\"\n\n# evaluation metrics\nrmse_l1 <- (data_test$price-y_pred_l1)^2 |> mean() |> sqrt()\nmae_l1 <- (data_test$price-y_pred_l1) |> abs() |> mean()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Elastic net\nmodel_en_cv <- cv.glmnet(as.matrix(data_train[,-ncol(data_train)]),\n                         as.matrix(data_train[,ncol(data_train)]),\n                         alpha = 0.5)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda_en <- model_en_cv$lambda.min\n\n\nmodel_en <- glmnet(as.matrix(data_train[,-ncol(data_train)]),\n                   as.matrix(data_train[,ncol(data_train)]),\n                   alpha = 0.5,\n                   lambda = best_lambda_en)\n#coef(model_en)\n\n# Predictions on test data\ny_pred_en <- predict(model_en,  s = best_lambda_en,\n                     newx= as.matrix(data_test[,-ncol(data_train)]))\n\n# data frame for plotting\ndf_pred_en <- data.frame(predicted = as.vector(y_pred_en),\n                    observed = data_test$price)\ndf_pred_en$model <- \"elastic_net\"\n\n# evaluation metrics\nrmse_en <- (data_test$price-y_pred_en)^2 |> mean() |> sqrt()\nmae_en <- (data_test$price-y_pred_en) |> abs() |> mean()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tree approach\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Fit regression tree\nmodel_tree <- rpart(price ~ ., data = data_train, method = \"anova\")\n\n# Summary\nsummary(model_tree)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nrpart(formula = price ~ ., data = data_train, method = \"anova\")\n  n= 156 \n\n          CP nsplit  rel error    xerror       xstd\n1 0.66748804      0 1.00000000 1.0108913 0.18358732\n2 0.19874312      1 0.33251196 0.3413841 0.03673654\n3 0.01975068      2 0.13376884 0.1499870 0.02214728\n4 0.01882083      3 0.11401816 0.1438535 0.02136248\n5 0.01000000      4 0.09519733 0.1338515 0.02033514\n\nVariable importance\nengine_size curb_weight    city_mpg  horsepower highway_mpg       width \n         22          20          15          13          13          12 \n     length  wheel_base \n          4           1 \n\nNode number 1: 156 observations,    complexity param=0.667488\n  mean=12895.28, MSE=5.891084e+07 \n  left son=2 (143 obs) right son=3 (13 obs)\n  Primary splits:\n      engine_size < 182    to the left,  improve=0.6674880, (0 missing)\n      curb_weight < 2697.5 to the left,  improve=0.5362224, (0 missing)\n      highway_mpg < 28.5   to the right, improve=0.5099084, (0 missing)\n      city_mpg    < 22.5   to the right, improve=0.5085646, (0 missing)\n      horsepower  < 118    to the left,  improve=0.5079888, (0 missing)\n  Surrogate splits:\n      curb_weight < 3338   to the left,  agree=0.981, adj=0.769, (0 split)\n      width       < 69.25  to the left,  agree=0.968, adj=0.615, (0 split)\n      city_mpg    < 16.5   to the right, agree=0.968, adj=0.615, (0 split)\n      horsepower  < 169    to the left,  agree=0.962, adj=0.538, (0 split)\n      highway_mpg < 21     to the right, agree=0.955, adj=0.462, (0 split)\n\nNode number 2: 143 observations,    complexity param=0.1987431\n  mean=11004.58, MSE=1.940408e+07 \n  left son=4 (93 obs) right son=5 (50 obs)\n  Primary splits:\n      curb_weight < 2544   to the left,  improve=0.6582376, (0 missing)\n      highway_mpg < 28.5   to the right, improve=0.5964810, (0 missing)\n      wheel_base  < 98.95  to the left,  improve=0.5957530, (0 missing)\n      length      < 176.4  to the left,  improve=0.5499178, (0 missing)\n      width       < 66.05  to the left,  improve=0.5262094, (0 missing)\n  Surrogate splits:\n      highway_mpg < 28.5   to the right, agree=0.944, adj=0.84, (0 split)\n      city_mpg    < 22     to the right, agree=0.909, adj=0.74, (0 split)\n      engine_size < 126    to the left,  agree=0.902, adj=0.72, (0 split)\n      horsepower  < 104    to the left,  agree=0.888, adj=0.68, (0 split)\n      length      < 178.15 to the left,  agree=0.881, adj=0.66, (0 split)\n\nNode number 3: 13 observations\n  mean=33693, MSE=2.161777e+07 \n\nNode number 4: 93 observations,    complexity param=0.01975068\n  mean=8384.097, MSE=4263549 \n  left son=8 (48 obs) right son=9 (45 obs)\n  Primary splits:\n      curb_weight < 2216.5 to the left,  improve=0.4577704, (0 missing)\n      horsepower  < 83     to the left,  improve=0.3937776, (0 missing)\n      engine_size < 105.5  to the left,  improve=0.3843303, (0 missing)\n      length      < 175    to the left,  improve=0.3785992, (0 missing)\n      wheel_base  < 98.6   to the left,  improve=0.3736707, (0 missing)\n  Surrogate splits:\n      length      < 167.4  to the left,  agree=0.882, adj=0.756, (0 split)\n      width       < 64.5   to the left,  agree=0.882, adj=0.756, (0 split)\n      engine_size < 105.5  to the left,  agree=0.882, adj=0.756, (0 split)\n      wheel_base  < 96     to the left,  agree=0.871, adj=0.733, (0 split)\n      city_mpg    < 27.5   to the right, agree=0.860, adj=0.711, (0 split)\n\nNode number 5: 50 observations,    complexity param=0.01882083\n  mean=15878.68, MSE=1.103613e+07 \n  left son=10 (25 obs) right son=11 (25 obs)\n  Primary splits:\n      wheel_base  < 100.8  to the left,  improve=0.3134524, (0 missing)\n      width       < 68.6   to the left,  improve=0.2351563, (0 missing)\n      length      < 186.65 to the left,  improve=0.1914644, (0 missing)\n      curb_weight < 2697.5 to the left,  improve=0.1466294, (0 missing)\n      peak_rpm    < 5275   to the left,  improve=0.1203158, (0 missing)\n  Surrogate splits:\n      length            < 186.65 to the left,  agree=0.90, adj=0.80, (0 split)\n      width             < 66.7   to the left,  agree=0.86, adj=0.72, (0 split)\n      curb_weight       < 2930.5 to the left,  agree=0.80, adj=0.60, (0 split)\n      bore              < 3.66   to the left,  agree=0.66, adj=0.32, (0 split)\n      compression_ratio < 9.405  to the left,  agree=0.66, adj=0.32, (0 split)\n\nNode number 8: 48 observations\n  mean=7031.417, MSE=852175.8 \n\nNode number 9: 45 observations\n  mean=9826.956, MSE=3868779 \n\nNode number 10: 25 observations\n  mean=14018.76, MSE=6810907 \n\nNode number 11: 25 observations\n  mean=17738.6, MSE=8342755 \n```\n\n\n:::\n\n```{.r .cell-code}\n# control complexity of tree\nprintcp(model_tree) # shows cross-validated error by cp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression tree:\nrpart(formula = price ~ ., data = data_train, method = \"anova\")\n\nVariables actually used in tree construction:\n[1] curb_weight engine_size wheel_base \n\nRoot node error: 9190090574/156 = 58910837\n\nn= 156 \n\n        CP nsplit rel error  xerror     xstd\n1 0.667488      0  1.000000 1.01089 0.183587\n2 0.198743      1  0.332512 0.34138 0.036737\n3 0.019751      2  0.133769 0.14999 0.022147\n4 0.018821      3  0.114018 0.14385 0.021362\n5 0.010000      4  0.095197 0.13385 0.020335\n```\n\n\n:::\n\n```{.r .cell-code}\nbest_cp <- model_tree$cptable[which.min(model_tree$cptable[,\"xerror\"]), \"CP\"]\n\n# Plot\nmodel_tree_pruned <- prune(model_tree, cp = best_cp)\nrpart.plot(model_tree_pruned, type = 3, extra = 101)\n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/tree-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Predictions on test data\ny_pred_tree <- predict(model_tree, data_test)\n\n# data frame for plotting\ndf_pred_tree <- data.frame(predicted = y_pred_tree, \n                    observed = data_test$price)\ndf_pred_tree$model <- \"tree\"\n\n# evaluation metrics\nrmse_tree <- (data_test$price-y_pred_tree)^2 |> mean() |> sqrt()\nmae_tree <- (data_test$price-y_pred_tree) |> abs() |> mean()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random forest\nlibrary(randomForest)\nmodel_rf <- randomForest(price ~ ., data = data_train)\npredict(model_rf, data_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        3         6         9        13        21        22        25        42 \n14262.940 14275.078 20853.727 18738.098  6010.565  5931.475  6588.243 11366.779 \n       44        47        49        51        54        60        75        76 \n 9030.611 11565.093 33850.958  6543.531  6804.512 10153.271 37203.109 19382.710 \n       77        87        96       105       106       108       109       115 \n 6039.265  8294.962  7361.326 16943.393 19797.553 16033.994 16451.646 17763.117 \n      117       126       127       128       152       154       159       160 \n16428.010 16002.628 30780.565 30780.565  6475.500  7676.276  8259.292  8255.036 \n      162       174       193       194       195       198       200 \n 8110.738 10512.567 12322.078 12314.946 15558.877 15669.915 18742.750 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Predictions on test data\ny_pred_rf <- predict(model_rf, data_test)\n\n# data frame for plotting\ndf_pred_rf <- data.frame(predicted = y_pred_rf, \n                      observed = data_test$price)\ndf_pred_rf$model <- \"random_forest\"\n\n# evaluation metrics\nrmse_rf <- (data_test$price-y_pred_rf)^2 |> mean() |> sqrt()\nmae_rf <- (data_test$price-y_pred_rf) |> abs() |> mean()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# SVM\nlibrary(e1071)\nmodel_svm <- svm(price ~ ., data = data_train, \n                 kernel = \"radial\", cost = 10, \n                 gamma = 0.1)\n\n# Predictions on test data\ny_pred_svm <- predict(model_svm, data_test)\n\n# data frame for plotting\ndf_pred_svm <- data.frame(predicted = y_pred_svm, \n                          observed = data_test$price)\ndf_pred_svm$model <- \"svm\"\n\n# evaluation metrics\nrmse_svm <- (data_test$price-y_pred_svm)^2 |> mean() |> sqrt()\nmae_svm <- (data_test$price-y_pred_svm) |> abs() |> mean()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot observed vs. predicted for all the models\ndf_pred <- rbind(df_pred_mlr, df_pred_tree, \n                 df_pred_l1,df_pred_l2,df_pred_en,\n                 df_pred_rf, df_pred_svm)\n\n# Create a observed vs. predicted plot combined for all the models\nggplot(df_pred,aes(predicted,observed))+geom_point()+\n  lims(x = c(10000,50000) , y = c(10000,50000))+\n  labs(y = \"Observed\", x=\"Predicted\")+\n  facet_grid(~model, scales=\"free\")+\n  geom_abline()+\n  theme_bw(base_size = 15)\n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/metrics-1.png){width=1344}\n:::\n\n```{.r .cell-code}\n# Evaluation metrics for all the models\nmetrics_all <- data.frame(Model = c(\"linear_model\", \"lasso\", \"ridge\",\n                                    \"elastic_net\", \"tree\", \"random_forest\",\n                                    \"svm\"),\n                          RMSE = c(rmse_lm,rmse_l1, rmse_l2, rmse_en,\n                                   rmse_tree, rmse_rf, rmse_svm),\n                          MAE = c(mae_lm,mae_l1, mae_l2, mae_en,\n                                  mae_tree, mae_rf, mae_svm))\n\n# Print evaluation metrics\nkable(metrics_all, digits = 0, caption = \"Model Performance Metrics\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Model Performance Metrics\n\n|Model         | RMSE|  MAE|\n|:-------------|----:|----:|\n|linear_model  | 3352| 2475|\n|lasso         | 3734| 2543|\n|ridge         | 4027| 2693|\n|elastic_net   | 3461| 2477|\n|tree          | 3502| 2555|\n|random_forest | 2291| 1552|\n|svm           | 2313| 1863|\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics_long <- melt(metrics_all, id.vars = \"Model\",\n                     variable.name = \"Metric\",\n                     value.name = \"Value\")\n\n# To plot RMSE and MAE side by side\nggplot(metrics_long, aes(x = Model, y = Value, fill = Model)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(aes(label = round(Value)), \n            position = position_dodge(width = 0.5),\n            hjust = 1.25, size = 3) +\n  coord_flip(clip = \"off\") +  # horizontal bars, no clipping of text\n  facet_grid2(~Metric, scales=\"free\")+\n  labs(title = \"Model Performance Comparison\", \n       y = \"Error Value\", x = \"Model\") +\n  theme_bw(base_size = 15) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](cars_files/figure-html/metrics2-1.png){width=1344}\n:::\n\n```{.r .cell-code}\n# # To plot RMSE and MAE on the same plot\n# ggplot(metrics_long, aes(x = Model, y = Value, fill = Metric)) +\n#   geom_col(position = position_dodge(width = 0.8)) +\n#   geom_text(aes(label = round(Value, 2)),\n#             position = position_dodge(width = 0.8),\n#             vjust = -0.3, size = 3) +\n#   labs(title = \"Model Performance Comparison\",\n#        y = \"Error Value\", x = \"Model\") +\n#   theme_bw(base_size = 10) +\n#   theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nBased on the metrics, random forest model seems to be the best model.\n:::\n",
    "supporting": [
      "cars_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}