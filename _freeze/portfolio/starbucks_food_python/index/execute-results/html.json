{
  "hash": "cd48bcb8730c713a45da64198b0293a7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Starbucks food nutritional information\"\nsubtitle: \"We all like starbucks food, but what about the nutritional value of them?\"\nauthor: \"Aditya Ranade\"\nhighlight-style: github-light\ndate: \"2025-03-14\"\ncategories: [analysis, python]\nimage: \"./starbucks_food.jpg\"\njupyter: python3\n---\n\n\n\n\n::: {style=\"text-align: justify\"}\nStarbucks is one of the most valued coffee chain in the world. A lot of people like to consume the food available at starbucks. But how good are they in terms of the nutritional value?\n:::\n\n::: {style=\"text-align: justify\"}\nI found this dataset on Kaggle which gives the nutritional information about their food products. In my precious post, I built a multiple linear regression model to predict the calories in beverage based on the nutritional contents of the beverage. Now we will try to do the same for the food products.\n\nFirst, we look at the exploratory data analysis and later try some simple regression models. First let us access and process the data through python\n:::\n\n::: {#cell-load-packages .cell execution_count=1}\n``` {.python .cell-code}\n# Load Libraries\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom plotnine import * # for plots\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nimport random\nfrom scipy.stats import pearsonr\n\n# Get starbucks data from github repo\n#df0=pd.read_csv(\"https://raw.githubusercontent.com//adityaranade//starbucks//refs//heads//main//data//starbucks-menu-nutrition-food.csv\", encoding='unicode_escape')\n\npath = \"https://raw.githubusercontent.com//adityaranade//portfolio//refs//heads//main//starbucks//starbucks-menu-nutrition-food.csv\"\ndf0=pd.read_csv(path, encoding='unicode_escape')\n\ndf0.head()\n```\n\n::: {#load-packages .cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Calories</th>\n      <th>Fat (g)</th>\n      <th>Carb. (g)</th>\n      <th>Fiber (g)</th>\n      <th>Protein (g)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Chonga Bagel</td>\n      <td>300</td>\n      <td>5.0</td>\n      <td>50</td>\n      <td>3</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8-Grain Roll</td>\n      <td>380</td>\n      <td>6.0</td>\n      <td>70</td>\n      <td>7</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Almond Croissant</td>\n      <td>410</td>\n      <td>22.0</td>\n      <td>45</td>\n      <td>3</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Apple Fritter</td>\n      <td>460</td>\n      <td>23.0</td>\n      <td>56</td>\n      <td>2</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Banana Nut Bread</td>\n      <td>420</td>\n      <td>22.0</td>\n      <td>52</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#data_processing1 .cell execution_count=2}\n``` {.python .cell-code}\n#modify the column names\ndf0.columns = ['name', 'calories','fat','carbs','fiber','protein']\ndf0.head()\n\n#convert data type to float for all the columns except name\nfor i in df0.columns[1:]:\n    df0[i]=df0[i].astype(\"float\")\n# df0.info()\n```\n:::\n\n\n::: {#data_processing2 .cell execution_count=3}\n``` {.python .cell-code}\ndf = df0\n# Use melt function for the histograms\ndf2 = pd.melt(df, id_vars=['name'])\n# df2.head()\n```\n:::\n\n\n::: {style=\"text-align: justify\"}\nNow that we have the data ready, let us look at the histogram of each variables namely calories, fat, carbs, fiber, protein and sodium\n:::\n\n::: {#cell-EDA .cell execution_count=4}\n``` {.python .cell-code}\np = (\n    ggplot(df2, aes(\"value\"))\n    + geom_histogram(bins=15)\n    + facet_grid(\". ~ variable\", scales='free_x')\n    )\n\np.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/eda-output-1.png){#eda width=672 height=480}\n:::\n:::\n\n\nThe histogram of each of the variables do not show any problems as all the plots look decent. We will look at the correlation plot.\n\n::: {#cell-EDA2 .cell execution_count=5}\n``` {.python .cell-code}\n# Check the correlation between the variables\n# plt.figure(figsize=(20,7))\nsns.heatmap(df.iloc[:,1:].corr(),annot=True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/eda2-output-1.png){#eda2}\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nCorrelation plot indicates positive association between all the variables which is desired. Now we will look at the pairs plot which will show the pairwise histogram.\n:::\n\n\n::: {#cell-EDA3 .cell execution_count=7}\n``` {.python .cell-code}\n# Pairs plot\ng = sns.PairGrid(df.iloc[:,1:])\ng.map_diag(sns.histplot)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/eda3-output-1.png){#eda3}\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nThe scatterplots of each variable with calories which can be seen in the upper triangular plots in the very first row. It seems there is a linear association between calories and fat, carbs and protein. However, it does not seem to have a linear association with fiber.\n:::\n\n::: {#cell-mlr_train .cell execution_count=8}\n``` {.python .cell-code}\n# Split data into train and test set\nindices = range(len(df)) # Create a list of indices\n\n# Get 75% random indices for training data\nrandom.seed(23) # for repreducible example\nrandom_indices = random.sample(indices, round(0.75*len(df)))\n\n# Training dataset\ndata_train = df.iloc[random_indices,]\n\n# Testing dataset\ndata_test = df.iloc[df.index.difference(random_indices),]\n\n\n# Build a multiple linear regression model to predict calories using other variables using training data\nresult = smf.ols(\"calories ~ fat + carbs + fiber + protein\", data = data_train).fit()\n# check the summary\nresult.summary()\n```\n\n::: {#mlr_train .cell-output .cell-output-display execution_count=8}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>        <td>calories</td>     <th>  R-squared:         </th> <td>   0.995</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4351.</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sat, 17 May 2025</td> <th>  Prob (F-statistic):</th> <td>1.06e-92</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>10:05:52</td>     <th>  Log-Likelihood:    </th> <td> -304.54</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    85</td>      <th>  AIC:               </th> <td>   619.1</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    80</td>      <th>  BIC:               </th> <td>   631.3</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   -2.8394</td> <td>    3.006</td> <td>   -0.945</td> <td> 0.348</td> <td>   -8.821</td> <td>    3.142</td>\n</tr>\n<tr>\n  <th>fat</th>       <td>    8.8760</td> <td>    0.129</td> <td>   68.933</td> <td> 0.000</td> <td>    8.620</td> <td>    9.132</td>\n</tr>\n<tr>\n  <th>carbs</th>     <td>    4.0291</td> <td>    0.066</td> <td>   61.478</td> <td> 0.000</td> <td>    3.899</td> <td>    4.160</td>\n</tr>\n<tr>\n  <th>fiber</th>     <td>   -1.1151</td> <td>    0.402</td> <td>   -2.772</td> <td> 0.007</td> <td>   -1.916</td> <td>   -0.315</td>\n</tr>\n<tr>\n  <th>protein</th>   <td>    4.3474</td> <td>    0.156</td> <td>   27.915</td> <td> 0.000</td> <td>    4.037</td> <td>    4.657</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 8.990</td> <th>  Durbin-Watson:     </th> <td>   2.042</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.011</td> <th>  Jarque-Bera (JB):  </th> <td>  12.887</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.426</td> <th>  Prob(JB):          </th> <td> 0.00159</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 4.707</td> <th>  Cond. No.          </th> <td>    151.</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nNow let us make prediction on the testing data and plot the observed vs. predicted plot\n:::\n\n::: {#cell-prediction_test .cell execution_count=9}\n``` {.python .cell-code}\n# Make predictions using testing data\npredictions = result.predict(data_test)\n\n# Observed vs. Predicted plot\nplt.figure(figsize=(20,7))\nplt.scatter(predictions, data_test[\"calories\"])\nplt.ylabel(\"Observed calories\")\nplt.xlabel(\"Predicted calories\")\n# Create the abline\nx_line = np.linspace(min(data_test[\"calories\"]), max(data_test[\"calories\"]), 100)\ny_line = 1 * x_line + 1\nplt.plot(x_line, y_line, color='red')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/prediction_test-output-1.png){#prediction_test}\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nThe observed vs. predicted looks good. However there is low number of data points and hence we should take this with a grain of salt. Let us check some evaluation metrics like the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n:::\n\n::: {#83bd6c9b .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nprint(\"Mean Absolute Error:\",round(mean_absolute_error(data_test[\"calories\"],predictions),2))\nprint(\"Root Mean Squared Error:\",round((mean_squared_error(data_test[\"calories\"],predictions))** 0.5,2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Absolute Error: 7.54\nRoot Mean Squared Error: 10.7\n```\n:::\n:::\n\n\n::: {style=\"text-align: justify\"}\nRoot Mean Squared Error (RMSE) of 7.54 and Mean Absolute Error (MAE) of 10.7 is decent and indicates model is performing fairly well.\n:::\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}