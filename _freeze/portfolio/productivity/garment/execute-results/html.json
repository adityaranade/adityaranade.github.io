{
  "hash": "ae8b019298fade6ea362ab5ff7bc1bbe",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting productivity in garment factory\"\nsubtitle: \"Understanding the factors affecting the productivity in a garment factory\"\nauthor: \"Aditya Ranade\"\nhighlight-style:\n  light: github\ndate: \"2025-06-08\"\ncategories: [analysis, R]\nimage: \"./garment.jpg\"\n---\n\n\n\n::: {style=\"text-align: justify\"}\nI found this [dataset](https://archive.ics.uci.edu/dataset/597/productivity+prediction+of+garment+employees) on UCI machine learning repository which gives the dataset regarding the productivity in a garment factory. One variable measures the target productivity which has been set by the management and the variable of interest is the actual productivity. The goal is to predict the actual predictivity using the other variables.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(GGally) # for pairs plot using ggplot framework\nlibrary(dplyr)\nlibrary(glmnet)\nlibrary(knitr)\n\n# Get cars data from github repo\npath <- \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/productivity/productivity.csv\"\ndata0 <- read.table(path, fill = TRUE, header = FALSE)\n\ndata0 <- read.csv(path, header = TRUE)\n\nhead(data0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      date  quarter department      day team targeted_productivity   smv  wip\n1 1/1/2015 Quarter1     sweing Thursday    8                  0.80 26.16 1108\n2 1/1/2015 Quarter1 finishing  Thursday    1                  0.75  3.94   NA\n3 1/1/2015 Quarter1     sweing Thursday   11                  0.80 11.41  968\n4 1/1/2015 Quarter1     sweing Thursday   12                  0.80 11.41  968\n5 1/1/2015 Quarter1     sweing Thursday    6                  0.80 25.90 1170\n6 1/1/2015 Quarter1     sweing Thursday    7                  0.80 25.90  984\n  over_time incentive idle_time idle_men no_of_style_change no_of_workers\n1      7080        98         0        0                  0          59.0\n2       960         0         0        0                  0           8.0\n3      3660        50         0        0                  0          30.5\n4      3660        50         0        0                  0          30.5\n5      1920        50         0        0                  0          56.0\n6      6720        38         0        0                  0          56.0\n  actual_productivity\n1           0.9407254\n2           0.8865000\n3           0.8005705\n4           0.8005705\n5           0.8003819\n6           0.8001250\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the type of data\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t1197 obs. of  15 variables:\n $ date                 : chr  \"1/1/2015\" \"1/1/2015\" \"1/1/2015\" \"1/1/2015\" ...\n $ quarter              : chr  \"Quarter1\" \"Quarter1\" \"Quarter1\" \"Quarter1\" ...\n $ department           : chr  \"sweing\" \"finishing \" \"sweing\" \"sweing\" ...\n $ day                  : chr  \"Thursday\" \"Thursday\" \"Thursday\" \"Thursday\" ...\n $ team                 : int  8 1 11 12 6 7 2 3 2 1 ...\n $ targeted_productivity: num  0.8 0.75 0.8 0.8 0.8 0.8 0.75 0.75 0.75 0.75 ...\n $ smv                  : num  26.16 3.94 11.41 11.41 25.9 ...\n $ wip                  : int  1108 NA 968 968 1170 984 NA 795 733 681 ...\n $ over_time            : int  7080 960 3660 3660 1920 6720 960 6900 6000 6900 ...\n $ incentive            : int  98 0 50 50 50 38 0 45 34 45 ...\n $ idle_time            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ idle_men             : int  0 0 0 0 0 0 0 0 0 0 ...\n $ no_of_style_change   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ no_of_workers        : num  59 8 30.5 30.5 56 56 8 57.5 55 57.5 ...\n $ actual_productivity  : num  0.941 0.886 0.801 0.801 0.8 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the rows which do not have any entries\nsum(is.na(data0)) # 506 NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 506\n```\n\n\n:::\n\n```{.r .cell-code}\n# Delete the NA values\ndata1 <- na.omit(data0)\nsum(is.na(data1)) # no NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe distributions of the continuous variables on the original scale indicates some non linear relationships between the response variable actual_productivity and the other variables. So we convert the data to log scale and the relationships become close to linear. Hence we will use the data on log scale for predictions. The distribution of the data on the log scale is as follows\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pairs plot between the explanatory variables to \n# check correlation between each pair of the \n# continuous variables\nggpairs(data1[,sapply(data1,is.numeric)])\n```\n\n::: {.cell-output-display}\n![](garment_files/figure-html/EDA01-1.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe response variable, actual_productivity is correlated with all the variables which is good. However, the explanatory variables are correlated within themselves which is not a good indication. This indicates there is some multicollinearity. This means two variables give similar information about the response variable. One way to mitigate the effect is to consider the principal components and then use the principal components for the models. Another way is to use some regularization to mitigate the effect of multicollinearity.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split the data into training and testing data\ndataa <- data1 |> dplyr::select(targeted_productivity, incentive, \n                        over_time, actual_productivity)\n\n# If considering original scale\ndata <- dataa\n# # If considering log scale\n# dataa$over_time <- dataa$over_time + min(dataa$over_time[dataa$over_time > 0])\n# dataa$incentive <- dataa$incentive + min(dataa$incentive[dataa$incentive > 0])\n# data <- dataa |> log()\n\n# Split data in training and testing set\nseed <- 23\nset.seed(seed)\n\nind <- sample(floor(0.8*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train <- data[ind,]\n# Testing dataset\ndata_test <- data[-ind,]\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nBased on the correlation between pairs of variables and the relationship of each variable, it seems difficult to identify which variables will be useful in the regression model. So, we will look at variable which logically should impact the productivity. The variables we consider for the analysis are targeted productivity, incentive and overtime. First, we will look at a multiple linear regression model\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a multiple linear regression model\nmodel_lm <- glm(actual_productivity ~ ., data = data_train)\n\n# Check the summary of the model\nmodel_lm |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = actual_productivity ~ ., data = data_train)\n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            1.377e-01  2.624e-02   5.250 2.18e-07 ***\ntargeted_productivity  6.146e-01  3.641e-02  16.883  < 2e-16 ***\nincentive              3.480e-03  1.287e-04  27.045  < 2e-16 ***\nover_time             -2.763e-06  1.107e-06  -2.497   0.0128 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.005828163)\n\n    Null deviance: 14.5158  on 551  degrees of freedom\nResidual deviance:  3.1938  on 548  degrees of freedom\nAIC: -1267.6\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_lm <- predict(model_lm, data_test)\n\n# Data frame for observed vs predicted\ndf_pred_mlr <- data.frame(predicted = y_pred_lm, \n                    observed = data_test$actual_productivity)\ndf_pred_mlr$model <- \"mlr\"\n\n# Evaluation metrics\nrmse_lm <- sqrt(sum(data_test$actual_productivity-y_pred_lm)^2)\nmae_lm <- mean(abs(data_test$actual_productivity-y_pred_lm))\nr2_lm   <- 1 - sum((data_test$actual_productivity - y_pred_lm)^2) / sum((data_test$actual_productivity - mean(data_test$actual_productivity))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the lasso regression which uses the $L^1$ penalty.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Lasso regression (L1 penalty)\nmodel_l1_cv <- cv.glmnet(as.matrix(data_train[,-1]),\n                         as.matrix(data_train[,1]),\n                         alpha = 0)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda_l1 <- model_l1_cv$lambda.min\nbest_lambda_l1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.007170258\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_l1 <- glmnet(as.matrix(data_train[,-1]),\n                   as.matrix(data_train[,1]),\n                   alpha = 0, \n                   lambda = best_lambda_l1)\n\n# Coefficients of the lasso regression model \ncoef(model_l1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                               s0\n(Intercept)          4.224381e-01\nincentive           -2.986123e-04\nover_time           -1.624768e-06\nactual_productivity  4.535232e-01\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_l1 <- predict(model_l1,  s = best_lambda_l1,\n                     newx= as.matrix(data_test[,-1]))\n\n# Data frame for observed vs predicted\ndf_pred_l1 <- data.frame(predicted = as.vector(y_pred_l1), \n                    observed = data_test$actual_productivity)\ndf_pred_l1$model <- \"lasso\"\n\n# Evaluation metrics\nrmse_l1 <- sqrt(sum(data_test$actual_productivity-y_pred_l1)^2)\nmae_l1 <- mean(abs(data_test$actual_productivity-y_pred_l1))\nr2_l1   <- 1 - sum((data_test$actual_productivity - y_pred_l1)^2) / sum((data_test$actual_productivity - mean(data_test$actual_productivity))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the ridge regression which uses the $L^2$ penalty.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ridge regression (L2 penalty)\nmodel_l2_cv <- cv.glmnet(as.matrix(data_train[,-1]),\n                         as.matrix(data_train[,1]),\n                         alpha = 1)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda <- model_l2_cv$lambda.min\nbest_lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0001544785\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_l2 <- glmnet(as.matrix(data_train[,-1]),\n                   as.matrix(data_train[,1]),\n                   alpha = 1, \n                   lambda = best_lambda)\n\n# Coefficients of the ridge regression model \ncoef(model_l2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                               s0\n(Intercept)          3.692767e-01\nincentive           -7.601171e-04\nover_time           -1.152732e-06\nactual_productivity  5.515953e-01\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_l2 <- predict(model_l2,  s = best_lambda,\n                     newx= as.matrix(data_test[,-1]))\n\n# Data frame for observed vs predicted\ndf_pred_l2 <- data.frame(predicted = as.vector(y_pred_l2), \n                    observed = data_test$actual_productivity)\ndf_pred_l2$model <- \"ridge\"\n\n# Evaluation metrics\nrmse_l2 <- sqrt(sum(data_test$actual_productivity-y_pred_l2)^2)\nmae_l2 <- mean(abs(data_test$actual_productivity-y_pred_l2))\nr2_l2   <- 1 - sum((data_test$actual_productivity - y_pred_l2)^2) / sum((data_test$actual_productivity - mean(data_test$actual_productivity))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the elastic net regression which is a combination of lasso ($L^1$ penalty) and ridge ($L^2$ penalty) regression.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Elastic net\nmodel_en_cv <- cv.glmnet(as.matrix(data_train[,-1]),\n                         as.matrix(data_train[,1]),\n                         alpha = 0.5)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda_en <- model_en_cv$lambda.min\nbest_lambda_en\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002337147\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_en <- glmnet(as.matrix(data_train[,-1]),\n                   as.matrix(data_train[,1]),\n                   alpha = 0.5, \n                   lambda = best_lambda_en)\n\n# Coefficients of the elastic net regression model \ncoef(model_en)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                               s0\n(Intercept)          3.699412e-01\nincentive           -7.557217e-04\nover_time           -1.169229e-06\nactual_productivity  5.505558e-01\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_en <- predict(model_en,  s = best_lambda_en,\n                     newx= as.matrix(data_test[,-1]))\n\n# Data frame for observed vs predicted\ndf_pred_en <- data.frame(predicted = as.vector(y_pred_en), \n                    observed = data_test$actual_productivity)\ndf_pred_en$model <- \"elastic_net\"\n\n# Evaluation metrics\nrmse_en <- sqrt(sum(data_test$actual_productivity-y_pred_en)^2)\nmae_en <- mean(abs(data_test$actual_productivity-y_pred_en))\nr2_en   <- 1 - sum((data_test$actual_productivity - y_pred_en)^2) / sum((data_test$actual_productivity - mean(data_test$actual_productivity))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the tree based approach.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tree approach\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Fit regression tree\nmodel_tree <- rpart(actual_productivity ~ ., data = data_train, method = \"anova\")\n\n# summary(model_tree)\n\n# Plot\nrpart.plot(model_tree, type = 3, extra = 101, fallen.leaves = TRUE)\n```\n\n::: {.cell-output-display}\n![](garment_files/figure-html/tree-1.png){width=672}\n:::\n\n```{.r .cell-code}\ny_pred_tree <- predict(model_tree, data_test)\n\n# Data frame for observed vs predicted\ndf_pred_tree <- data.frame(predicted = y_pred_tree, \n                    observed = data_test$actual_productivity)\ndf_pred_tree$model <- \"tree\"\n\n# Evaluation metrics\nrmse_tree <- sqrt(sum(data_test$actual_productivity-y_pred_tree)^2)\nmae_tree <- mean(abs(data_test$actual_productivity-y_pred_tree))\nr2_tree   <- 1 - sum((data_test$actual_productivity - y_pred_tree)^2) / sum((data_test$actual_productivity - mean(data_test$actual_productivity))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the random forest approach. In random forest approach, we build multiple trees and then average the predictions of all the trees.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random forest\nlibrary(randomForest)\nmodel_rf <- randomForest(actual_productivity ~ ., data = data_train)\n\ny_pred_rf <- predict(model_rf, data_test)\n\n# Data frame for observed vs predicted\ndf_pred_rf <- data.frame(predicted = y_pred_rf, \n                    observed = data_test$actual_productivity)\ndf_pred_rf$model <- \"random_forest\"\n\n# Evaluation metrics\nrmse_rf <- sqrt(sum(data_test$actual_productivity-y_pred_rf)^2)\nmae_rf <- mean(abs(data_test$actual_productivity-y_pred_rf))\nr2_rf   <- 1 - sum((data_test$actual_productivity - y_pred_rf)^2) / sum((data_test$actual_productivity - mean(data_test$actual_productivity))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nNext, we will try the support vector machine (SVM) approach.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(e1071)\nmodel_svm <- svm(actual_productivity ~ ., data = data_train, \n                 kernel = \"radial\", cost = 10, \n                 gamma = 0.1)\n\n# Predict on test data\ny_pred_svm <- predict(model_svm, data_test)\n\n# Data frame for observed vs predicted\ndf_pred_svm <- data.frame(predicted = y_pred_svm, \n                    observed = data_test$actual_productivity)\ndf_pred_svm$model <- \"svm\"\n\n# Evaluation metrics\nrmse_svm <- sqrt(sum(data_test$actual_productivity-y_pred_svm)^2)\nmae_svm <- mean(abs(data_test$actual_productivity-y_pred_svm))\nr2_svm   <- 1 - sum((data_test$actual_productivity - y_pred_svm)^2) / sum((data_test$actual_productivity - mean(data_test$actual_productivity))^2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe observed vs. predicted for all the models side by side can be seen in the plot below\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot observed vs. predicted for all the models\ndf_pred <- rbind(df_pred_mlr,df_pred_l1,df_pred_l2,\n                 df_pred_en, df_pred_tree, df_pred_rf,\n                 df_pred_svm)\n\n# Create a observed vs. predicted plot combined for all the models\nggplot(df_pred,aes(predicted,observed))+geom_point()+\n  lims(x = c(0,1.15) , y = c(0,1.15))+\n  labs(y = \"Observed\", x=\"Predicted\")+\n  facet_grid(~model, scales=\"free\")+\n  geom_abline()+\n  theme_bw(base_size = 10)\n```\n\n::: {.cell-output-display}\n![](garment_files/figure-html/ovp_plot-1.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nCombined evaluation metrics to compare all the models can be seen in the table below.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Evaluation metrics for all the models\nmetrics_all <- data.frame(Model = c(\"linear_model\", \"lasso\", \"ridge\",\n                                 \"elastic_net\", \"tree\", \"random_forest\",\n                                 \"svm\"),\n                       RMSE = c(rmse_lm,rmse_l1, rmse_l2, rmse_en, \n                                rmse_tree, rmse_rf, rmse_svm),\n                       MAE = c(mae_lm,mae_l1, mae_l2, mae_en, \n                                mae_tree, mae_rf, mae_svm),\n                       R_squared = c(r2_lm, r2_l1, r2_l2, r2_en, \n                                r2_tree, r2_rf, r2_svm))\n\n# Print evaluation metrics\nkable(metrics_all, digits = 2, caption = \"Model Performance Metrics\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Model Performance Metrics\n\n|Model         | RMSE|  MAE| R_squared|\n|:-------------|----:|----:|---------:|\n|linear_model  | 1.70| 0.04|      0.71|\n|lasso         | 1.74| 0.05|      0.65|\n|ridge         | 1.87| 0.05|      0.69|\n|elastic_net   | 1.87| 0.05|      0.69|\n|tree          | 1.36| 0.05|      0.58|\n|random_forest | 1.02| 0.04|      0.64|\n|svm           | 1.38| 0.04|      0.62|\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nVisualization of the combined evaluation metrics can be seen in the plot below.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# To plot RMSE and MAE on the same plot\nmetrics_long <- melt(metrics_all, id.vars = \"Model\", \n                     variable.name = \"Metric\", \n                     value.name = \"Value\")\n\nggplot(metrics_long, aes(x = Model, y = Value, fill = Model)) +\n  geom_col(show.legend = TRUE) +\n  geom_text(aes(label = round(Value, 2)), \n            #position = position_dodge(width = 0.8), \n            size = 3) +\n  coord_flip(clip = \"off\") +  # horizontal bars, no clipping of text\n  facet_grid2(~Metric, scales=\"free\")+\n  labs(title = \"Model Performance Comparison\", \n       y = \"Error Value\", x = \"Model\") +\n  theme_bw(base_size = 10) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](garment_files/figure-html/combine3-1.png){width=1344}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nBased on the evaluation metrics and observed vs. predicted plot, support vector machine and tree model seems to be the optimum models.\n:::\n",
    "supporting": [
      "garment_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}