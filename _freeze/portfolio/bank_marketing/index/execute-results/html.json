{
  "hash": "601d4113c0b08133fc613dab301b4388",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting bank marketing success\"\nsubtitle: \"Comparing AI methods to predict if customer will enroll into term deposit\"\nauthor: \"Aditya Ranade\"\nhighlight-style: github-light\ndate: \"2025-06-17\"\ncategories: [AI, analysis, R]\nimage: \"./bank_deposit.jpg\"\n---\n\n\n\n::: {style=\"text-align: justify\"}\nI found this [dataset](https://archive.ics.uci.edu/dataset/222/bank+marketing) on UCI machine learning repository which gives bank marketing data for a Portuguese banking institution. The goal is to predict if the client will subscribe to a term deposit. The data has various predictor variables. We will look at the data first and then look to build a prediction model.\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggh4x)\nlibrary(GGally)\nlibrary(pROC)\nlibrary(naivebayes)\nlibrary(caret)\nlibrary(e1071)\nlibrary(xgboost)\n\n# Load data in R\npath <- \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/bank_marketing/bank.csv\"\ndata0 <- read.csv(path, sep = \";\", header = TRUE)\n\n# Check the first 6 rows of the dataset\nhead(data0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  age         job marital education default balance housing loan  contact day\n1  30  unemployed married   primary      no    1787      no   no cellular  19\n2  33    services married secondary      no    4789     yes  yes cellular  11\n3  35  management  single  tertiary      no    1350     yes   no cellular  16\n4  30  management married  tertiary      no    1476     yes  yes  unknown   3\n5  59 blue-collar married secondary      no       0     yes   no  unknown   5\n6  35  management  single  tertiary      no     747      no   no cellular  23\n  month duration campaign pdays previous poutcome  y\n1   oct       79        1    -1        0  unknown no\n2   may      220        1   339        4  failure no\n3   apr      185        1   330        1  failure no\n4   jun      199        4    -1        0  unknown no\n5   may      226        1    -1        0  unknown no\n6   feb      141        2   176        3  failure no\n```\n\n\n:::\n\n```{.r .cell-code}\n# change column names\ncolnames(data0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"age\"       \"job\"       \"marital\"   \"education\" \"default\"   \"balance\"  \n [7] \"housing\"   \"loan\"      \"contact\"   \"day\"       \"month\"     \"duration\" \n[13] \"campaign\"  \"pdays\"     \"previous\"  \"poutcome\"  \"y\"        \n```\n\n\n:::\n\n```{.r .cell-code}\n#\"age\" \"job\" \"marital\" \"education\",\"default\" \"balance\"\n#\"housing\" \"loan\" \"contact\" \"day\" \"month\" \"duration\"\n#\"campaign\" \"pdays\" \"previous\" \"poutcome\" \"y\" \n\n# Check the type of data\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t4521 obs. of  17 variables:\n $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n $ job      : chr  \"unemployed\" \"services\" \"management\" \"management\" ...\n $ marital  : chr  \"married\" \"married\" \"single\" \"married\" ...\n $ education: chr  \"primary\" \"secondary\" \"tertiary\" \"tertiary\" ...\n $ default  : chr  \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n $ housing  : chr  \"no\" \"yes\" \"yes\" \"yes\" ...\n $ loan     : chr  \"no\" \"yes\" \"no\" \"yes\" ...\n $ contact  : chr  \"cellular\" \"cellular\" \"cellular\" \"unknown\" ...\n $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n $ month    : chr  \"oct\" \"may\" \"apr\" \"jun\" ...\n $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n $ poutcome : chr  \"unknown\" \"failure\" \"failure\" \"unknown\" ...\n $ y        : chr  \"no\" \"no\" \"no\" \"no\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the rows which do not have any entries\nsum(is.na(data0)) # No NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data processing\ndata <- data0 |> select(age,job,marital,education,default,\n                        balance,housing,loan,duration,\n                        campaign,pdays,previous,poutcome,y)\n\n# Check data type\ndata %>% str\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t4521 obs. of  14 variables:\n $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n $ job      : chr  \"unemployed\" \"services\" \"management\" \"management\" ...\n $ marital  : chr  \"married\" \"married\" \"single\" \"married\" ...\n $ education: chr  \"primary\" \"secondary\" \"tertiary\" \"tertiary\" ...\n $ default  : chr  \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n $ housing  : chr  \"no\" \"yes\" \"yes\" \"yes\" ...\n $ loan     : chr  \"no\" \"yes\" \"no\" \"yes\" ...\n $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n $ poutcome : chr  \"unknown\" \"failure\" \"failure\" \"unknown\" ...\n $ y        : chr  \"no\" \"no\" \"no\" \"no\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\ndata$job <- as.factor(data$job)\ndata$marital <- as.factor(data$marital)\ndata$education <- as.factor(data$education)\ndata$default <- as.factor(data$default)\ndata$housing <- as.factor(data$housing)\ndata$loan <- as.factor(data$loan)\ndata$poutcome <- as.factor(data$poutcome)\ndata$y <- as.factor(data$y)\n\n# Check the distribution of the outcome y\nggplot(data, aes(x = factor(y))) +\n  geom_bar(fill = \"purple\") +\n  labs(x = \"Target\", y = \"Count\", title = \"Distribution of Target\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/data_process-1.png){width=672}\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe number of yes are considerably low compared to no. This indicates we have imbalanced class. First we will look at a simple logistic regression.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# To ensure reproducibility\nset.seed(55)\n\n# Split data into training and testing set\nind <- sample(1:nrow(data),\n              floor(0.7*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train <- data[ind,]\n\n# Testing dataset\ndata_test <- data[-ind,]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logistic regression\nmodel <- glm(y ~ ., data = data_train, family = binomial())\n\n# Predicted probability\ny_pred_prob <- predict(model, data_test,\"response\")\n\n# Predicted class\ny_pred <- ifelse(y_pred_prob>0.5,\"yes\",\"no\")\n\n# Confusion matrix\nconfusionMatrix(data_test$y, as.factor(y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1163   30\n       yes  117   47\n                                          \n               Accuracy : 0.8917          \n                 95% CI : (0.8739, 0.9077)\n    No Information Rate : 0.9433          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.339           \n                                          \n Mcnemar's Test P-Value : 1.311e-12       \n                                          \n            Sensitivity : 0.9086          \n            Specificity : 0.6104          \n         Pos Pred Value : 0.9749          \n         Neg Pred Value : 0.2866          \n             Prevalence : 0.9433          \n         Detection Rate : 0.8570          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.7595          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Storage for confusion matrices\ncm_list <- list()\n\n# Confusion Matrix\ncm_list$logistic <- confusionMatrix(data_test$y, as.factor(y_pred))\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe accuracy is close to 90% which is good. Next we will try Naive Bayes classification method.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Naive Bayes\nmodel_nb <- naiveBayes(y ~ ., data = data_train) \n\n# Predictions\ny_pred_nb <- predict(model_nb, newdata = data_test)\n\n# Confusion matrix\nconfusionMatrix(data_test$y, y_pred_nb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1098   95\n       yes   85   79\n                                         \n               Accuracy : 0.8674         \n                 95% CI : (0.8481, 0.885)\n    No Information Rate : 0.8718         \n    P-Value [Acc > NIR] : 0.7037         \n                                         \n                  Kappa : 0.3918         \n                                         \n Mcnemar's Test P-Value : 0.5023         \n                                         \n            Sensitivity : 0.9281         \n            Specificity : 0.4540         \n         Pos Pred Value : 0.9204         \n         Neg Pred Value : 0.4817         \n             Prevalence : 0.8718         \n         Detection Rate : 0.8091         \n   Detection Prevalence : 0.8791         \n      Balanced Accuracy : 0.6911         \n                                         \n       'Positive' Class : no             \n                                         \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix\ncm_list$naive_bayes <- confusionMatrix(data_test$y, y_pred_nb)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe accuracy is close to 87.25% which is not bad. Next we will try random forest classification method.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random forest\nlibrary(randomForest)\n\nmodel_rf <- randomForest(y ~ ., \n                         data = data_train, \n                         ntree = 500, \n                         mtry = 2, \n                         importance = TRUE)\n\n# Predictions\ny_pred_rf <- predict(model_rf, data_test)\n\n# Confusion Matrix\ncm_list$random_forest <- confusionMatrix(data_test$y, y_pred_rf)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe accuracy is close to 88.65% which is not bad. Next we will try some method to deal with imbalanced dataset using the SMOTE (Synthetic Minority Oversampling Technique) method which tries to balance the classes by oversamling from the minority classes. We will try to run the three models based on the oversampled data.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# SMOTE method\nlibrary(ROSE)\n\ndata_train2 <- ovun.sample(y ~ ., \n                           data = data_train,\n                           method = \"over\", \n                           N = (nrow(data_train)*3))$data\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logistic regression\nmodel2 <- glm(y ~ ., data = data_train2, family = binomial())\n\n# Predicted probability\ny_pred_prob2 <- predict(model2, data_test,\"response\")\n\n# Predicted class\ny_pred2 <- ifelse(y_pred_prob2>0.5,\"yes\",\"no\")\n\n# Confusion matrix\nconfusionMatrix(data_test$y, as.factor(y_pred2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  749 444\n       yes  14 150\n                                          \n               Accuracy : 0.6625          \n                 95% CI : (0.6366, 0.6876)\n    No Information Rate : 0.5623          \n    P-Value [Acc > NIR] : 3.111e-14       \n                                          \n                  Kappa : 0.2546          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9817          \n            Specificity : 0.2525          \n         Pos Pred Value : 0.6278          \n         Neg Pred Value : 0.9146          \n             Prevalence : 0.5623          \n         Detection Rate : 0.5520          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.6171          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix\ncm_list$smote_logistic <- confusionMatrix(data_test$y, as.factor(y_pred2))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Naive Bayes\nmodel_nb2 <- naiveBayes(y ~ ., data = data_train2) \n\n# Predictions\ny_pred_nb2 <- predict(model_nb2, newdata = data_test)\n\n# Confusion matrix\nconfusionMatrix(data_test$y, y_pred_nb2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  797 396\n       yes  25 139\n                                          \n               Accuracy : 0.6898          \n                 95% CI : (0.6644, 0.7143)\n    No Information Rate : 0.6057          \n    P-Value [Acc > NIR] : 7.788e-11       \n                                          \n                  Kappa : 0.261           \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9696          \n            Specificity : 0.2598          \n         Pos Pred Value : 0.6681          \n         Neg Pred Value : 0.8476          \n             Prevalence : 0.6057          \n         Detection Rate : 0.5873          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.6147          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix\ncm_list$smote_naive_bayes <- confusionMatrix(data_test$y, y_pred_nb2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random forest\nmodel_rf2 <- randomForest(y ~ ., \n                         data = data_train, \n                         ntree = 500, \n                         mtry = 2, \n                         importance = TRUE)\n\n# Predictions\ny_pred_rf2 <- predict(model_rf2, data_test)\n\n# Confusion Matrix\ncm_list$smote_random_forest <- confusionMatrix(data_test$y, y_pred_rf2)\n```\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nThe comparison of all the models is as follows\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract multiple metrics into one table\n\nresults_df <- data.frame(\n  Model = names(cm_list),\n  Accuracy     = sapply(cm_list, function(l) l$overall[\"Accuracy\"]),\n  Sensitivity  = sapply(cm_list, function(l) l$byClass[\"Sensitivity\"]),\n  Specificity  = sapply(cm_list, function(l) l$byClass[\"Specificity\"])\n)\n\nrownames(results_df) <- NULL\nresults_df <- results_df %>% arrange(desc(Accuracy))\nresults_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Model  Accuracy Sensitivity Specificity\n1            logistic 0.8916728   0.9085937   0.6103896\n2       random_forest 0.8820929   0.8898113   0.5625000\n3 smote_random_forest 0.8798821   0.8895613   0.5142857\n4         naive_bayes 0.8673545   0.9281488   0.4540230\n5   smote_naive_bayes 0.6897568   0.9695864   0.2598131\n6      smote_logistic 0.6624908   0.9816514   0.2525253\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"text-align: justify\"}\nBased on accuracy, logistic regression model on original data is the best and based on the sensitivity (true positive rate), logistic regression on oversampled data is the best.\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}