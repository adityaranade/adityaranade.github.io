{
  "hash": "2d5c04ec526d397365c4505468a07c1a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting Abalone age using Poisson Regression\"\nsubtitle: \"Predicting Abalone age using Poisson Regression\"\nauthor: \"Aditya Ranade\"\nhighlight-style:\n  light: github\ndate: \"2025-05-31\"\ncategories: [analysis, R]\nimage: \"./abalone.jpg\"\n---\n\n\n\n\n::: {style=\"text-align: justify\"}\nI found this [dataset](https://archive.ics.uci.edu/dataset/1/abalone) on UCI machine learning repository which gives the dataset measuring the characteristics of Abalone. The goal is to predict the age of Abalone which is done using the number of rings in Abalone. Generally, the age of abalone is the number of rings plus 1.5 As an example, if the number of rings of Abalone is 2, the age is estimated to be 2 + 1.5 = 3 years. However, measuring the number of rings is a painful process for the Abalone. Hence we will try to build a predictive model to predict the number of rings in Abalone using the phisical characteristics like the weight, height, diameter, etc.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(ggcorrplot)\nlibrary(GGally) # for pairs plot using ggplot framework\nlibrary(dplyr)\n\n# Load the data\npath <- \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/abalone/abalone.data\"\ndata0 <- read.csv(path, header = TRUE)\ncolnames(data0) <- c(\"Sex\", \"Length\", \"Diameter\", \"Height\", \"whole_weight\"\n              ,\"shucked_weight\", \"viscera_weight\", \"shell_weight\", \"Rings\")\n\n# Data processing\n\n# Check the type of data\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t4176 obs. of  9 variables:\n $ Sex           : chr  \"M\" \"F\" \"M\" \"I\" ...\n $ Length        : num  0.35 0.53 0.44 0.33 0.425 0.53 0.545 0.475 0.55 0.525 ...\n $ Diameter      : num  0.265 0.42 0.365 0.255 0.3 0.415 0.425 0.37 0.44 0.38 ...\n $ Height        : num  0.09 0.135 0.125 0.08 0.095 0.15 0.125 0.125 0.15 0.14 ...\n $ whole_weight  : num  0.226 0.677 0.516 0.205 0.351 ...\n $ shucked_weight: num  0.0995 0.2565 0.2155 0.0895 0.141 ...\n $ viscera_weight: num  0.0485 0.1415 0.114 0.0395 0.0775 ...\n $ shell_weight  : num  0.07 0.21 0.155 0.055 0.12 0.33 0.26 0.165 0.32 0.21 ...\n $ Rings         : int  7 9 10 7 8 20 16 9 19 14 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the rows which do not have any entries\nsum(is.na(data0)) # no NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the first 6 rows of the dataset\ndata0 |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sex Length Diameter Height whole_weight shucked_weight viscera_weight\n1   M  0.350    0.265  0.090       0.2255         0.0995         0.0485\n2   F  0.530    0.420  0.135       0.6770         0.2565         0.1415\n3   M  0.440    0.365  0.125       0.5160         0.2155         0.1140\n4   I  0.330    0.255  0.080       0.2050         0.0895         0.0395\n5   I  0.425    0.300  0.095       0.3515         0.1410         0.0775\n6   F  0.530    0.415  0.150       0.7775         0.2370         0.1415\n  shell_weight Rings\n1        0.070     7\n2        0.210     9\n3        0.155    10\n4        0.055     7\n5        0.120     8\n6        0.330    20\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlation plot\nggcorrplot(round(cor(data0[,-1]), 2), \n           lab = TRUE,\n           hc.order = TRUE,\n           type = \"upper\")\n```\n\n::: {.cell-output-display}\n![](abalone_files/figure-html/EDA0-1.png){width=1344}\n:::\n\n```{.r .cell-code}\n# num_data <- data0[, sapply(data0, is.numeric)]\n# \n# # Compute correlation matrix\n# corr <- round(cor(num_data, use = \"pairwise.complete.obs\"), 2)\n# \n# # Plot with correlation values\n# ggcorrplot(\n#   corr,\n#   hc.order = TRUE,       # Cluster variables\n#   type = \"lower\",        # Show only lower triangle\n#   lab = TRUE,            # Add correlation numbers\n#   lab_size = 3,          # Number size\n#   show.legend = TRUE,    # Keep color legend\n#   colors = c(\"red\", \"white\", \"blue\") # Optional custom colors\n# )\n\n\n# Pairs plot between the explanatory variables to \n# check correlation between each pair of the variables\nggpairs(data0)\n```\n\n::: {.cell-output-display}\n![](abalone_files/figure-html/EDA0-2.png){width=1344}\n:::\n:::\n\n\n\n\n::: {style=\"text-align: justify\"}\nThe response variable rings is high correlated with all the variables which is good. However, the explanatory variables are correlated within themselves which is not a good indication. This indicates there is severe multicollinearity. This means two variables give similar information about the response variable.\n\nThe number of rings is a integer and the histogram can be seen below.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check the histogram of the response variable\nggplot(data0,aes(Rings))+\n  geom_histogram(bins=100)+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](abalone_files/figure-html/EDA1-1.png){width=1344}\n:::\n:::\n\n\n\n\n::: {style=\"text-align: justify\"}\nThis can be treated as a count data and we can use the Poisson regression. However in Poisson regression, the mean and variance of the response variable quality should be same.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check if the mean and variance of response variable is same.\n\n# Mean\ndata0$Rings %>% mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.932471\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance\ndata0$Rings %>% var\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.39161\n```\n\n\n:::\n:::\n\n\n\n\n::: {style=\"text-align: justify\"}\nThe mean is quite similar to the variance. Hence we can use the Poisson regression to model the data. Based on the correlation plot, shell weight has the highest correlation with rings, so we will use only shell weight and sex as response variable. First we split the data into training and testing set and then run the regression model. We will compare a linear regression model and Poisson regression model. The predictions will be rounded and compared to the raw data through a table.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Only select the variables\ndata <- data0 |> select(Sex, shell_weight, Rings)\n\n# split the data into training and testing data\nseed <- 23\nset.seed(seed)\n\nind <- sample(1:nrow(data),\n              floor(0.8*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train <- data[ind,]\n# Testing dataset\ndata_test <- data[-ind,]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_lm <- lm(Rings ~ shell_weight,\n            data = data_train)\n\nsummary(model_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Rings ~ shell_weight, data = data_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9169 -1.5781 -0.5746  0.9083 15.6804 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   6.46573    0.08582   75.34   <2e-16 ***\nshell_weight 14.42926    0.31268   46.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.493 on 3338 degrees of freedom\nMultiple R-squared:  0.3895,\tAdjusted R-squared:  0.3893 \nF-statistic:  2130 on 1 and 3338 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction on the testing dataset\ny_pred_lm <- predict(model_lm, data_test)\n\nrmse_lm <- sqrt(sum((y_pred_lm-data_test$Rings)^2))\nrmse_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 74.40407\n```\n\n\n:::\n\n```{.r .cell-code}\ntable_lm <- table(Predicted = as.factor(round(y_pred_lm)),\n                  Actual = as.factor(data_test$Rings))\ntable_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Actual\nPredicted  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27\n       7   4  7 21 26 18  9  2  1  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n       8   0  0  1 17 34 31 24 11  6  4  2  1  0  1  0  0  0  0  0  0  0  0  0\n       9   0  0  0  3 18 43 28 21 17 12  5  1  1  1  0  3  0  0  1  0  0  0  0\n       10  0  0  0  2  7 27 32 29 15 11  6  8  3  0  2  1  2  1  0  0  0  0  0\n       11  0  0  0  0  0  5 28 31 21 10 14 10  5  1  3  3  1  0  0  0  0  0  0\n       12  0  0  0  1  0  3 10 16 17 11  9  5  2  1  1  3  2  1  1  0  0  1  0\n       13  0  0  0  0  1  2  0  8 17  7  1  4  2  0  2  1  2  1  0  0  0  0  0\n       14  0  0  0  0  0  0  0  3  7  1  5  0  1  1  1  1  2  1  0  0  1  0  1\n       15  0  0  0  0  0  0  0  0  3  3  0  1  1  0  1  0  1  1  4  1  0  0  0\n       16  0  0  0  0  0  0  0  0  0  1  1  0  0  3  0  1  0  0  0  0  0  0  0\n       18  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n       19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n       21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0\n```\n\n\n:::\n:::\n\n\n\n\n::: {style=\"text-align: justify\"}\nNow we will try the Poisson regression model.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Poisson regression\nmodel_poisson <- glm(Rings ~ shell_weight + Sex, \n                     family = poisson,\n                     data = data_train)\nsummary(model_poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Rings ~ shell_weight + Sex, family = poisson, data = data_train)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   2.04562    0.01711 119.567   <2e-16 ***\nshell_weight  1.17142    0.04490  26.092   <2e-16 ***\nSexI         -0.13868    0.01651  -8.402   <2e-16 ***\nSexM         -0.02075    0.01283  -1.618    0.106    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3263.8  on 3339  degrees of freedom\nResidual deviance: 1898.0  on 3336  degrees of freedom\nAIC: 15594\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction for testing data\ny_pred_poisson <- predict(model_poisson, \n                          data_test,\n                          type = \"response\")\n\nrmse_poisson <- sqrt(sum((y_pred_poisson-data_test$Rings)^2))\nrmse_poisson\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 75.30501\n```\n\n\n:::\n\n```{.r .cell-code}\ntable_poisson <- table(as.factor(round(y_pred_poisson)),\n                       as.factor(data_test$Rings))\ntable_poisson\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    \n      3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27\n  7   3  6 19 27 21 13  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0\n  8   1  1  3 15 35 41 27  7  8  3  1  1  0  1  0  2  0  0  0  0  0  0  0\n  9   0  0  0  5 13 30 29 30 11  7  5  1  0  1  0  1  0  0  0  0  0  0  0\n  10  0  0  0  2  6 26 24 18 19 14  6  8  4  0  3  1  2  1  1  0  0  0  0\n  11  0  0  0  0  2  5 34 36 23 13 15 12  4  1  4  5  2  0  0  0  0  0  0\n  12  0  0  0  0  0  3  9 17 14  9  8  3  2  1  0  1  1  0  1  0  0  1  0\n  13  0  0  0  0  1  2  0  8 18  8  2  4  3  0  1  2  2  2  0  0  0  0  0\n  14  0  0  0  0  0  0  0  2  7  1  5  0  1  0  1  0  2  1  0  0  1  0  1\n  15  0  0  0  0  0  0  0  1  2  1  0  1  0  1  1  0  0  0  4  1  0  0  0\n  16  0  0  0  0  0  0  0  0  1  3  1  0  1  1  0  0  1  1  0  0  0  0  0\n  17  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  1  0  0  0  0  0  0  0\n  20  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n  22  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n  25  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0\n```\n\n\n:::\n:::\n\n\n\n\n::: {style=\"text-align: justify\"}\nIf we look carefully, after rounding, the predictions are just around the diagonals, which is decent. Due to multicolinearity, we cannot use multiple variables in the model without some data reduction techniques. We can use the principal component analysis then use the linear regression or Poisson regression. I did try them and the results were similar. So overall, its not a bad model but not extremely good model.\n:::\n",
    "supporting": [
      "abalone_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}