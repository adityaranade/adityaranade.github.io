{
  "hash": "fc88ce1ac52b7f732aa6e64cb722302c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting bank marketing success\"\nsubtitle: \"Comparing AI methods to predict if customer will enroll into term deposit\"\nauthor: \"Aditya Ranade\"\nhighlight-style: github-light\ndate: \"2025-06-17\"\ncategories: [AI, analysis, R]\nimage: \"./bank_deposit.jpg\"\n---\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nI found this [dataset](https://archive.ics.uci.edu/dataset/222/bank+marketing) on UCI machine learning repository which gives bank marketing data for a Portuguese banking institution. The goal is to predict if the client will subscribe to a term deposit. The data has various predictor variables. We will look at the data first and then look to build a prediction model.\n\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggh4x)\nlibrary(GGally)\nlibrary(pROC)\nlibrary(naivebayes)\nlibrary(caret)\nlibrary(e1071)\nlibrary(nnet)\nlibrary(xgboost)\n\n# Load data in R\npath <- \"https://raw.githubusercontent.com/adityaranade/portfolio/refs/heads/main/bank_marketing/bank.csv\"\ndata0 <- read.csv(path, sep = \";\", header = TRUE)\n\n# Check the first 6 rows of the dataset\nhead(data0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  age         job marital education default balance housing loan  contact day\n1  30  unemployed married   primary      no    1787      no   no cellular  19\n2  33    services married secondary      no    4789     yes  yes cellular  11\n3  35  management  single  tertiary      no    1350     yes   no cellular  16\n4  30  management married  tertiary      no    1476     yes  yes  unknown   3\n5  59 blue-collar married secondary      no       0     yes   no  unknown   5\n6  35  management  single  tertiary      no     747      no   no cellular  23\n  month duration campaign pdays previous poutcome  y\n1   oct       79        1    -1        0  unknown no\n2   may      220        1   339        4  failure no\n3   apr      185        1   330        1  failure no\n4   jun      199        4    -1        0  unknown no\n5   may      226        1    -1        0  unknown no\n6   feb      141        2   176        3  failure no\n```\n\n\n:::\n\n```{.r .cell-code}\n# change column names\ncolnames(data0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"age\"       \"job\"       \"marital\"   \"education\" \"default\"   \"balance\"  \n [7] \"housing\"   \"loan\"      \"contact\"   \"day\"       \"month\"     \"duration\" \n[13] \"campaign\"  \"pdays\"     \"previous\"  \"poutcome\"  \"y\"        \n```\n\n\n:::\n\n```{.r .cell-code}\n#\"age\" \"job\" \"marital\" \"education\",\"default\" \"balance\"\n#\"housing\" \"loan\" \"contact\" \"day\" \"month\" \"duration\"\n#\"campaign\" \"pdays\" \"previous\" \"poutcome\" \"y\" \n\n# Check the type of data\ndata0 |> str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t4521 obs. of  17 variables:\n $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n $ job      : chr  \"unemployed\" \"services\" \"management\" \"management\" ...\n $ marital  : chr  \"married\" \"married\" \"single\" \"married\" ...\n $ education: chr  \"primary\" \"secondary\" \"tertiary\" \"tertiary\" ...\n $ default  : chr  \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n $ housing  : chr  \"no\" \"yes\" \"yes\" \"yes\" ...\n $ loan     : chr  \"no\" \"yes\" \"no\" \"yes\" ...\n $ contact  : chr  \"cellular\" \"cellular\" \"cellular\" \"unknown\" ...\n $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n $ month    : chr  \"oct\" \"may\" \"apr\" \"jun\" ...\n $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n $ poutcome : chr  \"unknown\" \"failure\" \"failure\" \"unknown\" ...\n $ y        : chr  \"no\" \"no\" \"no\" \"no\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the rows which do not have any entries\nsum(is.na(data0)) # No NA values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data processing\ndata <- data0 |> select(age,job,marital,education,default,\n                        balance,housing,loan,duration,\n                        campaign,pdays,previous,poutcome,y)\n\n# # Check data type\n# data %>% str\n\n# Convert the variables to categorical\ndata$job <- as.factor(data$job)\ndata$marital <- as.factor(data$marital)\ndata$education <- as.factor(data$education)\ndata$default <- as.factor(data$default)\ndata$housing <- as.factor(data$housing)\ndata$loan <- as.factor(data$loan)\ndata$poutcome <- as.factor(data$poutcome)\ndata$y <- as.factor(data$y)\n\n# Check the distribution of the outcome y\nggplot(data, aes(x = factor(y), fill = factor(y))) +\n  geom_bar() +\n  #geom_bar(fill = \"purple\") +\n  geom_text(stat = \"count\", aes(label = ..count..), vjust = -0.5)+\n  labs(x = \"Target\", y = \"Count\", title = \"Distribution of Target\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/data_process-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {style=\"text-align: justify\"}\nThe number of yes are considerably low compared to no. This indicates we have imbalanced class. First we will look at a simple logistic regression.\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# To ensure reproducibility\nset.seed(55)\n\n# Split data into training and testing set\nind <- sample(1:nrow(data),\n              floor(0.7*nrow(data)),\n              replace = FALSE)\n\n# Training dataset\ndata_train <- data[ind,]\n\n# Testing dataset\ndata_test <- data[-ind,]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logistic regression\nmodel <- glm(y ~ ., data = data_train, family = binomial())\n\n# Predicted probability\ny_pred_prob <- predict(model, data_test,\"response\")\n\n# Predicted class\ny_pred <- ifelse(y_pred_prob>0.5,\"yes\",\"no\")\n\n# Confusion matrix\nconfusionMatrix(data_test$y, as.factor(y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1163   30\n       yes  117   47\n                                          \n               Accuracy : 0.8917          \n                 95% CI : (0.8739, 0.9077)\n    No Information Rate : 0.9433          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.339           \n                                          \n Mcnemar's Test P-Value : 1.311e-12       \n                                          \n            Sensitivity : 0.9086          \n            Specificity : 0.6104          \n         Pos Pred Value : 0.9749          \n         Neg Pred Value : 0.2866          \n             Prevalence : 0.9433          \n         Detection Rate : 0.8570          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.7595          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Storage for confusion matrices\ncm_list <- list()\n\n# Confusion Matrix\ncm_list$logistic <- confusionMatrix(data_test$y, as.factor(y_pred))\n```\n:::\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nThe accuracy is around 89.17% which is good. Next we will try stepwise model selection process on the logistic regression model.\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Stepwise both directions\nmodel_step <- step(model, direction = \"both\")  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=1618.01\ny ~ age + job + marital + education + default + balance + housing + \n    loan + duration + campaign + pdays + previous + poutcome\n\n            Df Deviance    AIC\n- age        1   1560.0 1616.0\n- previous   1   1560.0 1616.0\n- balance    1   1560.0 1616.0\n- job       11   1580.1 1616.1\n- pdays      1   1560.3 1616.3\n- marital    2   1562.7 1616.7\n- default    1   1562.0 1618.0\n<none>           1560.0 1618.0\n- education  3   1569.6 1621.6\n- campaign   1   1567.5 1623.5\n- loan       1   1569.7 1625.7\n- housing    1   1574.3 1630.3\n- poutcome   3   1668.7 1720.7\n- duration   1   1954.8 2010.8\n\nStep:  AIC=1616.03\ny ~ job + marital + education + default + balance + housing + \n    loan + duration + campaign + pdays + previous + poutcome\n\n            Df Deviance    AIC\n- previous   1   1560.0 1614.0\n- balance    1   1560.0 1614.0\n- pdays      1   1560.3 1614.3\n- marital    2   1562.9 1614.9\n- default    1   1562.0 1616.0\n<none>           1560.0 1616.0\n- job       11   1582.9 1616.9\n+ age        1   1560.0 1618.0\n- education  3   1570.0 1620.0\n- campaign   1   1567.5 1621.5\n- loan       1   1569.7 1623.7\n- housing    1   1574.5 1628.5\n- poutcome   3   1668.7 1718.7\n- duration   1   1955.1 2009.1\n\nStep:  AIC=1614.05\ny ~ job + marital + education + default + balance + housing + \n    loan + duration + campaign + pdays + poutcome\n\n            Df Deviance    AIC\n- balance    1   1560.1 1612.1\n- pdays      1   1560.3 1612.3\n- marital    2   1562.9 1612.9\n- default    1   1562.0 1614.0\n<none>           1560.0 1614.0\n- job       11   1582.9 1614.9\n+ previous   1   1560.0 1616.0\n+ age        1   1560.0 1616.0\n- education  3   1570.0 1618.0\n- campaign   1   1567.5 1619.5\n- loan       1   1569.7 1621.7\n- housing    1   1574.6 1626.6\n- poutcome   3   1677.9 1725.9\n- duration   1   1955.2 2007.2\n\nStep:  AIC=1612.07\ny ~ job + marital + education + default + housing + loan + duration + \n    campaign + pdays + poutcome\n\n            Df Deviance    AIC\n- pdays      1   1560.3 1610.3\n- marital    2   1563.0 1611.0\n- default    1   1562.1 1612.1\n<none>           1560.1 1612.1\n- job       11   1582.9 1612.9\n+ balance    1   1560.0 1614.0\n+ age        1   1560.0 1614.0\n+ previous   1   1560.0 1614.0\n- education  3   1570.0 1616.0\n- campaign   1   1567.6 1617.6\n- loan       1   1569.7 1619.7\n- housing    1   1574.6 1624.6\n- poutcome   3   1677.9 1723.9\n- duration   1   1955.4 2005.4\n\nStep:  AIC=1610.35\ny ~ job + marital + education + default + housing + loan + duration + \n    campaign + poutcome\n\n            Df Deviance    AIC\n- marital    2   1563.3 1609.3\n<none>           1560.3 1610.3\n- default    1   1562.4 1610.4\n- job       11   1583.1 1611.1\n+ pdays      1   1560.1 1612.1\n+ balance    1   1560.3 1612.3\n+ age        1   1560.3 1612.3\n+ previous   1   1560.3 1612.3\n- education  3   1570.2 1614.2\n- campaign   1   1567.8 1615.8\n- loan       1   1569.9 1617.9\n- housing    1   1574.6 1622.6\n- poutcome   3   1721.4 1765.4\n- duration   1   1955.5 2003.5\n\nStep:  AIC=1609.29\ny ~ job + education + default + housing + loan + duration + campaign + \n    poutcome\n\n            Df Deviance    AIC\n<none>           1563.3 1609.3\n- default    1   1565.8 1609.8\n+ marital    2   1560.3 1610.3\n+ pdays      1   1563.0 1611.0\n- job       11   1587.1 1611.1\n+ age        1   1563.1 1611.1\n+ balance    1   1563.2 1611.2\n+ previous   1   1563.3 1611.3\n- education  3   1574.3 1614.3\n- campaign   1   1570.7 1614.7\n- loan       1   1572.9 1616.9\n- housing    1   1578.0 1622.0\n- poutcome   3   1723.7 1763.7\n- duration   1   1962.6 2006.6\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(model_step)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ job + education + default + housing + loan + \n    duration + campaign + poutcome, family = binomial(), data = data_train)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        -2.9206986  0.3848207  -7.590  3.2e-14 ***\njobblue-collar     -0.3983436  0.2844555  -1.400 0.161402    \njobentrepreneur    -0.6110379  0.4715547  -1.296 0.195046    \njobhousemaid       -0.8500412  0.5432529  -1.565 0.117647    \njobmanagement      -0.2057875  0.2866986  -0.718 0.472891    \njobretired          0.8255495  0.3253605   2.537 0.011170 *  \njobself-employed    0.0226447  0.4001233   0.057 0.954868    \njobservices        -0.1731728  0.3259470  -0.531 0.595217    \njobstudent          0.3476402  0.4376270   0.794 0.426977    \njobtechnician      -0.0850288  0.2719354  -0.313 0.754524    \njobunemployed      -0.1266363  0.4461336  -0.284 0.776523    \njobunknown         -0.5545394  0.7945078  -0.698 0.485198    \neducationsecondary  0.1854277  0.2371710   0.782 0.434314    \neducationtertiary   0.6573345  0.2701526   2.433 0.014966 *  \neducationunknown   -0.2956239  0.4108627  -0.720 0.471821    \ndefaultyes          0.7489408  0.4421353   1.694 0.090281 .  \nhousingyes         -0.5532075  0.1449790  -3.816 0.000136 ***\nloanyes            -0.6490925  0.2211968  -2.934 0.003341 ** \nduration            0.0042745  0.0002405  17.774  < 2e-16 ***\ncampaign           -0.0863822  0.0346880  -2.490 0.012765 *  \npoutcomeother       0.3088592  0.3227868   0.957 0.338642    \npoutcomesuccess     2.5223389  0.3064338   8.231  < 2e-16 ***\npoutcomeunknown    -0.5724993  0.2106880  -2.717 0.006582 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2230.0  on 3163  degrees of freedom\nResidual deviance: 1563.3  on 3141  degrees of freedom\nAIC: 1609.3\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predicted probability\ny_pred_prob_step <- predict(model_step, data_test,\"response\")\n\n# Predicted class\ny_pred_step <- ifelse(y_pred_prob_step>0.5,\"yes\",\"no\")\n\n# Confusion matrix\nconfusionMatrix(data_test$y, as.factor(y_pred_step))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1162   31\n       yes  118   46\n                                          \n               Accuracy : 0.8902          \n                 95% CI : (0.8723, 0.9063)\n    No Information Rate : 0.9433          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.33            \n                                          \n Mcnemar's Test P-Value : 1.849e-12       \n                                          \n            Sensitivity : 0.9078          \n            Specificity : 0.5974          \n         Pos Pred Value : 0.9740          \n         Neg Pred Value : 0.2805          \n             Prevalence : 0.9433          \n         Detection Rate : 0.8563          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.7526          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix\ncm_list$logistic_step <- confusionMatrix(data_test$y, as.factor(y_pred_step))\n```\n:::\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nThe accuracy is around 89.02% which is good. Next we will try Naive Bayes classification method. \n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Naive Bayes\nmodel_nb <- naiveBayes(y ~ ., data = data_train) \n\n# Predictions\ny_pred_nb <- predict(model_nb, newdata = data_test)\n\n# Confusion matrix\nconfusionMatrix(data_test$y, y_pred_nb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1098   95\n       yes   85   79\n                                         \n               Accuracy : 0.8674         \n                 95% CI : (0.8481, 0.885)\n    No Information Rate : 0.8718         \n    P-Value [Acc > NIR] : 0.7037         \n                                         \n                  Kappa : 0.3918         \n                                         \n Mcnemar's Test P-Value : 0.5023         \n                                         \n            Sensitivity : 0.9281         \n            Specificity : 0.4540         \n         Pos Pred Value : 0.9204         \n         Neg Pred Value : 0.4817         \n             Prevalence : 0.8718         \n         Detection Rate : 0.8091         \n   Detection Prevalence : 0.8791         \n      Balanced Accuracy : 0.6911         \n                                         \n       'Positive' Class : no             \n                                         \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix data frame\ncm_list$naive_bayes <- confusionMatrix(data_test$y, y_pred_nb)\n```\n:::\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nThe accuracy is close to 86.74% which is not bad. Next we will try random forest classification method.\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random forest\nlibrary(randomForest)\n\nmodel_rf <- randomForest(y ~ ., \n                         data = data_train, \n                         ntree = 500, \n                         mtry = 2, \n                         importance = TRUE)\n\n# Predictions\ny_pred_rf <- predict(model_rf, data_test)\n\n# Confusion Matrix\nconfusionMatrix(data_test$y, y_pred_rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1179   14\n       yes  146   18\n                                          \n               Accuracy : 0.8821          \n                 95% CI : (0.8637, 0.8988)\n    No Information Rate : 0.9764          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.1501          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.8898          \n            Specificity : 0.5625          \n         Pos Pred Value : 0.9883          \n         Neg Pred Value : 0.1098          \n             Prevalence : 0.9764          \n         Detection Rate : 0.8688          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.7262          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix data frame\ncm_list$random_forest <- confusionMatrix(data_test$y, y_pred_rf)\n```\n:::\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nThe accuracy is close to 88.21% which is not bad. Next we will neural network method.\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Neural Networks\nmodel_nnet <- nnet(y ~ ., \n              data = data_train, \n              size = 5, \n              decay = 0.01, \n              maxit = 200)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# weights:  151\ninitial  value 1932.835777 \niter  10 value 1084.378919\niter  20 value 1008.871476\niter  30 value 1000.762095\niter  40 value 980.193591\niter  50 value 940.441583\niter  60 value 926.854358\niter  70 value 921.815841\niter  80 value 917.811037\niter  90 value 884.518071\niter 100 value 825.223570\niter 110 value 802.825670\niter 120 value 794.840877\niter 130 value 784.739292\niter 140 value 766.855650\niter 150 value 759.677806\niter 160 value 757.825474\niter 170 value 756.109305\niter 180 value 752.439765\niter 190 value 748.184352\niter 200 value 746.727211\nfinal  value 746.727211 \nstopped after 200 iterations\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predictions\ny_pred_nnet <- predict(model_nnet,\n                        data_test, \n                        type = \"class\")\n\n# Confusion Matrix\nconfusionMatrix(data_test$y, as.factor(y_pred_nnet))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1158   35\n       yes  106   58\n                                          \n               Accuracy : 0.8961          \n                 95% CI : (0.8786, 0.9118)\n    No Information Rate : 0.9315          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.3988          \n                                          \n Mcnemar's Test P-Value : 3.745e-09       \n                                          \n            Sensitivity : 0.9161          \n            Specificity : 0.6237          \n         Pos Pred Value : 0.9707          \n         Neg Pred Value : 0.3537          \n             Prevalence : 0.9315          \n         Detection Rate : 0.8534          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.7699          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix data frame\ncm_list$neural_network <- confusionMatrix(data_test$y, as.factor(y_pred_nnet))\n```\n:::\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nThe accuracy is close to 89.61% which is not bad. Next we will try support vector classification method.\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# support vector classifier \nmodel_svc <- svm(y ~ .,\n                 data = data_train,\n                 # kernel = \"linear\",  # or \"radial\", \"polynomial\", \"sigmoid\"\n                 kernel = \"sigmoid\",  # or \"radial\", \"polynomial\", \"sigmoid\"\n                 cost = 1,           # regularization parameter \n                 scale = FALSE)\n\n# Predictions\ny_pred_svc <- predict(model_svc,\n                        data_test)\n\n# Confusion Matrix\nconfusionMatrix(data_test$y, as.factor(y_pred_svc))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1017  176\n       yes  147   17\n                                          \n               Accuracy : 0.762           \n                 95% CI : (0.7384, 0.7844)\n    No Information Rate : 0.8578          \n    P-Value [Acc > NIR] : 1.0000          \n                                          \n                  Kappa : -0.0408         \n                                          \n Mcnemar's Test P-Value : 0.1192          \n                                          \n            Sensitivity : 0.87371         \n            Specificity : 0.08808         \n         Pos Pred Value : 0.85247         \n         Neg Pred Value : 0.10366         \n             Prevalence : 0.85777         \n         Detection Rate : 0.74945         \n   Detection Prevalence : 0.87915         \n      Balanced Accuracy : 0.48090         \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix data frame\ncm_list$smote_support_vector_classifier <- confusionMatrix(data_test$y, as.factor(y_pred_svc))\n```\n:::\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nThe accuracy is close to 89.61% which is not bad. Next we will try some method to deal with imbalanced dataset using the SMOTE (Synthetic Minority Oversampling Technique) method which tries to balance the classes by oversampling from the minority classes. We will try to run all the models based on the over sampled data and compare them.\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# SMOTE method\nlibrary(ROSE)\n\ndata_train2 <- ovun.sample(y ~ ., \n                           data = data_train,\n                           method = \"over\", \n                           N = (nrow(data_train)*3))$data\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logistic regression\nmodel2 <- glm(y ~ ., data = data_train2, family = binomial())\n\n# Predicted probability\ny_pred_prob2 <- predict(model2, data_test,\"response\")\n\n# Predicted class\ny_pred2 <- ifelse(y_pred_prob2>0.5,\"yes\",\"no\")\n\n# Confusion matrix\nconfusionMatrix(data_test$y, as.factor(y_pred2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  748 445\n       yes  14 150\n                                          \n               Accuracy : 0.6618          \n                 95% CI : (0.6359, 0.6869)\n    No Information Rate : 0.5615          \n    P-Value [Acc > NIR] : 3.163e-14       \n                                          \n                  Kappa : 0.2539          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9816          \n            Specificity : 0.2521          \n         Pos Pred Value : 0.6270          \n         Neg Pred Value : 0.9146          \n             Prevalence : 0.5615          \n         Detection Rate : 0.5512          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.6169          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix data frame\ncm_list$smote_logistic <- confusionMatrix(data_test$y, as.factor(y_pred2))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Stepwise both directions\nmodel_step2 <- step(model2, direction = \"both\")  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=7440.23\ny ~ age + job + marital + education + default + balance + housing + \n    loan + duration + campaign + pdays + previous + poutcome\n\n            Df Deviance     AIC\n- age        1   7382.3  7438.3\n- balance    1   7382.6  7438.6\n- previous   1   7382.7  7438.7\n- default    1   7382.8  7438.8\n<none>           7382.2  7440.2\n- pdays      1   7386.2  7442.2\n- marital    2   7392.7  7446.7\n- education  3   7423.7  7475.7\n- housing    1   7481.0  7537.0\n- job       11   7519.7  7555.7\n- loan       1   7540.3  7596.3\n- campaign   1   7540.6  7596.6\n- poutcome   3   7756.2  7808.2\n- duration   1  10193.3 10249.3\n\nStep:  AIC=7438.25\ny ~ job + marital + education + default + balance + housing + \n    loan + duration + campaign + pdays + previous + poutcome\n\n            Df Deviance     AIC\n- balance    1   7382.7  7436.7\n- previous   1   7382.7  7436.7\n- default    1   7382.8  7436.8\n<none>           7382.3  7438.3\n+ age        1   7382.2  7440.2\n- pdays      1   7386.2  7440.2\n- marital    2   7393.5  7445.5\n- education  3   7425.9  7475.9\n- housing    1   7482.9  7536.9\n- job       11   7546.8  7580.8\n- loan       1   7540.4  7594.4\n- campaign   1   7540.6  7594.6\n- poutcome   3   7756.4  7806.4\n- duration   1  10194.7 10248.7\n\nStep:  AIC=7436.68\ny ~ job + marital + education + default + housing + loan + duration + \n    campaign + pdays + previous + poutcome\n\n            Df Deviance     AIC\n- previous   1   7383.2  7435.2\n- default    1   7383.3  7435.3\n<none>           7382.7  7436.7\n+ balance    1   7382.3  7438.3\n- pdays      1   7386.6  7438.6\n+ age        1   7382.6  7438.6\n- marital    2   7394.0  7444.0\n- education  3   7426.0  7474.0\n- housing    1   7483.1  7535.1\n- job       11   7546.8  7578.8\n- loan       1   7540.5  7592.5\n- campaign   1   7540.8  7592.8\n- poutcome   3   7756.4  7804.4\n- duration   1  10195.4 10247.4\n\nStep:  AIC=7435.17\ny ~ job + marital + education + default + housing + loan + duration + \n    campaign + pdays + poutcome\n\n            Df Deviance     AIC\n- default    1   7383.8  7433.8\n<none>           7383.2  7435.2\n+ previous   1   7382.7  7436.7\n+ balance    1   7382.7  7436.7\n- pdays      1   7386.8  7436.8\n+ age        1   7383.1  7437.1\n- marital    2   7394.5  7442.5\n- education  3   7426.5  7472.5\n- housing    1   7483.6  7533.6\n- job       11   7546.9  7576.9\n- loan       1   7540.7  7590.7\n- campaign   1   7541.0  7591.0\n- poutcome   3   7841.6  7887.6\n- duration   1  10202.1 10252.1\n\nStep:  AIC=7433.76\ny ~ job + marital + education + housing + loan + duration + campaign + \n    pdays + poutcome\n\n            Df Deviance     AIC\n<none>           7383.8  7433.8\n+ default    1   7383.2  7435.2\n+ balance    1   7383.2  7435.2\n+ previous   1   7383.3  7435.3\n- pdays      1   7387.4  7435.4\n+ age        1   7383.7  7435.7\n- marital    2   7395.8  7441.8\n- education  3   7427.4  7471.4\n- housing    1   7484.9  7532.9\n- job       11   7546.9  7574.9\n- loan       1   7540.8  7588.8\n- campaign   1   7542.9  7590.9\n- poutcome   3   7841.6  7885.6\n- duration   1  10208.6 10256.6\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(model_step2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ job + marital + education + housing + loan + \n    duration + campaign + pdays + poutcome, family = binomial(), \n    data = data_train2)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        -0.1052809  0.2082530  -0.506 0.613177    \njobblue-collar     -0.8114544  0.1154661  -7.028 2.10e-12 ***\njobentrepreneur    -0.5055734  0.1830097  -2.763 0.005735 ** \njobhousemaid       -1.0576435  0.2196267  -4.816 1.47e-06 ***\njobmanagement      -0.4349114  0.1233348  -3.526 0.000421 ***\njobretired          0.5651946  0.1513535   3.734 0.000188 ***\njobself-employed   -0.3235794  0.1738904  -1.861 0.062769 .  \njobservices        -0.5776775  0.1348496  -4.284 1.84e-05 ***\njobstudent          0.0991976  0.2075718   0.478 0.632725    \njobtechnician      -0.3372895  0.1133161  -2.977 0.002915 ** \njobunemployed      -0.4847497  0.1946102  -2.491 0.012743 *  \njobunknown         -1.4111436  0.3711723  -3.802 0.000144 ***\nmaritalmarried     -0.2163072  0.0927979  -2.331 0.019756 *  \nmaritalsingle      -0.0078369  0.1038239  -0.075 0.939830    \neducationsecondary  0.2633713  0.0993298   2.651 0.008014 ** \neducationtertiary   0.6399636  0.1143595   5.596 2.19e-08 ***\neducationunknown   -0.1708036  0.1756055  -0.973 0.330725    \nhousingyes         -0.6224268  0.0623738  -9.979  < 2e-16 ***\nloanyes            -1.1926751  0.0969035 -12.308  < 2e-16 ***\nduration            0.0060072  0.0001572  38.218  < 2e-16 ***\ncampaign           -0.1844988  0.0153296 -12.035  < 2e-16 ***\npdays               0.0009013  0.0004785   1.883 0.059655 .  \npoutcomeother       0.3051444  0.1484881   2.055 0.039878 *  \npoutcomesuccess     2.6996244  0.2030783  13.294  < 2e-16 ***\npoutcomeunknown    -0.3949296  0.1456172  -2.712 0.006686 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11527.0  on 9491  degrees of freedom\nResidual deviance:  7383.8  on 9467  degrees of freedom\nAIC: 7433.8\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predicted probability\ny_pred_prob_step2 <- predict(model_step2, data_test,\"response\")\n\n# Predicted class\ny_pred_step2 <- ifelse(y_pred_prob_step2>0.5,\"yes\",\"no\")\n\n# Confusion matrix\nconfusionMatrix(data_test$y, as.factor(y_pred_step2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  749 444\n       yes  14 150\n                                          \n               Accuracy : 0.6625          \n                 95% CI : (0.6366, 0.6876)\n    No Information Rate : 0.5623          \n    P-Value [Acc > NIR] : 3.111e-14       \n                                          \n                  Kappa : 0.2546          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9817          \n            Specificity : 0.2525          \n         Pos Pred Value : 0.6278          \n         Neg Pred Value : 0.9146          \n             Prevalence : 0.5623          \n         Detection Rate : 0.5520          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.6171          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix\ncm_list$smote_logistic_step <- confusionMatrix(data_test$y, as.factor(y_pred_step2))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Naive Bayes\nmodel_nb2 <- naiveBayes(y ~ ., data = data_train2) \n\n# Predictions\ny_pred_nb2 <- predict(model_nb2, newdata = data_test)\n\n# Confusion matrix\nconfusionMatrix(data_test$y, y_pred_nb2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  797 396\n       yes  25 139\n                                          \n               Accuracy : 0.6898          \n                 95% CI : (0.6644, 0.7143)\n    No Information Rate : 0.6057          \n    P-Value [Acc > NIR] : 7.788e-11       \n                                          \n                  Kappa : 0.261           \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9696          \n            Specificity : 0.2598          \n         Pos Pred Value : 0.6681          \n         Neg Pred Value : 0.8476          \n             Prevalence : 0.6057          \n         Detection Rate : 0.5873          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.6147          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix data frame\ncm_list$smote_naive_bayes <- confusionMatrix(data_test$y, y_pred_nb2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random forest\nmodel_rf2 <- randomForest(y ~ ., \n                         data = data_train, \n                         ntree = 500, \n                         mtry = 2, \n                         importance = TRUE)\n\n# Predictions\ny_pred_rf2 <- predict(model_rf2, data_test)\n\n# Confusion Matrix\nconfusionMatrix(data_test$y, y_pred_rf2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1179   14\n       yes  147   17\n                                         \n               Accuracy : 0.8814         \n                 95% CI : (0.863, 0.8981)\n    No Information Rate : 0.9772         \n    P-Value [Acc > NIR] : 1              \n                                         \n                  Kappa : 0.1414         \n                                         \n Mcnemar's Test P-Value : <2e-16         \n                                         \n            Sensitivity : 0.8891         \n            Specificity : 0.5484         \n         Pos Pred Value : 0.9883         \n         Neg Pred Value : 0.1037         \n             Prevalence : 0.9772         \n         Detection Rate : 0.8688         \n   Detection Prevalence : 0.8791         \n      Balanced Accuracy : 0.7188         \n                                         \n       'Positive' Class : no             \n                                         \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix data frame\ncm_list$smote_random_forest <- confusionMatrix(data_test$y, y_pred_rf2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Neural Networks\nmodel_nnet2 <- nnet(y ~ ., \n              data = data_train2, \n              size = 5, \n              decay = 0.01, \n              maxit = 200)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# weights:  151\ninitial  value 6672.439994 \niter  10 value 5444.151418\niter  20 value 4928.673252\niter  30 value 4668.104777\niter  40 value 4574.222061\niter  50 value 4493.420471\niter  60 value 4331.950671\niter  70 value 4099.714709\niter  80 value 4002.793010\niter  90 value 3980.135694\niter 100 value 3847.275435\niter 110 value 3731.594349\niter 120 value 3678.434483\niter 130 value 3672.812783\niter 140 value 3671.252056\niter 150 value 3665.833513\niter 160 value 3665.133792\niter 170 value 3663.637799\niter 180 value 3659.793664\niter 190 value 3659.407716\niter 200 value 3658.453928\nfinal  value 3658.453928 \nstopped after 200 iterations\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predictions\ny_pred_nnet2 <- predict(model_nnet2,\n                        data_test, \n                        type = \"class\")\n\n# Confusion Matrix\nconfusionMatrix(data_test$y, as.factor(y_pred_nnet2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  806 387\n       yes  25 139\n                                          \n               Accuracy : 0.6964          \n                 95% CI : (0.6711, 0.7208)\n    No Information Rate : 0.6124          \n    P-Value [Acc > NIR] : 6.596e-11       \n                                          \n                  Kappa : 0.268           \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9699          \n            Specificity : 0.2643          \n         Pos Pred Value : 0.6756          \n         Neg Pred Value : 0.8476          \n             Prevalence : 0.6124          \n         Detection Rate : 0.5940          \n   Detection Prevalence : 0.8791          \n      Balanced Accuracy : 0.6171          \n                                          \n       'Positive' Class : no              \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix data frame\ncm_list$smote_neural_network <- confusionMatrix(data_test$y, as.factor(y_pred_nnet2))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# support vector classifier \nmodel_svc2 <- svm(y ~ .,\n                  data = data_train2,\n                  kernel = \"sigmoid\",  # or \"radial\", \"polynomial\", \"sigmoid\" \n                  cost = 1,           # regularization parameter \n                  scale = TRUE)\n\n# Predictions\ny_pred_svc2 <- predict(model_svc2,\n                       data_test)\n\n# Confusion Matrix\nconfusionMatrix(data_test$y, as.factor(y_pred_svc2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  629 564\n       yes  17 147\n                                         \n               Accuracy : 0.5718         \n                 95% CI : (0.545, 0.5984)\n    No Information Rate : 0.5239         \n    P-Value [Acc > NIR] : 0.0002213      \n                                         \n                  Kappa : 0.1737         \n                                         \n Mcnemar's Test P-Value : < 2.2e-16      \n                                         \n            Sensitivity : 0.9737         \n            Specificity : 0.2068         \n         Pos Pred Value : 0.5272         \n         Neg Pred Value : 0.8963         \n             Prevalence : 0.4761         \n         Detection Rate : 0.4635         \n   Detection Prevalence : 0.8791         \n      Balanced Accuracy : 0.5902         \n                                         \n       'Positive' Class : no             \n                                         \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix data frame\ncm_list$support_vector_classifier <- confusionMatrix(data_test$y, as.factor(y_pred_svc2))\n```\n:::\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nThe comparison of all the models is as follows\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract multiple metrics into one table\n\nresults_df <- data.frame(\n  Model = names(cm_list),\n  Accuracy     = sapply(cm_list, function(l) l$overall[\"Accuracy\"]),\n  Sensitivity  = sapply(cm_list, function(l) l$byClass[\"Sensitivity\"]),\n  Specificity  = sapply(cm_list, function(l) l$byClass[\"Specificity\"])\n)\n\nrownames(results_df) <- NULL\nresults_df <- results_df %>% arrange(desc(Accuracy))\nresults_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                             Model  Accuracy Sensitivity Specificity\n1                   neural_network 0.8960943   0.9161392   0.6236559\n2                         logistic 0.8916728   0.9085937   0.6103896\n3                    logistic_step 0.8901990   0.9078125   0.5974026\n4                    random_forest 0.8820929   0.8898113   0.5625000\n5              smote_random_forest 0.8813559   0.8891403   0.5483871\n6                      naive_bayes 0.8673545   0.9281488   0.4540230\n7  smote_support_vector_classifier 0.7619749   0.8737113   0.0880829\n8             smote_neural_network 0.6963891   0.9699158   0.2642586\n9                smote_naive_bayes 0.6897568   0.9695864   0.2598131\n10             smote_logistic_step 0.6624908   0.9816514   0.2525253\n11                  smote_logistic 0.6617539   0.9816273   0.2521008\n12       support_vector_classifier 0.5718497   0.9736842   0.2067511\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nBased on accuracy, neural network model on original data is the best and based on the sensitivity (true positive rate), logistic regression on over sampled data is the best.\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}